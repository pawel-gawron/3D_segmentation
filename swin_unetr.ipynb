{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.3.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /home/<username>/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: 1.14.0\n",
      "Pillow version: 10.3.0\n",
      "Tensorboard version: 2.17.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.8.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import pytorch_lightning\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Resized,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    "    list_data_collate,\n",
    "    pad_list_data_collate,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from dotenv import dotenv_values\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'AeroPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp9cm139aq\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_name = \"left_upper\"\n",
    "# segment_name = \"right_upper\"\n",
    "# segment_name = \"right_bottom\"\n",
    "# segment_name = \"left_bottom\"\n",
    "# segment_name = \"whole\"\n",
    "# segment_name = \"2Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = dotenv_values(\".env\")\n",
    "api_key = keys[\"API_KEY\"]\n",
    "\n",
    "neptune_logger = pytorch_lightning.loggers.NeptuneLogger(\n",
    "    project=\"aeropath-workspace/airways-model\",\n",
    "    api_key=api_key,\n",
    "    tags=['aeropath', 'airways', 'monai', 'SwinUNETR', segment_name],\n",
    "    name='airways-training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_dice',\n",
    "    dirpath=os.path.join(data_dir, 'checkpoints'),  # Directory to save checkpoints\n",
    "    filename=segment_name +'_swin_unetr' '_{epoch:02d}-{val_dice:.4f}',  # Filename prefix for saving checkpoints\n",
    "    save_top_k=1,  # Save only the best checkpoint\n",
    "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
    "    verbose=True,  # Log a message when saving the best checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'img_size': (64, 64, 64),\n",
    "    'in_channels': 1,\n",
    "    'out_channels': 2,\n",
    "    'feature_size':48,\n",
    "    'use_checkpoint': True,\n",
    "}\n",
    "\n",
    "weight = torch.load(\"./model_swinvit.pt\")\n",
    "\n",
    "class SwinUNetClass(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, mode, roi_size, spatial_size):\n",
    "        super().__init__()\n",
    "        self._model = SwinUNETR(**parameters)\n",
    "        self._model.load_from(weights=weight)\n",
    "        self.loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        self.mode = mode\n",
    "        self.roi_size = roi_size\n",
    "        self.spatial_size = spatial_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def prepare_data(self, prepare_val_data=True, prepare_test_data=True):\n",
    "        # # set up the correct data path\n",
    "        def get_patterns(mode):\n",
    "            patterns = {\n",
    "                'whole': ('**/*_CT_HR_label_airways.nii.gz', '**/*_CT_HR.nii.gz'),\n",
    "                '1Q': ('nonoverlapping_labels/**/quadrant_1_*.nii.gz', 'nonoverlapping_quadrants/**/quadrant_1_*_CT_HR.nii.gz'),\n",
    "                '2Q': ('nonoverlapping_labels/**/quadrant_2_*.nii.gz', 'nonoverlapping_quadrants/**/quadrant_2_*_CT_HR.nii.gz'),\n",
    "                'left_bottom': ('dataset/airways_patched_4/**/*left_bottom_*.nii.gz', 'dataset/scan_patched_4/**/*left_bottom_*.nii.gz'),\n",
    "                'left_upper': ('dataset/airways_patched_4/**/*left_upper_*.nii.gz', 'dataset/scan_patched_4/**/*left_upper_*.nii.gz'),\n",
    "                'right_bottom': ('dataset/airways_patched_4/**/*right_bottom_*.nii.gz', 'dataset/scan_patched_4/**/*right_bottom_*.nii.gz'),\n",
    "                'right_upper': ('dataset/airways_patched_4/**/*right_upper_*.nii.gz', 'dataset/scan_patched_4/**/*right_upper_*.nii.gz')\n",
    "            }\n",
    "            return patterns.get(mode, (None, None))\n",
    "\n",
    "        pattern_labels, pattern_images = get_patterns(self.mode)\n",
    "        if pattern_labels and pattern_images:\n",
    "            train_labels = sorted(glob.glob(pattern_labels, recursive=True))\n",
    "            train_images = sorted(glob.glob(pattern_images, recursive=True))\n",
    "\n",
    "\n",
    "\n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=0)\n",
    "\n",
    "        # define the data transforms\n",
    "        num_samples = 2\n",
    "\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-1024,\n",
    "                    a_max=1024,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.0, 1.0, 1.35),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "                # Resized(keys=[\"image\", \"label\"], spatial_size=self.spatial_size),\n",
    "                RandCropByPosNegLabeld(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    label_key=\"label\",\n",
    "                    spatial_size=(64, 64, 64),\n",
    "                    pos=1,\n",
    "                    neg=1,\n",
    "                    num_samples=num_samples,\n",
    "                    image_key=\"image\",\n",
    "                    image_threshold=0,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[0],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[1],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandFlipd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    spatial_axis=[2],\n",
    "                    prob=0.10,\n",
    "                ),\n",
    "                RandRotate90d(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.10,\n",
    "                    max_k=3,\n",
    "                ),\n",
    "                RandShiftIntensityd(\n",
    "                    keys=[\"image\"],\n",
    "                    offsets=0.10,\n",
    "                    prob=0.50,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "                ScaleIntensityRanged(keys=[\"image\"], a_min=-1024, a_max=1024, b_min=0.0, b_max=1.0, clip=True),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.0, 1.0, 1.35),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "                # Resized(keys=[\"image\", \"label\"], spatial_size=self.spatial_size),\n",
    "            ]\n",
    "        )                    \n",
    "\n",
    "        if prepare_test_data:\n",
    "            self.train_ds = CacheDataset(\n",
    "                data=train_files,\n",
    "                transform=train_transforms,\n",
    "                cache_rate=1.0,\n",
    "                num_workers=8,\n",
    "            )\n",
    "        if prepare_val_data:\n",
    "            self.val_ds = CacheDataset(\n",
    "                data=val_files,\n",
    "                transform=val_transforms,\n",
    "                cache_rate=1.0,\n",
    "                num_workers=4,\n",
    "            )\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = ThreadDataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = ThreadDataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self._model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"].cuda(), batch[\"label\"].cuda()\n",
    "        roi_size = self.roi_size\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "\n",
    "        outputs_np = [output.argmax(dim=0).cpu().numpy() for output in outputs]\n",
    "        labels_np = [label.argmax(dim=0).cpu().numpy() for label in labels]\n",
    "        iou = np.mean([jaccard_score(ln.flatten(), on.flatten(), average='macro') for ln, on in zip(labels_np, outputs_np)])\n",
    "\n",
    "        d = {\"val_loss\": loss, \"val_number\": len(outputs), \"iou\": iou}\n",
    "        self.validation_step_outputs.append(d)\n",
    "        return d\n",
    "    \n",
    "\n",
    "    \n",
    "    def perform_inference(self, model, data):\n",
    "        # Perform inference using the model\n",
    "        with torch.no_grad():\n",
    "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
    "            model_output = model(data.unsqueeze(0))\n",
    "        return model_output\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss, num_items, total_iou = 0, 0, 0\n",
    "        for output in self.validation_step_outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "            total_iou += output[\"iou\"] * output[\"val_number\"]\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        mean_val_iou = total_iou / num_items\n",
    "        self.dice_metric.reset()\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        tensorboard_logs = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_loss\": mean_val_loss,\n",
    "            \"val_iou\": mean_val_iou,\n",
    "        }\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current mean dice: {mean_val_dice:.4f} \"\n",
    "            f\"current mean iou: {mean_val_iou:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.validation_step_outputs.clear()\n",
    "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_loss', mean_val_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val_iou', mean_val_iou, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "\n",
    "        return {\"log\": tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    def dice_score(self, prediction_tensor, label_tensor):\n",
    "        # Compute Dice score\n",
    "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
    "        dice_score = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "\n",
    "        print(dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net_segment       = SwinUNetClass(mode = segment_name,       roi_size=[64, 64, 64],   spatial_size=(160, 160, 160))\n",
    "NetWhole          = SwinUNetClass(mode = 'whole',            roi_size=(32, 32, 32),   spatial_size=(160, 160, 160))\n",
    "Net1Q             = SwinUNetClass(mode = '1Q',               roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
    "Net2Q             = SwinUNetClass(mode = '2Q',               roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
    "\n",
    "# NetWhole    = UNetClass(mode = 'whole', roi_size=(192, 192, 212),   spatial_size=(192, 192, 212))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/aeropath-workspace/airways-model/e/AIR-79\n"
     ]
    }
   ],
   "source": [
    "neptune_logger.experiment[\"model/parameters\"] = stringify_unsupported(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 18/18 [00:08<00:00,  2.25it/s]\n",
      "Loading dataset: 100%|██████████| 9/9 [00:04<00:00,  2.07it/s]\n",
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/aeropath-workspace/airways-model/e/AIR-84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /home/pawel/Documents/RISA/3D_segmentation/AeroPath/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type       | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | _model        | SwinUNETR  | 62.2 M | train\n",
      "1 | loss_function | DiceCELoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "62.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "62.2 M    Total params\n",
      "248.747   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc3784bca774c86b29b222fc8055e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 0 current mean dice: 0.0131 current mean iou: 0.0745\n",
      "best mean dice: 0.0131 at epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638221a288404ae39571b276d8f61129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed58670edd8741a788132d328b5701b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 18: 'val_dice' reached 0.00670 (best 0.00670), saving model to '/home/pawel/Documents/RISA/3D_segmentation/AeroPath/checkpoints/left_upper_swin_unetr_epoch=00-val_dice=0.0067.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 0 current mean dice: 0.0067 current mean iou: 0.4972\n",
      "best mean dice: 0.0131 at epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method MetadataContainer._before_fork of <neptune.metadata_containers.run.Run object at 0x72e91a27d0f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/neptune/metadata_containers/metadata_container.py\", line 285, in _before_fork\n",
      "    self._op_processor.pause()\n",
      "  File \"/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 159, in pause\n",
      "    self._consumer.pause()\n",
      "  File \"/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/neptune/internal/threading/daemon.py\", line 56, in pause\n",
      "    self._wait_condition.wait_for(lambda: self._state != Daemon.DaemonState.PAUSING)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 355, in wait_for\n",
      "    self.wait(waittime)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "# net = Net1Q\n",
    "# net = Net2Q\n",
    "net = Net_segment\n",
    "# net = NetWhole\n",
    "# set up loggers and checkpoints\n",
    "log_dir = os.path.join(root_dir, \"logs\")\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    devices=[0],\n",
    "    max_epochs=600,\n",
    "    logger=neptune_logger,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    num_sanity_val_steps=1,\n",
    "    log_every_n_steps=16,\n",
    ")\n",
    "# train\n",
    "trainer.fit(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
