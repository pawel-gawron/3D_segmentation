{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from patchify import patchify\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nibabel.processing import resample_to_output\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wersja CUDNN: 8902\n"
     ]
    }
   ],
   "source": [
    "print(\"Wersja CUDNN:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, images_ct_scans, images_ct_masks, transform=None):\n",
    "        self.images_ct_scans = images_ct_scans\n",
    "        self.images_ct_masks = images_ct_masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_ct_scans)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.images_ct_scans[idx]\n",
    "        # print(\"image_file\", image_file)\n",
    "        # print(\"image_file.shape\", image_file.shape)\n",
    "        # print(\"images_ct_masks\", self.images_ct_masks)\n",
    "        # print(\"images_ct_masks.shape\", self.images_ct_masks.shape)\n",
    "\n",
    "        mask_file = self.images_ct_masks[idx]\n",
    "\n",
    "        print(image_file.shape[-1])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed_images = []\n",
    "            transformed_masks = []\n",
    "            for i in range(0, image_file.shape[-1]):\n",
    "                # print(image_file[..., i].shape)\n",
    "                # print(i)\n",
    "                # print(\"image_file[..., i]\", image_file[..., i])\n",
    "                image_slice = image_file[..., i]\n",
    "\n",
    "                # print(\"type: \", type(image_slice))\n",
    "                # print(\"image_slice: \", image_slice)\n",
    "                # print(\"image_slice shape: \", image_slice.shape)\n",
    "                mask_slice = mask_file[..., i]\n",
    "                \n",
    "                image_slice = image_slice.astype(np.int16)\n",
    "                mask_slice = mask_slice.astype(np.uint8)\n",
    "\n",
    "                # print(\"max: \", np.max(image_slice))\n",
    "                # print(\"min: \", np.min(image_slice))\n",
    "                # print(\"dtype: \", image_slice[0].dtype)\n",
    "\n",
    "                transformed = self.transform(image=image_slice, mask=mask_slice)\n",
    "\n",
    "                transformed_images.append(transformed[\"image\"])\n",
    "                transformed_masks.append(transformed[\"mask\"])\n",
    "\n",
    "            image_file = transformed_images\n",
    "            mask_file = transformed_masks\n",
    "        return image_file, mask_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmentations = A.Compose([\n",
    "        A.ToFloat(max_value=1024+400, always_apply=True),\n",
    "        A.Resize(height=50, width=50),\n",
    "        # A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.Normalize(mean=[-1024/400], std=[1/400], always_apply=True),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "        self.transforms = A.Compose([\n",
    "        A.ToFloat(max_value=1024+400, always_apply=True),\n",
    "        A.Resize(height=50, width=50),\n",
    "        # A.Normalize(mean=[-1024/400], std=[1/400], always_apply=True),\n",
    "        ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "        self.ct_dataset= None\n",
    "\n",
    "        self.all_ct_scan_patches = []\n",
    "        self.all_ct_mask_patches = []\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.ct_dataset = load_dataset(\"andreped/AeroPath\")\n",
    "        executor = ThreadPoolExecutor()\n",
    "\n",
    "        print('len dataset', self.ct_dataset['test'].num_rows)\n",
    "\n",
    "        print(self.ct_dataset)\n",
    "\n",
    "        for index, img in enumerate(range(len(self.ct_dataset['test']))):\n",
    "            print(index)     #just stop here to see all file names printed\n",
    "                    \n",
    "            large_image = self.ct_dataset['test'][img]\n",
    "\n",
    "            ct_image = nib.load(large_image[\"ct\"])\n",
    "            ct_image = resample_to_output(ct_image, order=1)\n",
    "            ct_data_scan = ct_image.get_fdata().astype(\"int16\")\n",
    "            ct_data_scan[ct_data_scan < -1024] = -1024\n",
    "            ct_data_scan[ct_data_scan > 400] = 400\n",
    "            # ct_data_scan = (ct_data_scan - np.min(ct_data_scan)) / (np.max(ct_data_scan) - np.min(ct_data_scan))\n",
    "\n",
    "            # ct_data_scan = ct_data_scan.astype(np.float16)\n",
    "            # ct_data_scan = np.clip(ct_data_scan, 0, 255).astype(\"uint8\")\n",
    "\n",
    "            # print(\"max: \", np.max(ct_data_scan))\n",
    "            # print(\"min: \", np.min(ct_data_scan))\n",
    "            # print(\"type:\", ct_data_scan[0].dtype)\n",
    "\n",
    "            ct_data_scan = ct_data_scan + 1024\n",
    "            # print('shape: ', ct_data_scan.shape)\n",
    "\n",
    "            # print('image: ', ct_data_scan)\n",
    "\n",
    "            # break\n",
    "\n",
    "            # ct_mask = nib.load(large_image[\"airways\"])\n",
    "            # ct_mask = resample_to_output(ct_mask, order=1)\n",
    "            # ct_data_mask = ct_mask.get_fdata().astype(\"uint8\")\n",
    "                    \n",
    "            # percent_size_x = 0.5  # Przykładowo 20% w osi X\n",
    "            # percent_size_y = 0.5  # Przykładowo 20% w osi Y\n",
    "            # fixed_size_z = 50  # Stała liczba sliców w osi Z\n",
    "\n",
    "            # scan_shape = ct_data_scan.shape\n",
    "\n",
    "            # patch_size_x = min(int(round(scan_shape[0] * percent_size_x)), scan_shape[0])\n",
    "            # patch_size_y = min(int(round(scan_shape[1] * percent_size_y)), scan_shape[1])\n",
    "\n",
    "            # patch_size = (patch_size_x, patch_size_y, fixed_size_z)\n",
    "\n",
    "            # print(\"patch_size\", patch_size)\n",
    "\n",
    "            # step = patch_size\n",
    "\n",
    "            # patches_scan = patchify(ct_data_scan, patch_size, step=step)\n",
    "            # patches_mask = patchify(ct_data_mask, patch_size, step=step)\n",
    "\n",
    "#             patches_scan = torch.tensor(patches_scan, dtype=torch.float32, device='cuda')\n",
    "#             patches_mask = torch.tensor(patches_mask, dtype=torch.float32, device='cuda')\n",
    "\n",
    "#  # Zrownoleglenie obliczeń\n",
    "#             futures = []\n",
    "#             with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#                 for i in range(patches_scan.shape[0]):\n",
    "#                     for j in range(patches_scan.shape[1]):\n",
    "#                         for k in range(patches_scan.shape[2]):\n",
    "#                             futures.append(executor.submit(self.process_patch, i, j, k, patches_scan, patches_mask))\n",
    "#                 for future in concurrent.futures.as_completed(futures):\n",
    "#                     patch_scan, patch_mask = future.result()\n",
    "#                     self.all_ct_scan_patches.append(patch_scan)\n",
    "#                     self.all_ct_mask_patches.append(patch_mask)   \n",
    "\n",
    "            # Pętle do iteracji przez otrzymane fragmenty danego skanu\n",
    "            # for i in range(patches_scan.shape[0]):\n",
    "            #     for j in range(patches_scan.shape[1]):\n",
    "            #         for k in range(patches_scan.shape[2]):\n",
    "            #             patch_scan = patches_scan[i, j, k, :, :, :]\n",
    "            #             patch_mask = patches_mask[i, j, k, :, :, :]\n",
    "                            \n",
    "            #             self.all_ct_scan_patches.append(patch_scan)\n",
    "            #             self.all_ct_mask_patches.append(patch_mask)\n",
    "\n",
    "            # print(\"self.all_ct_scan_patches.shape\", len(self.all_ct_scan_patches))\n",
    "            # print(\"self.all_ct_mask_patches.shape\", len(self.all_ct_mask_patches))\n",
    "\n",
    "##################################################################### for testing purposes\n",
    "            # print(len(self.all_ct_scan_patches))\n",
    "            # fig, ax = plt.subplots(2, 2, figsize=(20, 12))\n",
    "            # ax[0,0].cla()\n",
    "            # ax[0,1].cla()\n",
    "            # ax[1,0].cla()\n",
    "            # ax[1,1].cla()\n",
    "            # ax[0,0].imshow(self.all_ct_scan_patches[0][..., 1], cmap=\"gray\")\n",
    "            # print(\"ax[0,0] shape\", np.shape(ax[0,0]))\n",
    "            # ax[0,1].imshow(self.all_ct_scan_patches[patches_scan.shape[2] + 1][..., 1], cmap=\"gray\")\n",
    "            # ax[1,0].imshow(self.all_ct_scan_patches[2*patches_scan.shape[2] + 1][..., 1], cmap=\"gray\")\n",
    "            # ax[1,1].imshow(self.all_ct_scan_patches[3*patches_scan.shape[2] + 1][..., 1], cmap=\"gray\")\n",
    "\n",
    "            # print(\"image: \", self.all_ct_scan_patches[0][..., 49])\n",
    "            # print(\"shape: \", self.all_ct_scan_patches[0][..., 49].shape)\n",
    "\n",
    "            # plt.show()\n",
    "\n",
    "            # break\n",
    "#####################################################################\n",
    "\n",
    "        # images = np.array(all_img_patches)\n",
    "        # images = np.expand_dims(images, -1)\n",
    "                        \n",
    "            # if img == 1:\n",
    "            #     break\n",
    "\n",
    "    def process_patch(self, i, j, k, patches_scan, patches_mask):\n",
    "        patch_scan = patches_scan[i, j, k, :, :, :]\n",
    "        patch_mask = patches_mask[i, j, k, :, :, :]\n",
    "        return patch_scan, patch_mask\n",
    "\n",
    "    def setup(self):\n",
    "        # Split the data and assign datasets for use in dataloaders\n",
    "\n",
    "        all_indices = np.arange(len(self.all_ct_scan_patches))\n",
    "\n",
    "        # self.all_ct_scan_patches = np.array([np.array(patch) for patch in self.all_ct_scan_patches])\n",
    "        # self.all_ct_mask_patches = np.array([np.array(patch) for patch in self.all_ct_mask_patches])\n",
    "\n",
    "        self.all_ct_scan_patches = np.array(self.all_ct_scan_patches, dtype=object)\n",
    "        self.all_ct_mask_patches = np.array(self.all_ct_mask_patches, dtype=object)\n",
    "\n",
    "        train_index, val_index = train_test_split(all_indices, test_size = 0.3, random_state=42)\n",
    "        val_index, test_index = train_test_split(val_index, test_size = 0.5, random_state=42)\n",
    "\n",
    "        # print('Typ 1:', self.all_ct_scan_patches)\n",
    "        # print('Typ 2:', type(self.all_ct_mask_patches))\n",
    "        print('Indeksy zbioru walidacyjnego:', val_index)\n",
    "        print('Indeksy zbioru testowego:', test_index)\n",
    "        print('Indeksy zbioru treningowego:', train_index)\n",
    "\n",
    "        train_scans = self.all_ct_scan_patches[train_index]\n",
    "        train_masks = self.all_ct_mask_patches[train_index]\n",
    "\n",
    "        val_scans = self.all_ct_scan_patches[val_index]\n",
    "        val_masks = self.all_ct_mask_patches[val_index]\n",
    "\n",
    "        test_scans = self.all_ct_scan_patches[test_index]\n",
    "        test_masks = self.all_ct_mask_patches[test_index]\n",
    "\n",
    "        self.train_dataset = CTDataset(train_scans, train_masks, transform=self.augmentations)\n",
    "        self.val_dataset = CTDataset(val_scans, val_masks, transform=self.transforms)\n",
    "        self.test_dataset = CTDataset(test_scans, test_masks, transform=self.transforms)\n",
    "\n",
    "        data, mask = self.train_dataset.__getitem__(0)\n",
    "        print(\"len1\", len(data))\n",
    "        print(\"len2\", np.shape(data[0]))\n",
    "        print(\"data.shape\", np.shape(data))\n",
    "\n",
    "        plt.imshow(data[0].reshape(50, 50), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"data: \", data[0])\n",
    "\n",
    "        print(self.train_dataset)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=12)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=12)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/Documents/RISA/magisterka/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for andreped/AeroPath contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/andreped/AeroPath\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dataset 27\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['ct', 'airways', 'lungs'],\n",
      "        num_rows: 27\n",
      "    })\n",
      "})\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m CTDataModule()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m datamodule\u001b[38;5;241m.\u001b[39msetup()\n",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m, in \u001b[0;36mCTDataModule.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m large_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mct_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][img]\n\u001b[1;32m     42\u001b[0m ct_image \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(large_image[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 43\u001b[0m ct_image \u001b[38;5;241m=\u001b[39m \u001b[43mresample_to_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mct_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m ct_data_scan \u001b[38;5;241m=\u001b[39m ct_image\u001b[38;5;241m.\u001b[39mget_fdata()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m ct_data_scan[ct_data_scan \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1024\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1024\u001b[39m\n",
      "File \u001b[0;32m~/Documents/RISA/magisterka/.venv/lib/python3.10/site-packages/nibabel/processing.py:246\u001b[0m, in \u001b[0;36mresample_to_output\u001b[0;34m(in_img, voxel_sizes, order, mode, cval, out_class)\u001b[0m\n\u001b[1;32m    244\u001b[0m         voxel_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(voxel_sizes) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m-\u001b[39m n_dim)\n\u001b[1;32m    245\u001b[0m out_vox_map \u001b[38;5;241m=\u001b[39m vox2out_vox((in_img\u001b[38;5;241m.\u001b[39mshape, in_img\u001b[38;5;241m.\u001b[39maffine), voxel_sizes)\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresample_from_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_vox_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RISA/magisterka/.venv/lib/python3.10/site-packages/nibabel/processing.py:179\u001b[0m, in \u001b[0;36mresample_from_to\u001b[0;34m(from_img, to_vox_map, order, mode, cval, out_class)\u001b[0m\n\u001b[1;32m    177\u001b[0m to_vox2from_vox \u001b[38;5;241m=\u001b[39m npl\u001b[38;5;241m.\u001b[39minv(a_from_affine)\u001b[38;5;241m.\u001b[39mdot(a_to_affine)\n\u001b[1;32m    178\u001b[0m rzs, trans \u001b[38;5;241m=\u001b[39m to_matvec(to_vox2from_vox)\n\u001b[0;32m--> 179\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mspnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcval\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_class(data, to_affine, from_img\u001b[38;5;241m.\u001b[39mheader)\n",
      "File \u001b[0;32m~/Documents/RISA/magisterka/.venv/lib/python3.10/site-packages/scipy/ndimage/_interpolation.py:628\u001b[0m, in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    625\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[38;5;241m/\u001b[39mmatrix, output, order,\n\u001b[1;32m    626\u001b[0m                          mode, cval, npad, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 628\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometric_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datamodule = CTDataModule()\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
