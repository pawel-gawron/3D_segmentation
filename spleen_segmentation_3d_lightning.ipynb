{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3obUW0I7I0vL"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Spleen 3D segmentation with MONAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-YQ1gBbI0vL"
      },
      "source": [
        "This tutorial demonstrates how MONAI can be used in conjunction with the [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) framework.\n",
        "\n",
        "We demonstrate use of the following MONAI features:\n",
        "1. Transforms for dictionary format data.\n",
        "1. Loading Nifti images with metadata.\n",
        "1. Add channel dim to the data if no channel dimension.\n",
        "1. Scaling medical image intensity with expected range.\n",
        "1. Croping out a batch of balanced images based on  the positive / negative label ratio.\n",
        "1. Cache IO and transforms to accelerate training and validation.\n",
        "1. Use of a a 3D UNet model, Dice loss function, and mean Dice metric for a 3D segmentation task.\n",
        "1. The sliding window inference method.\n",
        "1. Deterministic training for reproducibility.\n",
        "\n",
        "The Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n",
        "\n",
        "![spleen](http://medicaldecathlon.com/img/spleen0.png)\n",
        "\n",
        "Target: Spleen  \n",
        "Modality: CT  \n",
        "Size: 61 3D volumes (41 Training + 20 Testing)  \n",
        "Source: Memorial Sloan Kettering Cancer Center  \n",
        "Challenge: Large ranging foreground size\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_lightning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3HtVqI0vM"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaMSdU8I0vM",
        "outputId": "fbf973c9-5926-4875-cda6-903844ab9f9b"
      },
      "outputs": [],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install -q pytorch-lightning~=2.0\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7oM0xTwI0vN"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7HEKxFEKI0vN",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.3.0\n",
            "Numpy version: 1.23.5\n",
            "Pytorch version: 2.0.0+cu117\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
            "MONAI __file__: /home/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "scipy version: 1.8.0\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.12.2\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: 0.15.1+cu117\n",
            "tqdm version: 4.65.0\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.7\n",
            "pandas version: 1.5.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        ")\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import nibabel as nib\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzzE3GGNI0vO"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1f_XjbPrI0vO",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmppuho5ada\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTzgAPbI0vP"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset.\n",
        "The dataset comes from http://medicaldecathlon.com/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/records/10069289/files/AeroPath.zip?download=1\"\n",
        "md5 = \"3fd5106c175c85d60eaece220f5dfd87\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"AeroPath.zip\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQVxGHhI0vQ"
      },
      "source": [
        "## Define the LightningModule\n",
        "\n",
        "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in `spleen_segmentation_3d.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_dice',\n",
        "    dirpath=os.path.join(root_dir, 'checkpoints'),  # Directory to save checkpoints\n",
        "    filename='best-checkpoint_whole_64',  # Filename prefix for saving checkpoints\n",
        "    save_top_k=1,  # Save only the best checkpoint\n",
        "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
        "    verbose=True,  # Log a message when saving the best checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y1-m7FfcI0vQ"
      },
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        )\n",
        "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # # set up the correct data path\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "        # train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "        # train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "        train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "        train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "\n",
        "                # user can also add other random transforms\n",
        "                #                 RandAffined(\n",
        "                #                     keys=['image', 'label'],\n",
        "                #                     mode=('bilinear', 'nearest'),\n",
        "                #                     prob=1.0,\n",
        "                #                     spatial_size=(96, 96, 96),\n",
        "                #                     rotate_range=(0, 0, np.pi/15),\n",
        "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        self.train_ds = CacheDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "        self.val_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "        self.infer_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "    #         self.train_ds = monai.data.Dataset(\n",
        "    #             data=train_files, transform=train_transforms)\n",
        "    #         self.val_ds = monai.data.Dataset(\n",
        "    #             data=val_files, transform=val_transforms)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "    \n",
        "    # def evaluate(self):\n",
        "    #     images, labels = self.infer_ds[\"image\"], self.infer_ds[\"label\"]\n",
        "\n",
        "    #     print(images)\n",
        "    #     roi_size = (160, 160, 160)\n",
        "    #     sw_batch_size = 4\n",
        "    #     outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "    #     loss = self.loss_function(outputs, labels)\n",
        "    #     outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    #     labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "    #     self.dice_metric(y_pred=outputs, y=labels)\n",
        "    #     d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "    #     # self.validation_step_outputs.append(d)\n",
        "\n",
        "    #     print(d)\n",
        "    #     return outputs\n",
        "    \n",
        "    # def evaluate(self):\n",
        "    #     infer_loader = DataLoader(self.infer_ds, batch_size=1, num_workers=4)\n",
        "    #     outputs = []\n",
        "\n",
        "    #     for batch in infer_loader:\n",
        "    #         images, labels = batch[\"image\"], batch[\"label\"]\n",
        "    #         roi_size = (160, 160, 160)\n",
        "    #         sw_batch_size = 4\n",
        "    #         output = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "    #         output = [self.post_pred(i) for i in decollate_batch(output)]\n",
        "    #         labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "\n",
        "    #         outputs.extend(output)\n",
        "        \n",
        "    #         loss = self.loss_function(outputs, labels)\n",
        "    #         d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "    #         print(d)\n",
        "\n",
        "    #     return outputs\n",
        "    # def evaluate(self):\n",
        "    #     # Get the images and labels from the inference dataset\n",
        "    #     dataset_images = [item['image'] for item in self.infer_ds]\n",
        "    #     dataset_labels = [item['label'] for item in self.infer_ds]\n",
        "\n",
        "    #     # Convert images and labels to tensors\n",
        "    #     images = torch.stack(dataset_images)\n",
        "    #     labels = torch.stack(dataset_labels)\n",
        "\n",
        "    #     # Perform inference\n",
        "    #     roi_size = (160, 160, 160)\n",
        "    #     sw_batch_size = 4\n",
        "    #     outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "    #     loss = self.loss_function(outputs, labels)\n",
        "    #     outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    #     labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "    #     self.dice_metric(y_pred=outputs, y=labels)\n",
        "    #     d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        \n",
        "    #     print(d)\n",
        "    #     return outputs\n",
        "    \n",
        "    def evaluate_single_model(self, model, file_paths):\n",
        "        # Load and preprocess NIfTI files\n",
        "        input_data = [self.load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "        # Perform inference using the model\n",
        "        model_output = [self.perform_inference(model, data) for data in input_data]\n",
        "\n",
        "        return model_output\n",
        "    \n",
        "    def load_and_preprocess_nifti(self, file_path):\n",
        "        img = nib.load(file_path)\n",
        "        data = img.get_fdata()\n",
        "        data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "        # Apply any preprocessing steps here, such as normalization\n",
        "        # Remember to ensure the data shape matches the input shape expected by your models\n",
        "        return data\n",
        "\n",
        "\n",
        "    \n",
        "    def perform_inference(self, model, data):\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "            model_output = model(data.unsqueeze(0))\n",
        "        return model_output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True) # log\n",
        "\n",
        "        return {\"log\": tensorboard_logs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = Net()\n",
        "\n",
        "net.prepare_data()\n",
        "net.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_overlapping_quadrants(image_shape, overlap_percentage=0.2):\n",
        "    x, y, z = image_shape\n",
        "    overlap_z = int(z * overlap_percentage)\n",
        "    if z < 3 * overlap_z:\n",
        "        return [(0, z)]\n",
        "    mid_z = z // 2\n",
        "    return [\n",
        "        (0, mid_z + overlap_z),\n",
        "        (mid_z - overlap_z, z)\n",
        "    ]\n",
        "\n",
        "def load_nifti_file(file_path):\n",
        "    return nib.load(file_path)\n",
        "\n",
        "def save_nifti_file(data, affine, file_path):\n",
        "    img = nib.Nifti1Image(data, affine)\n",
        "    nib.save(img, file_path)\n",
        "\n",
        "def process_file(file_path, output_dir):\n",
        "    img = load_nifti_file(file_path)\n",
        "    data = img.get_fdata()\n",
        "    affine = img.affine\n",
        "    quadrants = create_overlapping_quadrants(data.shape)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for i, (start, end) in enumerate(quadrants):\n",
        "        quadrant_data = data[:, :, start:end]\n",
        "        quadrant_file_path = os.path.join(output_dir, f\"quadrant_{i+1}_{os.path.basename(file_path)}\")\n",
        "        save_nifti_file(quadrant_data, affine, quadrant_file_path)\n",
        "        print(f\"Saved: {quadrant_file_path}\")\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "for idx, (image_name, label_name) in enumerate(zip(train_images, train_labels)):\n",
        "    output_dir = f'overlapping_quadrants/{idx}'\n",
        "    process_file(image_name, output_dir)\n",
        "    output_dir = f'overlapping_labels/{idx}'\n",
        "    process_file(label_name, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for img, label in zip (train_images, train_labels):\n",
        "    img = nib.load(img).get_fdata()\n",
        "    label = nib.load(label).get_fdata()\n",
        "\n",
        "    print(f'img shape: {img.shape}, label shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# quadrant1 = nib.load('quadrant1.nii.gz').get_fdata()\n",
        "# quadrant2 = nib.load('quadrant2.nii.gz').get_fdata()\n",
        "# quadrant3 = nib.load('quadrant3.nii.gz').get_fdata()\n",
        "# quadrant4 = nib.load('quadrant4.nii.gz').get_fdata()\n",
        "\n",
        "\n",
        "def plot_slice(data, title):\n",
        "    plt.figure()\n",
        "    plt.imshow(data[data.shape[0] // 2, :, :], cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# plot_slice(quadrant1, \"Quadrant 1\")\n",
        "# plot_slice(quadrant2, \"Quadrant 2\")\n",
        "# plot_slice(quadrant3, \"Quadrant 3\")\n",
        "# plot_slice(quadrant4, \"Quadrant 4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new = nib.load('overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz').get_fdata()\n",
        "\n",
        "plot_slice(new, 'new')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVCubC2I0vR"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8BpsB6sI0vR",
        "scrolled": false,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# initialise the LightningModule\n",
        "net = Net()\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TucnezFI0vR",
        "outputId": "4e0a75d6-eb09-4989-bb2b-a2c6b1388edf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f\"train completed, best_metric: {net.best_val_dice:.4f} \" f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "\n",
        "\n",
        "# Load the model weights from the checkpoint file\n",
        "checkpoint_path = 'best-checkpoint.ckpt'\n",
        "model = Net.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor(data)  \u001b[38;5;66;03m# Convert data to type Double\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/nets/unet.py:300\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/blocks/convolutions.py:316\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 316\u001b[0m     res: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# create the additive residual from x\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     cx: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)  \u001b[38;5;66;03m# apply x to sequence of operations\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cx \u001b[38;5;241m+\u001b[39m res\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]"
          ]
        }
      ],
      "source": [
        "# Instantiate the Net class\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "file_path = '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model inference (works)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_337381/2343696732.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data_dict[\"image\"]).unsqueeze(0).to(model.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input raw shape:  (1, 512, 512, 185)\n",
            "input shape:  torch.Size([1, 1, 512, 512, 185])\n",
            "output shape:  (2, 512, 512, 185)\n",
            "Inference complete.\n"
          ]
        }
      ],
      "source": [
        "# Model inference\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    Spacingd, ScaleIntensityRanged, CropForegroundd, Orientationd, EnsureTyped, Compose\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "\n",
        "# Load the trained model\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt')\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for new data\n",
        "infer_transforms = Compose(\n",
        "    [\n",
        "        Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
        "        # Spacingd(keys=\"image\", pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\")),\n",
        "        ScaleIntensityRanged(keys=\"image\", a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
        "        # CropForegroundd(keys=\"image\", source_key=\"image\"),\n",
        "        EnsureTyped(keys=\"image\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data\n",
        "file_path = 'overlapping_quadrants/18/quadrant_1_26_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "# Manually add the channel dimension\n",
        "data = data[None, ...]\n",
        "data_raw = data\n",
        "# Create a dictionary with the image\n",
        "data_dict = {\"image\": data}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = infer_transforms(data_dict)\n",
        "\n",
        "# Convert data to tensor and add batch dimension\n",
        "data = torch.tensor(data_dict[\"image\"]).unsqueeze(0).to(model.device)\n",
        "\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference\n",
        "predictions = infer_on_single_image(model, data)\n",
        "\n",
        "# Process and save the predictions as needed\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # Save or process each prediction here\n",
        "    # For example, save as NIfTI file\n",
        "    prediction_np = prediction.cpu().numpy()\n",
        "    pred_img = nib.Nifti1Image(prediction_np, img.affine)\n",
        "    # nib.save(pred_img, f\"prediction_{i}.nii.gz\")\n",
        "    nib.save(pred_img, \"prediction.nii.gz\")\n",
        "\n",
        "print(\"input raw shape: \", data_raw.shape)\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dice score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img shape: (512, 512, 536), label shape: (512, 512, 536)\n"
          ]
        }
      ],
      "source": [
        "label = nib.load('overlapping_labels/0/quadrant_1_1_CT_HR_label_airways.nii.gz')\n",
        "# image = nib.load('overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz')\n",
        "image = nib.load('prediction_0.nii.gz')\n",
        "\n",
        "label = label.get_fdata()\n",
        "image = image.get_fdata()\n",
        "\n",
        "image = image[0, :, :]\n",
        "\n",
        "print(f'img shape: {image.shape}, label shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score: 0.4997\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.transforms import AsDiscrete, EnsureType\n",
        "\n",
        "# Load the prediction and label NIfTI files\n",
        "prediction_path = \"prediction.nii.gz\"  # Update with your actual prediction file path\n",
        "label_path = \"overlapping_labels/18/quadrant_1_26_CT_HR_label_airways.nii.gz\"  # Update with your actual label file path\n",
        "# prediction_path = 'overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "prediction_img = nib.load(prediction_path)\n",
        "label_img = nib.load(label_path)\n",
        "\n",
        "# Get the data from the NIfTI images\n",
        "prediction_data = prediction_img.get_fdata()[0, :, :]\n",
        "label_data = label_img.get_fdata()\n",
        "\n",
        "# Ensure the data has the same shape and type\n",
        "assert prediction_data.shape == label_data.shape, \"Prediction and label must have the same shape\"\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "prediction_tensor = torch.tensor(prediction_data).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "label_tensor = torch.tensor(label_data).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "\n",
        "# Convert to binary using argmax if needed (assume binary classification for Dice score)\n",
        "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
        "\n",
        "prediction_tensor = post_pred(prediction_tensor)\n",
        "label_tensor = post_label(label_tensor)\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(f\"Dice Score: {dice_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'EnsureTyped' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define the transformations for new data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m infer_transforms \u001b[38;5;241m=\u001b[39m Compose(\n\u001b[1;32m     14\u001b[0m     [\n\u001b[1;32m     15\u001b[0m         EnsureChannelFirstd(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     16\u001b[0m         Orientationd(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, axcodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAS\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m         Spacingd(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, pixdim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m2.0\u001b[39m), mode\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     18\u001b[0m         ScaleIntensityRanged(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, a_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m57\u001b[39m, a_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m164\u001b[39m, b_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, b_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     19\u001b[0m         CropForegroundd(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m---> 20\u001b[0m         \u001b[43mEnsureTyped\u001b[49m(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load and preprocess the new data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EnsureTyped' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n",
            "\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor(data)  \u001b[38;5;66;03m# Convert data to type Double\u001b[39;00m\n",
            "\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "\u001b[0;32m---> 15\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
            "\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/nets/unet.py:300\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
            "\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n",
            "\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
            "\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/blocks/convolutions.py:316\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "\u001b[0;32m--> 316\u001b[0m     res: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# create the additive residual from x\u001b[39;00m\n",
            "\u001b[1;32m    317\u001b[0m     cx: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)  \u001b[38;5;66;03m# apply x to sequence of operations\u001b[39;00m\n",
            "\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cx \u001b[38;5;241m+\u001b[39m res\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n",
            "\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
            "\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n",
            "\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n",
            "\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n",
            "\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n",
            "\u001b[1;32m    607\u001b[0m     )\n",
            "\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n",
            "\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]"
          ]
        }
      ],
      "source": [
        "# Instantiate the Net class\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "file_path = '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the Net class\n",
        "model = Net()\n",
        "\n",
        "# Load the checkpoint for the single model\n",
        "checkpoint_path = 'checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint['state_dict'])  # Assuming 'state_dict' is the key for model state_dict in the checkpoint\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the paths to the NIfTI files\n",
        "file_paths = [\n",
        "    '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz',\n",
        "    # Add paths for other files if applicable\n",
        "]\n",
        "\n",
        "# Perform inference using the single model\n",
        "model_output = model.evaluate_single_model(model, file_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_nifti(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    # Apply any preprocessing steps here, such as normalization\n",
        "    # Remember to ensure the data shape matches the input shape expected by your models\n",
        "    return data\n",
        "\n",
        "\n",
        "model = Net().double()  # Ensure the model is set to accept Double type input\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Define the paths to the NIfTI files and load them\n",
        "input_data = [load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "# Convert input data to Double type\n",
        "input_data = [torch.DoubleTensor(data) for data in input_data]\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Ensure each data has a batch dimension\n",
        "    input_data = [data.unsqueeze(0) for data in input_data]\n",
        "    # Perform inference\n",
        "    model_output = [model(data) for data in input_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the paths to the checkpoints for each model\n",
        "checkpoint_paths = [\n",
        "    'checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt',\n",
        "    'checkpoints/best-checkpoint_new_method_2Q_spatial_size_48_0.7714 at epoch 584.ckpt'\n",
        "    # Add paths for other models if applicable\n",
        "]\n",
        "\n",
        "# Create instances of the Net class for each model\n",
        "models = []\n",
        "for checkpoint_path in checkpoint_paths:\n",
        "    # Initialize the model\n",
        "    model = Net()\n",
        "    \n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])  # Assuming 'state_dict' is the key for model state_dict in the checkpoint\n",
        "    \n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    model.prepare_data()\n",
        "    model.evaluate()\n",
        "    # Append the model to the list of models\n",
        "    models.append(model)\n",
        "\n",
        "    break\n",
        "# Now the models list contains instances of the Net class loaded with the trained weights from checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define a function to load and preprocess NIfTI files\n",
        "def load_and_preprocess_nifti(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    # Apply any preprocessing steps here, such as normalization\n",
        "    # Remember to ensure the data shape matches the input shape expected by your models\n",
        "    return torch.tensor(data).unsqueeze(0).float()  # Assuming input shape is (batch_size, channels, height, width, depth)\n",
        "\n",
        "# Define a function to perform inference on NIfTI data using the ensemble of models\n",
        "def perform_inference(models, input_data):\n",
        "    model_outputs = []\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(input_data)\n",
        "            model_outputs.append(output)\n",
        "    # Aggregate the outputs from different models\n",
        "    aggregated_output = torch.mean(torch.stack(model_outputs), dim=0)\n",
        "    return aggregated_output\n",
        "\n",
        "# Define the paths to the NIfTI files and load them\n",
        "file_paths = [\"path_to_file1.nii.gz\", \"path_to_file2.nii.gz\", ...]\n",
        "input_data = [load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "# Perform inference using the ensemble of models\n",
        "ensemble_output = [perform_inference(models, data) for data in input_data]\n",
        "\n",
        "# Post-process the output predictions if necessary\n",
        "# For example, you can convert the logits to probabilities using softmax\n",
        "\n",
        "# Save or visualize the results\n",
        "# You can save the output as NIfTI files or visualize them using plotting libraries like matplotlib or mayavi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0TIqDI0vR"
      },
      "source": [
        "## View training in tensorboard\n",
        "\n",
        "Please uncomment the following cell to load tensorboard results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dteU4yMVI0vR",
        "outputId": "8aa4354d-4d1a-4e57-d50e-d9a6eb88db5f"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDZoq7ZI0vS"
      },
      "source": [
        "## Check best model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pp1HWrqI0vS",
        "jupyter": {
          "outputs_hidden": true
        },
        "lines_to_next_cell": 2,
        "outputId": "c7388a09-ec21-44bb-9747-1f1c8d1f81f9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net.to(device)\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(net.val_dataloader()):\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, net)\n",
        "        # plot the slice [:, :, 80]\n",
        "        plt.figure(\"check\", (18, 6))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(f\"image {i}\")\n",
        "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(f\"label {i}\")\n",
        "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(f\"output {i}\")\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzE-kjOI0vS"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhZOGgOI0vS"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
