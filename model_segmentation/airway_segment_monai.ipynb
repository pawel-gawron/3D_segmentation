{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.0\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.2.1+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
      "MONAI __file__: /home/<username>/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.22.0\n",
      "scipy version: 1.12.0\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.17.1+cu121\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.1\n",
      "einops version: 0.7.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "/home/pawel/Documents/RISA/3D_segmentation/MONAI_DATA_DIRECTORY\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "print(directory)\n",
    "root_dir = Path(\"/home/pawel/Documents/RISA/3D_segmentation/MONAI_DATA_DIRECTORY\")\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task09_Spleen.tar: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task09_Spleen.tar: 1.50GB [01:41, 15.8MB/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 12:35:41,062 - INFO - Downloaded: /home/pawel/Documents/RISA/3D_segmentation/MONAI_DATA_DIRECTORY/Task09_Spleen.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 12:35:42,907 - INFO - Verified 'Task09_Spleen.tar', md5: 410d4a301da4e5b2f6f86ec3ddba524e.\n",
      "2024-03-25 12:35:42,908 - INFO - Writing into directory: /home/pawel/Documents/RISA/3D_segmentation/MONAI_DATA_DIRECTORY.\n"
     ]
    }
   ],
   "source": [
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, images_filepaths, transform=None):\n",
    "        self.images_filepaths = images_filepaths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.images_filepaths[idx]\n",
    "        image_file = np.load(str(image_filepath))\n",
    "\n",
    "        path_elements = list(Path(image_filepath).parts)\n",
    "        index = path_elements.index('scans')\n",
    "        path_elements[index] = 'airways'\n",
    "\n",
    "        mask_filepath = os.path.join(*path_elements)\n",
    "        mask_file = np.load(str(mask_filepath))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed_images = []\n",
    "            transformed_masks = []\n",
    "            for i in range(0, image_file.shape[-1]):\n",
    "                \n",
    "                image_slice = image_file[..., i]\n",
    "                mask_slice = mask_file[..., i]\n",
    "\n",
    "                transformed = self.transform(image=image_slice, mask=mask_slice)\n",
    "\n",
    "                transformed_images.append(transformed[\"image\"])\n",
    "                transformed_masks.append(transformed[\"mask\"])\n",
    "\n",
    "            image_file = torch.stack(transformed_images, dim=0)\n",
    "            mask_file = torch.stack(transformed_masks, dim=0).type(torch.float16)\n",
    "            mask_file = mask_file.unsqueeze(0)\n",
    "\n",
    "            image_file = image_file.permute(1, 0, 2, 3)\n",
    "\n",
    "        return image_file, mask_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmentations = A.Compose([\n",
    "        A.ToFloat(max_value=1024+400, always_apply=True),\n",
    "        A.Resize(height=64, width=64),\n",
    "        # A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.Normalize(mean=[-1024/400], std=[1/400], always_apply=True),\n",
    "        ToTensorV2()\n",
    "        ])\n",
    "        self.transforms = A.Compose([\n",
    "        A.ToFloat(max_value=1024+400, always_apply=True),\n",
    "        A.Resize(height=64, width=64),\n",
    "        # A.Normalize(mean=[-1024/400], std=[1/400], always_apply=True),\n",
    "        ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "        self.path_to_file = '/home/pawel/Documents/RISA/3D_segmentation/dataset'\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if os.path.exists(self.path_to_file):\n",
    "            print(\"Path exists\")\n",
    "            images_paths = sorted(Path(self.path_to_file).rglob('*.npy'))\n",
    "            # print(images_paths)\n",
    "            for image_path in images_paths:\n",
    "                image = np.load(str(image_path))\n",
    "\n",
    "                if image is None:\n",
    "                    print(\"Unlink image: \", image_path)\n",
    "                    image_path.unlink()\n",
    "        else:\n",
    "            print(\"Path does not exist\")\n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "        paths = sorted(Path(os.path.join(self.path_to_file, 'scans')).glob('*.npy'))\n",
    "\n",
    "        train_paths, val_paths = train_test_split(paths, test_size=0.3, random_state=42)\n",
    "        # val_paths, test_paths = train_test_split(val_paths, test_size=0.5, random_state=42)\n",
    "\n",
    "        self.train_dataset = CTDataset(train_paths, transform=self.augmentations)\n",
    "        self.val_dataset = CTDataset(val_paths, transform=self.transforms)\n",
    "        # self.test_dataset = CTDataset(test_paths, transform=self.transforms)\n",
    "\n",
    "        # return self.train_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # print(DataLoader(self.train_dataset, batch_size=1).shape)\n",
    "        return DataLoader(self.train_dataset, batch_size=5, num_workers=1)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=5, num_workers=1)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test_dataset, batch_size=5, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._model = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=2,\n",
    "            channels=(16, 32, 64, 128, 256),\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2,\n",
    "            norm=Norm.BATCH,\n",
    "        )\n",
    "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        # self.augmentations = A.Compose([\n",
    "        #     A.ToFloat(max_value=1024+400, always_apply=True),\n",
    "        #     A.Resize(height=64, width=64),\n",
    "        #     # A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        #     # A.RandomBrightnessContrast(p=0.5),\n",
    "        #     # A.Normalize(mean=[-1024/400], std=[1/400], always_apply=True),\n",
    "        #     ToTensorV2()\n",
    "        # ])\n",
    "        # self.transforms = A.Compose([\n",
    "        #     A.ToFloat(max_value=1024+400, always_apply=True),\n",
    "        #     A.Resize(height=64, width=64),\n",
    "        #     # A.Normalize(mean=[-1024/400], std=[1/400], always_apply=True),\n",
    "        #     ToTensorV2(),\n",
    "        # ])\n",
    "        self.path_to_file = '/home/pawel/Documents/RISA/3D_segmentation/dataset'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    # def prepare_data(self):\n",
    "    #     if os.path.exists(self.path_to_file):\n",
    "    #         print(\"Path exists\")\n",
    "    #         images_paths = sorted(Path(self.path_to_file).rglob('*.npy'))\n",
    "    #         # print(images_paths)\n",
    "    #         for image_path in images_paths:\n",
    "    #             image = np.load(str(image_path))\n",
    "\n",
    "    #             if image is None:\n",
    "    #                 print(\"Unlink image: \", image_path)\n",
    "    #                 image_path.unlink()\n",
    "    #     else:\n",
    "    #         print(\"Path does not exist\")\n",
    "\n",
    "    #     paths = sorted(Path(os.path.join(self.path_to_file, 'scans')).glob('*.npy'))\n",
    "\n",
    "    #     train_paths, val_paths = train_test_split(paths, test_size=0.3, random_state=42)\n",
    "\n",
    "    #     self.train_dataset = CTDataset(train_paths, transform=self.augmentations)\n",
    "    #     self.val_dataset = CTDataset(val_paths, transform=self.transforms)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # set up the correct data path\n",
    "        train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "        train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "        data_dicts = [\n",
    "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
    "        ]\n",
    "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=0)\n",
    "\n",
    "        # define the data transforms\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-57,\n",
    "                    a_max=164,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "                # randomly crop out patch samples from\n",
    "                # big image based on pos / neg ratio\n",
    "                # the image centers of negative samples\n",
    "                # must be in valid image area\n",
    "                RandCropByPosNegLabeld(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    label_key=\"label\",\n",
    "                    spatial_size=(96, 96, 96),\n",
    "                    pos=1,\n",
    "                    neg=1,\n",
    "                    num_samples=4,\n",
    "                    image_key=\"image\",\n",
    "                    image_threshold=0,\n",
    "                ),\n",
    "                # user can also add other random transforms\n",
    "                #                 RandAffined(\n",
    "                #                     keys=['image', 'label'],\n",
    "                #                     mode=('bilinear', 'nearest'),\n",
    "                #                     prob=1.0,\n",
    "                #                     spatial_size=(96, 96, 96),\n",
    "                #                     rotate_range=(0, 0, np.pi/15),\n",
    "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
    "            ]\n",
    "        )\n",
    "        val_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "                Spacingd(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    pixdim=(1.5, 1.5, 2.0),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                ),\n",
    "                ScaleIntensityRanged(\n",
    "                    keys=[\"image\"],\n",
    "                    a_min=-57,\n",
    "                    a_max=164,\n",
    "                    b_min=0.0,\n",
    "                    b_max=1.0,\n",
    "                    clip=True,\n",
    "                ),\n",
    "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # we use cached datasets - these are 10x faster than regular datasets\n",
    "        self.train_ds = CacheDataset(\n",
    "            data=train_files,\n",
    "            transform=train_transforms,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        self.val_ds = CacheDataset(\n",
    "            data=val_files,\n",
    "            transform=val_transforms,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=2,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        # print(\"DataLoader: \", train_loader)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # images, labels = batch\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        print(\"shapes of images and labels:\", type(images), type(labels))\n",
    "        print(\"shapes of images and labels:\", images.size(), labels.size())\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # images, labels = batch\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self.forward)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "        self.validation_step_outputs.append(d)\n",
    "        return d\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in self.validation_step_outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        tensorboard_logs = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_loss\": mean_val_loss,\n",
    "        }\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "        return {\"log\": tensorboard_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 32/32 [00:31<00:00,  1.03it/s]\n",
      "Loading dataset: 100%|██████████| 9/9 [00:06<00:00,  1.40it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | _model        | UNet     | 4.8 M \n",
      "1 | loss_function | DiceLoss | 0     \n",
      "-------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.236    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]current epoch: 0 current mean dice: 0.0275\n",
      "best mean dice: 0.0275 at epoch: 0\n",
      "Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s]                            shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:   6%|▋         | 1/16 [00:01<00:19,  0.78it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  12%|█▎        | 2/16 [00:01<00:10,  1.33it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  19%|█▉        | 3/16 [00:01<00:08,  1.57it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  25%|██▌       | 4/16 [00:02<00:06,  1.72it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  31%|███▏      | 5/16 [00:02<00:06,  1.83it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  38%|███▊      | 6/16 [00:03<00:05,  1.91it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  44%|████▍     | 7/16 [00:03<00:04,  1.97it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  50%|█████     | 8/16 [00:03<00:03,  2.01it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  56%|█████▋    | 9/16 [00:04<00:03,  2.05it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  62%|██████▎   | 10/16 [00:04<00:02,  2.08it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  69%|██████▉   | 11/16 [00:05<00:02,  2.11it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  75%|███████▌  | 12/16 [00:05<00:01,  2.13it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  81%|████████▏ | 13/16 [00:06<00:01,  2.15it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  88%|████████▊ | 14/16 [00:06<00:00,  2.17it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0:  94%|█████████▍| 15/16 [00:06<00:00,  2.18it/s, v_num=209]shapes of images and labels: <class 'monai.data.meta_tensor.MetaTensor'> <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "shapes of images and labels: torch.Size([8, 1, 96, 96, 96]) torch.Size([8, 1, 96, 96, 96])\n",
      "Epoch 0: 100%|██████████| 16/16 [00:07<00:00,  2.19it/s, v_num=209]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "net = Net()\n",
    "\n",
    "# data_module = CTDataModule()\n",
    "\n",
    "# set up loggers and checkpoints\n",
    "log_dir = os.path.join(root_dir, \"logs\")\n",
    "# tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    devices=[0],\n",
    "    max_epochs=600,\n",
    "    # logger=tb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    num_sanity_val_steps=1,\n",
    "    log_every_n_steps=16,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of images and labels: <class 'nibabel.nifti1.Nifti1Image'>\n",
      "shapes of images and labels: (512, 512, 90)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "img = nib.load('/tmp/tmpr8adzjh2/Task09_Spleen/imagesTr/spleen_2.nii.gz')\n",
    "\n",
    "print(\"type of images and labels:\", type(img))\n",
    "print(\"shapes of images and labels:\", img.shape)\n",
    "# print(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
