{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3obUW0I7I0vL"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Spleen 3D segmentation with MONAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-YQ1gBbI0vL"
      },
      "source": [
        "This tutorial demonstrates how MONAI can be used in conjunction with the [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) framework.\n",
        "\n",
        "We demonstrate use of the following MONAI features:\n",
        "1. Transforms for dictionary format data.\n",
        "1. Loading Nifti images with metadata.\n",
        "1. Add channel dim to the data if no channel dimension.\n",
        "1. Scaling medical image intensity with expected range.\n",
        "1. Croping out a batch of balanced images based on  the positive / negative label ratio.\n",
        "1. Cache IO and transforms to accelerate training and validation.\n",
        "1. Use of a a 3D UNet model, Dice loss function, and mean Dice metric for a 3D segmentation task.\n",
        "1. The sliding window inference method.\n",
        "1. Deterministic training for reproducibility.\n",
        "\n",
        "The Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n",
        "\n",
        "![spleen](http://medicaldecathlon.com/img/spleen0.png)\n",
        "\n",
        "Target: Spleen  \n",
        "Modality: CT  \n",
        "Size: 61 3D volumes (41 Training + 20 Testing)  \n",
        "Source: Memorial Sloan Kettering Cancer Center  \n",
        "Challenge: Large ranging foreground size\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_lightning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3HtVqI0vM"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaMSdU8I0vM",
        "outputId": "fbf973c9-5926-4875-cda6-903844ab9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: python: command not found\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: line 1: python: command not found\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install -q pytorch-lightning~=2.0\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7oM0xTwI0vN"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7HEKxFEKI0vN",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
            "2024-06-03 19:46:56.748680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.3.0\n",
            "Numpy version: 1.23.5\n",
            "Pytorch version: 2.0.0+cu117\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
            "MONAI __file__: /home/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "scipy version: 1.8.0\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.12.2\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: 0.15.1+cu117\n",
            "tqdm version: 4.65.0\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.7\n",
            "pandas version: 1.5.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        ")\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import nibabel as nib\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzzE3GGNI0vO"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1f_XjbPrI0vO",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmpl1i1sq2c\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTzgAPbI0vP"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset.\n",
        "The dataset comes from http://medicaldecathlon.com/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/records/10069289/files/AeroPath.zip?download=1\"\n",
        "md5 = \"3fd5106c175c85d60eaece220f5dfd87\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"AeroPath.zip\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQVxGHhI0vQ"
      },
      "source": [
        "## Define the LightningModule\n",
        "\n",
        "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in `spleen_segmentation_3d.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y1-m7FfcI0vQ"
      },
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        )\n",
        "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # # set up the correct data path\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "        # train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "        # train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "        train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "        train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "\n",
        "                # user can also add other random transforms\n",
        "                #                 RandAffined(\n",
        "                #                     keys=['image', 'label'],\n",
        "                #                     mode=('bilinear', 'nearest'),\n",
        "                #                     prob=1.0,\n",
        "                #                     spatial_size=(96, 96, 96),\n",
        "                #                     rotate_range=(0, 0, np.pi/15),\n",
        "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        self.train_ds = CacheDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "        self.val_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "        self.infer_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "    #         self.train_ds = monai.data.Dataset(\n",
        "    #             data=train_files, transform=train_transforms)\n",
        "    #         self.val_ds = monai.data.Dataset(\n",
        "    #             data=val_files, transform=val_transforms)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "    \n",
        "    def evaluate_single_model(self, model, file_paths):\n",
        "        # Load and preprocess NIfTI files\n",
        "        input_data = [self.load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "        # Perform inference using the model\n",
        "        model_output = [self.perform_inference(model, data) for data in input_data]\n",
        "\n",
        "        return model_output\n",
        "    \n",
        "    def load_and_preprocess_nifti(self, file_path):\n",
        "        img = nib.load(file_path)\n",
        "        data = img.get_fdata()\n",
        "        data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "        # Apply any preprocessing steps here, such as normalization\n",
        "        # Remember to ensure the data shape matches the input shape expected by your models\n",
        "        return data\n",
        "\n",
        "\n",
        "    \n",
        "    def perform_inference(self, model, data):\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "            model_output = model(data.unsqueeze(0))\n",
        "        return model_output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True) # log\n",
        "\n",
        "        return {\"log\": tensorboard_logs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_dice',\n",
        "    dirpath=os.path.join(root_dir, 'checkpoints'),  # Directory to save checkpoints\n",
        "    filename='best-checkpoint_whole_64',  # Filename prefix for saving checkpoints\n",
        "    save_top_k=1,  # Save only the best checkpoint\n",
        "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
        "    verbose=True,  # Log a message when saving the best checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create overlapping quadrants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape:  (512, 512, 767)\n",
            "Saved: nonoverlapping_quadrants/1/quadrant_1_1_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 383)\n",
            "Saved: nonoverlapping_quadrants/1/quadrant_2_1_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 384)\n",
            "input shape:  (512, 512, 767)\n",
            "Saved: nonoverlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 383)\n",
            "Saved: nonoverlapping_labels/1/quadrant_2_1_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 384)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 829)\n",
            "Saved: nonoverlapping_quadrants/2/quadrant_1_2_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 414)\n",
            "Saved: nonoverlapping_quadrants/2/quadrant_2_2_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 415)\n",
            "input shape:  (512, 512, 829)\n",
            "Saved: nonoverlapping_labels/2/quadrant_1_2_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 414)\n",
            "Saved: nonoverlapping_labels/2/quadrant_2_2_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 415)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 714)\n",
            "Saved: nonoverlapping_quadrants/3/quadrant_1_3_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 357)\n",
            "Saved: nonoverlapping_quadrants/3/quadrant_2_3_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 357)\n",
            "input shape:  (512, 512, 714)\n",
            "Saved: nonoverlapping_labels/3/quadrant_1_3_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 357)\n",
            "Saved: nonoverlapping_labels/3/quadrant_2_3_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 357)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 487, 598)\n",
            "Saved: nonoverlapping_quadrants/4/quadrant_1_4_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 487, 299)\n",
            "Saved: nonoverlapping_quadrants/4/quadrant_2_4_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 487, 299)\n",
            "input shape:  (487, 487, 598)\n",
            "Saved: nonoverlapping_labels/4/quadrant_1_4_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 487, 299)\n",
            "Saved: nonoverlapping_labels/4/quadrant_2_4_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 487, 299)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 619)\n",
            "Saved: nonoverlapping_quadrants/5/quadrant_1_5_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 309)\n",
            "Saved: nonoverlapping_quadrants/5/quadrant_2_5_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "input shape:  (512, 512, 619)\n",
            "Saved: nonoverlapping_labels/5/quadrant_1_5_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 309)\n",
            "Saved: nonoverlapping_labels/5/quadrant_2_5_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 441, 575)\n",
            "Saved: nonoverlapping_quadrants/6/quadrant_1_6_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 441, 287)\n",
            "Saved: nonoverlapping_quadrants/6/quadrant_2_6_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 441, 288)\n",
            "input shape:  (487, 441, 575)\n",
            "Saved: nonoverlapping_labels/6/quadrant_1_6_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 441, 287)\n",
            "Saved: nonoverlapping_labels/6/quadrant_2_6_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 441, 288)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_quadrants/7/quadrant_1_7_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_quadrants/7/quadrant_2_7_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_labels/7/quadrant_1_7_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_labels/7/quadrant_2_7_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 533)\n",
            "Saved: nonoverlapping_quadrants/8/quadrant_1_8_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 266)\n",
            "Saved: nonoverlapping_quadrants/8/quadrant_2_8_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 267)\n",
            "input shape:  (512, 512, 533)\n",
            "Saved: nonoverlapping_labels/8/quadrant_1_8_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 266)\n",
            "Saved: nonoverlapping_labels/8/quadrant_2_8_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 267)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 727)\n",
            "Saved: nonoverlapping_quadrants/9/quadrant_1_9_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 363)\n",
            "Saved: nonoverlapping_quadrants/9/quadrant_2_9_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 364)\n",
            "input shape:  (512, 512, 727)\n",
            "Saved: nonoverlapping_labels/9/quadrant_1_9_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 363)\n",
            "Saved: nonoverlapping_labels/9/quadrant_2_9_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 364)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 241)\n",
            "Saved: nonoverlapping_quadrants/10/quadrant_1_10_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 120)\n",
            "Saved: nonoverlapping_quadrants/10/quadrant_2_10_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 121)\n",
            "input shape:  (512, 512, 241)\n",
            "Saved: nonoverlapping_labels/10/quadrant_1_10_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 120)\n",
            "Saved: nonoverlapping_labels/10/quadrant_2_10_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 121)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 553)\n",
            "Saved: nonoverlapping_quadrants/11/quadrant_1_11_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 276)\n",
            "Saved: nonoverlapping_quadrants/11/quadrant_2_11_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 277)\n",
            "input shape:  (512, 512, 553)\n",
            "Saved: nonoverlapping_labels/11/quadrant_1_11_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 276)\n",
            "Saved: nonoverlapping_labels/11/quadrant_2_11_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 277)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 602)\n",
            "Saved: nonoverlapping_quadrants/12/quadrant_1_12_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 301)\n",
            "Saved: nonoverlapping_quadrants/12/quadrant_2_12_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 301)\n",
            "input shape:  (512, 512, 602)\n",
            "Saved: nonoverlapping_labels/12/quadrant_1_12_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 301)\n",
            "Saved: nonoverlapping_labels/12/quadrant_2_12_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 301)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 620)\n",
            "Saved: nonoverlapping_quadrants/13/quadrant_1_13_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 310)\n",
            "Saved: nonoverlapping_quadrants/13/quadrant_2_13_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "input shape:  (512, 512, 620)\n",
            "Saved: nonoverlapping_labels/13/quadrant_1_13_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 310)\n",
            "Saved: nonoverlapping_labels/13/quadrant_2_13_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 584)\n",
            "Saved: nonoverlapping_quadrants/14/quadrant_1_14_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 292)\n",
            "Saved: nonoverlapping_quadrants/14/quadrant_2_14_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 292)\n",
            "input shape:  (512, 512, 584)\n",
            "Saved: nonoverlapping_labels/14/quadrant_1_14_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 292)\n",
            "Saved: nonoverlapping_labels/14/quadrant_2_14_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 292)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 556)\n",
            "Saved: nonoverlapping_quadrants/15/quadrant_1_15_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 278)\n",
            "Saved: nonoverlapping_quadrants/15/quadrant_2_15_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 278)\n",
            "input shape:  (512, 512, 556)\n",
            "Saved: nonoverlapping_labels/15/quadrant_1_15_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 278)\n",
            "Saved: nonoverlapping_labels/15/quadrant_2_15_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 278)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 843)\n",
            "Saved: nonoverlapping_quadrants/16/quadrant_1_16_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 421)\n",
            "Saved: nonoverlapping_quadrants/16/quadrant_2_16_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 422)\n",
            "input shape:  (512, 512, 843)\n",
            "Saved: nonoverlapping_labels/16/quadrant_1_16_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 421)\n",
            "Saved: nonoverlapping_labels/16/quadrant_2_16_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 422)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 407, 602)\n",
            "Saved: nonoverlapping_quadrants/17/quadrant_1_17_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 407, 301)\n",
            "Saved: nonoverlapping_quadrants/17/quadrant_2_17_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 407, 301)\n",
            "input shape:  (487, 407, 602)\n",
            "Saved: nonoverlapping_labels/17/quadrant_1_17_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 407, 301)\n",
            "Saved: nonoverlapping_labels/17/quadrant_2_17_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 407, 301)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 706)\n",
            "Saved: nonoverlapping_quadrants/18/quadrant_1_18_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 353)\n",
            "Saved: nonoverlapping_quadrants/18/quadrant_2_18_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 353)\n",
            "input shape:  (512, 512, 706)\n",
            "Saved: nonoverlapping_labels/18/quadrant_1_18_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 353)\n",
            "Saved: nonoverlapping_labels/18/quadrant_2_18_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 353)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 666)\n",
            "Saved: nonoverlapping_quadrants/19/quadrant_1_19_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 333)\n",
            "Saved: nonoverlapping_quadrants/19/quadrant_2_19_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 333)\n",
            "input shape:  (512, 512, 666)\n",
            "Saved: nonoverlapping_labels/19/quadrant_1_19_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 333)\n",
            "Saved: nonoverlapping_labels/19/quadrant_2_19_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 333)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 286)\n",
            "Saved: nonoverlapping_quadrants/20/quadrant_1_20_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 143)\n",
            "Saved: nonoverlapping_quadrants/20/quadrant_2_20_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 143)\n",
            "input shape:  (512, 512, 286)\n",
            "Saved: nonoverlapping_labels/20/quadrant_1_20_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 143)\n",
            "Saved: nonoverlapping_labels/20/quadrant_2_20_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 143)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 717)\n",
            "Saved: nonoverlapping_quadrants/21/quadrant_1_21_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 358)\n",
            "Saved: nonoverlapping_quadrants/21/quadrant_2_21_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 359)\n",
            "input shape:  (512, 512, 717)\n",
            "Saved: nonoverlapping_labels/21/quadrant_1_21_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 358)\n",
            "Saved: nonoverlapping_labels/21/quadrant_2_21_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 359)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 329)\n",
            "Saved: nonoverlapping_quadrants/22/quadrant_1_22_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_quadrants/22/quadrant_2_22_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 165)\n",
            "input shape:  (512, 512, 329)\n",
            "Saved: nonoverlapping_labels/22/quadrant_1_22_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_labels/22/quadrant_2_22_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 165)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 694)\n",
            "Saved: nonoverlapping_quadrants/23/quadrant_1_23_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 347)\n",
            "Saved: nonoverlapping_quadrants/23/quadrant_2_23_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 347)\n",
            "input shape:  (512, 512, 694)\n",
            "Saved: nonoverlapping_labels/23/quadrant_1_23_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 347)\n",
            "Saved: nonoverlapping_labels/23/quadrant_2_23_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 347)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 607)\n",
            "Saved: nonoverlapping_quadrants/24/quadrant_1_24_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 303)\n",
            "Saved: nonoverlapping_quadrants/24/quadrant_2_24_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 304)\n",
            "input shape:  (512, 512, 607)\n",
            "Saved: nonoverlapping_labels/24/quadrant_1_24_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 303)\n",
            "Saved: nonoverlapping_labels/24/quadrant_2_24_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 304)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 328)\n",
            "Saved: nonoverlapping_quadrants/25/quadrant_1_25_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_quadrants/25/quadrant_2_25_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 164)\n",
            "input shape:  (512, 512, 328)\n",
            "Saved: nonoverlapping_labels/25/quadrant_1_25_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_labels/25/quadrant_2_25_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 164)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 265)\n",
            "Saved: nonoverlapping_quadrants/26/quadrant_1_26_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 132)\n",
            "Saved: nonoverlapping_quadrants/26/quadrant_2_26_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 133)\n",
            "input shape:  (512, 512, 265)\n",
            "Saved: nonoverlapping_labels/26/quadrant_1_26_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 132)\n",
            "Saved: nonoverlapping_labels/26/quadrant_2_26_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 133)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_quadrants/27/quadrant_1_27_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_quadrants/27/quadrant_2_27_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_labels/27/quadrant_1_27_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_labels/27/quadrant_2_27_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "\n",
            "###########\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "def create_overlapping_quadrants(image_shape, overlap_percentage=0.0):\n",
        "    x, y, z = image_shape\n",
        "    overlap_z = int(z * overlap_percentage)\n",
        "    if z < 3 * overlap_z:\n",
        "        return [(0, z)]\n",
        "    mid_z = z // 2\n",
        "    return [\n",
        "        (0, mid_z + overlap_z),\n",
        "        (mid_z - overlap_z, z)\n",
        "    ]\n",
        "\n",
        "def load_nifti_file(file_path):\n",
        "    return nib.load(file_path)\n",
        "\n",
        "def save_nifti_file(data, affine, file_path):\n",
        "    img = nib.Nifti1Image(data, affine)\n",
        "    nib.save(img, file_path)\n",
        "\n",
        "def process_file(file_path, output_dir):\n",
        "    img = load_nifti_file(file_path)\n",
        "    data = img.get_fdata()\n",
        "    print(\"input shape: \", data.shape)\n",
        "    affine = img.affine\n",
        "    quadrants = create_overlapping_quadrants(data.shape)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for i, (start, end) in enumerate(quadrants):\n",
        "        quadrant_data = data[:, :, start:end]\n",
        "        quadrant_file_path = os.path.join(output_dir, f\"quadrant_{i+1}_{os.path.basename(file_path)}\")\n",
        "        save_nifti_file(quadrant_data, affine, quadrant_file_path)\n",
        "        print(f\"Saved: {quadrant_file_path}\")\n",
        "        print(f'Quadrant {i} shape: {quadrant_data.shape}')\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "train_labels = natsorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "train_images = natsorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "# for idx, (image_name, label_name) in enumerate(zip(train_images, train_labels)):\n",
        "#     output_dir = f'nonoverlapping_quadrants/{idx}'\n",
        "#     process_file(image_name, output_dir)\n",
        "#     output_dir = f'nonoverlapping_labels/{idx}'\n",
        "#     process_file(label_name, output_dir)\n",
        "\n",
        "\n",
        "\n",
        "for idx, (image_name, label_name) in enumerate(zip(train_images, train_labels)):\n",
        "    output_dir = f'nonoverlapping_quadrants/{idx + 1}'\n",
        "    process_file(image_name, output_dir)\n",
        "    output_dir = f'nonoverlapping_labels/{idx + 1}'\n",
        "    process_file(label_name, output_dir)\n",
        "    \n",
        "    print('')\n",
        "    print('###########')\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/1/1_CT_HR.nii.gz\n",
            "1\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/2/2_CT_HR.nii.gz\n",
            "2\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/3/3_CT_HR.nii.gz\n",
            "3\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/4/4_CT_HR.nii.gz\n",
            "4\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/5/5_CT_HR.nii.gz\n",
            "5\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/6/6_CT_HR.nii.gz\n",
            "6\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/7/7_CT_HR.nii.gz\n",
            "7\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/8/8_CT_HR.nii.gz\n",
            "8\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/9/9_CT_HR.nii.gz\n",
            "9\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/10/10_CT_HR.nii.gz\n",
            "10\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/11/11_CT_HR.nii.gz\n",
            "11\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/12/12_CT_HR.nii.gz\n",
            "12\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/13/13_CT_HR.nii.gz\n",
            "13\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/14/14_CT_HR.nii.gz\n",
            "14\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/15/15_CT_HR.nii.gz\n",
            "15\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/16/16_CT_HR.nii.gz\n",
            "16\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/17/17_CT_HR.nii.gz\n",
            "17\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/18/18_CT_HR.nii.gz\n",
            "18\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/19/19_CT_HR.nii.gz\n",
            "19\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/20/20_CT_HR.nii.gz\n",
            "20\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/21/21_CT_HR.nii.gz\n",
            "21\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/22/22_CT_HR.nii.gz\n",
            "22\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/23/23_CT_HR.nii.gz\n",
            "23\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/24/24_CT_HR.nii.gz\n",
            "24\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/25/25_CT_HR.nii.gz\n",
            "25\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/26/26_CT_HR.nii.gz\n",
            "26\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/27/27_CT_HR.nii.gz\n"
          ]
        }
      ],
      "source": [
        "from natsort import natsorted\n",
        "import glob\n",
        "\n",
        "# Assuming 'pattern' is defined\n",
        "pattern = \"/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/*/*_CT_HR.nii.gz\"\n",
        "\n",
        "# Get the list of files\n",
        "train_images = glob.glob(pattern, recursive=True)\n",
        "\n",
        "# Sort the files naturally\n",
        "sorted_train_images = natsorted(train_images)\n",
        "\n",
        "# Print the sorted list\n",
        "for idx, image in enumerate(sorted_train_images):\n",
        "    print(idx)\n",
        "    print(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/1/1_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/10/10_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/11/11_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/12/12_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/13/13_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/14/14_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/15/15_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/16/16_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/17/17_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/18/18_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/19/19_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/2/2_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/20/20_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/21/21_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/22/22_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/23/23_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/24/24_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/25/25_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/26/26_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/27/27_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/3/3_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/4/4_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/5/5_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/6/6_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/7/7_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/8/8_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/9/9_CT_HR.nii.gz']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "train_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img shape: (512, 512, 362), label shape: (512, 512, 362)\n"
          ]
        }
      ],
      "source": [
        "img = 'nonoverlapping_quadrants/24/quadrant_2_7_CT_HR.nii.gz'\n",
        "label = 'nonoverlapping_labels/24/quadrant_2_7_CT_HR_label_airways.nii.gz'\n",
        "img = nib.load(img).get_fdata()\n",
        "label = nib.load(label).get_fdata()\n",
        "\n",
        "print(f'img shape: {img.shape}, label shape: {label.shape}')\n",
        "\n",
        "\n",
        "# whole\n",
        "# img shape: (512, 512, 723), label shape: (512, 512, 723)\n",
        "\n",
        "# 1q\n",
        "# img shape: (512, 512, 361), label shape: (512, 512, 361)\n",
        "\n",
        "# 2q\n",
        "# img shape: (512, 512, 362), label shape: (512, 512, 362)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for img, label in zip (train_images, train_labels):\n",
        "    img = nib.load(img).get_fdata()\n",
        "    label = nib.load(label).get_fdata()\n",
        "\n",
        "    print(f'img shape: {img.shape}, label shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# quadrant1 = nib.load('quadrant1.nii.gz').get_fdata()\n",
        "# quadrant2 = nib.load('quadrant2.nii.gz').get_fdata()\n",
        "# quadrant3 = nib.load('quadrant3.nii.gz').get_fdata()\n",
        "# quadrant4 = nib.load('quadrant4.nii.gz').get_fdata()\n",
        "\n",
        "\n",
        "def plot_slice(data, title):\n",
        "    plt.figure()\n",
        "    plt.imshow(data[data.shape[0] // 2, :, :], cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# plot_slice(quadrant1, \"Quadrant 1\")\n",
        "# plot_slice(quadrant2, \"Quadrant 2\")\n",
        "# plot_slice(quadrant3, \"Quadrant 3\")\n",
        "# plot_slice(quadrant4, \"Quadrant 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVCubC2I0vR"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8BpsB6sI0vR",
        "scrolled": false,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# initialise the LightningModule\n",
        "net = Net()\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TucnezFI0vR",
        "outputId": "4e0a75d6-eb09-4989-bb2b-a2c6b1388edf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f\"train completed, best_metric: {net.best_val_dice:.4f} \" f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "\n",
        "\n",
        "# Load the model weights from the checkpoint file\n",
        "checkpoint_path = 'best-checkpoint.ckpt'\n",
        "model = Net.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor(data)  \u001b[38;5;66;03m# Convert data to type Double\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/nets/unet.py:300\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/blocks/convolutions.py:316\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 316\u001b[0m     res: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# create the additive residual from x\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     cx: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)  \u001b[38;5;66;03m# apply x to sequence of operations\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cx \u001b[38;5;241m+\u001b[39m res\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]"
          ]
        }
      ],
      "source": [
        "# Instantiate the Net class\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "file_path = '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model inference (works)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7c4230bad900>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/io/dictionary.py:162\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_key_postfix):\n\u001b[0;32m--> 162\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loader\u001b[38;5;241m.\u001b[39mimage_only:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/io/array.py:245\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[0;34m(self, filename, reader)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mLoad image file and metadata from the given filename(s).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mIf `reader` is not specified, this class automatically chooses readers based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mensure_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# allow Path objects\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m img, err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/io/array.py:246\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mLoad image file and metadata from the given filename(s).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mIf `reader` is not specified, this class automatically chooses readers based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpanduser \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m ensure_tuple(filename)  \u001b[38;5;66;03m# allow Path objects\u001b[39;00m\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    248\u001b[0m img, err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, []\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:960\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:594\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:578\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not ndarray",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: data}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43minfer_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Convert data to tensor and add batch dimension\u001b[39;00m\n\u001b[1;32m     51\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7c4230bad900>"
          ]
        }
      ],
      "source": [
        "# Model inference\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    Spacingd, ScaleIntensityRanged, CropForegroundd, Orientationd, EnsureTyped, Compose\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "\n",
        "# Load the trained model\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt')\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for new data\n",
        "infer_transforms = Compose(\n",
        "    [\n",
        "        Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
        "        # Spacingd(keys=\"image\", pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\")),\n",
        "        ScaleIntensityRanged(keys=\"image\", a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
        "        # CropForegroundd(keys=\"image\", source_key=\"image\"),\n",
        "        EnsureTyped(keys=\"image\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data\n",
        "file_path = 'overlapping_quadrants/18/quadrant_1_26_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "# Manually add the channel dimension\n",
        "data = data[None, ...]\n",
        "data_raw = data\n",
        "# Create a dictionary with the image\n",
        "data_dict = {\"image\": data}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = infer_transforms(data_dict)\n",
        "\n",
        "# Convert data to tensor and add batch dimension\n",
        "data = torch.tensor(data_dict[\"image\"]).unsqueeze(0).to(model.device)\n",
        "\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference\n",
        "predictions = infer_on_single_image(model, data)\n",
        "\n",
        "# Process and save the predictions as needed\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # Save or process each prediction here\n",
        "    # For example, save as NIfTI file\n",
        "    prediction_np = prediction.cpu().numpy()\n",
        "    pred_img = nib.Nifti1Image(prediction_np, img.affine)\n",
        "    # nib.save(pred_img, f\"prediction_{i}.nii.gz\")\n",
        "    nib.save(pred_img, \"prediction.nii.gz\")\n",
        "\n",
        "print(\"input raw shape: \", data_raw.shape)\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/tmp/ipykernel_81960/684908623.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(model.device)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'img' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Save or process each prediction here\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# For example, save as NIfTI file\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     prediction_np \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 59\u001b[0m     pred_img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mNifti1Image(prediction_np, \u001b[43mimg\u001b[49m\u001b[38;5;241m.\u001b[39maffine)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# nib.save(pred_img, f\"prediction_{i}.nii.gz\")\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     nib\u001b[38;5;241m.\u001b[39msave(pred_img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
          ]
        }
      ],
      "source": [
        "# Model inference\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, EnsureTyped, Compose\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "\n",
        "# Load the trained model\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt')\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "infer_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=\"image\"),\n",
        "        EnsureChannelFirstd(keys=\"image\"),\n",
        "        Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
        "        Spacingd(keys=\"image\", pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\")),\n",
        "        ScaleIntensityRanged(keys=\"image\", a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=\"image\", source_key=\"image\"),\n",
        "        EnsureTyped(keys=\"image\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data\n",
        "file_path = 'overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "data_dict = {\"image\": file_path}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = infer_transforms(data_dict)\n",
        "data = data_dict[\"image\"]\n",
        "\n",
        "data_raw = data\n",
        "# Convert data to tensor and add batch dimension\n",
        "data = torch.tensor(data).unsqueeze(0).to(model.device)\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference\n",
        "predictions = infer_on_single_image(model, data)\n",
        "\n",
        "# Process and save the predictions as needed\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # Save or process each prediction here\n",
        "    # For example, save as NIfTI file\n",
        "    prediction_np = prediction.cpu().numpy()\n",
        "    pred_img = nib.Nifti1Image(prediction_np, img.affine)\n",
        "    # nib.save(pred_img, f\"prediction_{i}.nii.gz\")\n",
        "    nib.save(pred_img, \"prediction.nii.gz\")\n",
        "\n",
        "print(\"input raw shape: \", data_raw.shape)\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dice score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img shape: (234, 205, 135), label shape: (512, 512, 536)\n"
          ]
        }
      ],
      "source": [
        "label = nib.load('overlapping_labels/0/quadrant_1_1_CT_HR_label_airways.nii.gz')\n",
        "# image = nib.load('overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz')\n",
        "image = nib.load('prediction.nii.gz')\n",
        "\n",
        "label = label.get_fdata()\n",
        "image = image.get_fdata()\n",
        "\n",
        "image = image[0, :, :]\n",
        "\n",
        "print(f'img shape: {image.shape}, label shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_121231/3556753813.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(device)\n",
            "/tmp/ipykernel_121231/3556753813.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input nii gz shape:  (512, 512, 241)\n",
            "input shape:  torch.Size([1, 1, 240, 240, 151])\n",
            "input shape:  torch.Size([1, 1, 240, 240, 151])\n",
            "output shape:  (2, 240, 240, 151)\n",
            "Dice Score: 0.9864\n",
            "Inference complete.\n",
            "output nii gz shape:  (2, 259, 259, 182)\n"
          ]
        }
      ],
      "source": [
        "# Model inference and Dice score calculation\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, EnsureTyped, Compose, AsDiscrete\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "# Load the trained model\n",
        "# model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_64_0.6679 at epoch: 498.ckpt')\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_2Q_spatial_size_64_0.8143 at epoch 499.ckpt')\n",
        "# moel = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data and the label\n",
        "\n",
        "# # 1Q\n",
        "# file_path = 'nonoverlapping_quadrants/7/quadrant_1_7_CT_HR.nii.gz'\n",
        "# label_path = 'nonoverlapping_labels/7/quadrant_1_7_CT_HR_label_airways.nii.gz'  # Update with your actual label file path\n",
        "\n",
        "# 2Q\n",
        "# file_path = 'nonoverlapping_quadrants/7/quadrant_2_7_CT_HR.nii.gz'\n",
        "# label_path = 'nonoverlapping_labels/7/quadrant_2_7_CT_HR_label_airways.nii.gz'  # Update with your actual label file path\n",
        "\n",
        "# # WHOLE\n",
        "file_path = 'AeroPath/7/7_CT_HR_label_lungs.nii.gz'\n",
        "label_path = 'AeroPath/7/7_CT_HR_label_lungs.nii.gz'\n",
        "\n",
        "data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = common_transforms(data_dict)\n",
        "data = data_dict[\"image\"]\n",
        "label = data_dict[\"label\"]\n",
        "\n",
        "# Convert data and label to tensors and add batch dimension\n",
        "\n",
        "\n",
        "data = torch.tensor(data).unsqueeze(0).to(device)\n",
        "label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "data_nii = nib.load(file_path).get_fdata()\n",
        "label_nii = nib.load(label_path).get_fdata()\n",
        "\n",
        "print(\"input nii gz shape: \", data_nii.shape)\n",
        "print(\"input shape: \", data.shape)\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (64, 64, 64)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference\n",
        "predictions = infer_on_single_image(model, data)\n",
        "\n",
        "# Process and save the predictions as needed\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # Save or process each prediction here\n",
        "    # For example, save as NIfTI file\n",
        "    prediction_np = prediction.cpu().numpy()\n",
        "    pred_img = nib.Nifti1Image(prediction_np, nib.load(file_path).affine)\n",
        "    nib.save(pred_img, \"whole_prediction.nii.gz\")\n",
        "\n",
        "# Convert predictions and labels to binary format if necessary\n",
        "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "prediction_tensor = post_pred(prediction.to(device))\n",
        "label_tensor = post_label(label.to(device))\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(f\"Dice Score: {dice_score:.4f}\")\n",
        "print(\"Inference complete.\")\n",
        "\n",
        "pred_nii = nib.load('prediction.nii.gz').get_fdata()\n",
        "\n",
        "print(\"output nii gz shape: \", pred_nii.shape)\n",
        "\n",
        "#NONOVERLAPPING\n",
        "# 1Q\n",
        "# input nii gz shape:  (512, 512, 361)\n",
        "# input shape:  torch.Size([1, 1, 259, 192, 91])\n",
        "# input shape:  torch.Size([1, 1, 259, 192, 91])\n",
        "# output shape:  (2, 259, 192, 91)\n",
        "# Dice Score: 0.5217\n",
        "\n",
        "# 2Q\n",
        "# input nii gz shape:  (512, 512, 362)\n",
        "# input shape:  torch.Size([1, 1, 259, 188, 91])\n",
        "# input shape:  torch.Size([1, 1, 259, 188, 91])\n",
        "# output shape:  (2, 259, 188, 91)\n",
        "# Dice Score: 0.6679\n",
        "\n",
        "# WHOLE\n",
        "# input nii gz shape:  (512, 512, 723)\n",
        "# input shape:  torch.Size([1, 1, 259, 259, 182])\n",
        "# input shape:  torch.Size([1, 1, 259, 259, 182])\n",
        "# output shape:  (2, 259, 259, 182)\n",
        "# Dice Score: 0.9771\n",
        "\n",
        "# OVERLAPPING\n",
        "# 1Q\n",
        "# input shape:  torch.Size([1, 1, 259, 194, 127])\n",
        "# output shape:  (2, 259, 194, 127)\n",
        "# Dice Score: 0.5829\n",
        "\n",
        "# 2Q\n",
        "# input shape:  torch.Size([1, 1, 259, 190, 127])\n",
        "# output shape:  (2, 259, 190, 127)\n",
        "# Dice Score: 0.6390\n",
        "\n",
        "#NONOVERLAPPING WITHOUT SPACINGD\n",
        "# 1Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolate ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1 shape: (2, 259, 192, 91)\n",
            "Q2 shape: (2, 259, 188, 91)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 192 and the array at index 1 has size 188",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ1 shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQ1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ2 shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQ2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m Q1_new \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ1_new shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQ1_new\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 192 and the array at index 1 has size 188"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "Q1 = nib.load('1Qprediction.nii.gz').get_fdata()\n",
        "Q2 = nib.load('2Qprediction.nii.gz').get_fdata()\n",
        "\n",
        "print(f'Q1 shape: {Q1.shape}')\n",
        "print(f'Q2 shape: {Q2.shape}')\n",
        "\n",
        "Q1_new = np.concatenate((Q1, Q2), axis=3)\n",
        "\n",
        "print(f'Q1_new shape: {Q1_new.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "merged: (2, 240, 240, 151)\n",
            "whole: (2, 240, 240, 151)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Define a function to interpolate predictions to match the dimensions of the WHOLE model\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Extract the original shape of the predictions\n",
        "    original_shape = predictions.shape\n",
        "    # Compute the scaling factors for interpolation along each axis\n",
        "    scale_factors = [t / o for t, o in zip(target_shape, original_shape)]\n",
        "    # Interpolate the predictions using zoom\n",
        "    interpolated_predictions = zoom(predictions, zoom=scale_factors, mode='nearest')\n",
        "    return interpolated_predictions\n",
        "\n",
        "# Assuming you have predictions from both 1Q and 2Q models\n",
        "predictions_1Q = nib.load('1Qprediction.nii.gz').get_fdata()  # Get predictions from 1Q model\n",
        "predictions_2Q = nib.load('2Qprediction.nii.gz').get_fdata()  # Get predictions from 2Q model\n",
        "\n",
        "whole = nib.load('whole_prediction.nii.gz').get_fdata()\n",
        "\n",
        "\n",
        "# Assuming target shape is the shape of the WHOLE model\n",
        "target_shape = (2, 259, 259, 91)  # Adjust as necessary\n",
        "target_shape = (2, 240, 240, 75)  # Adjust as necessary\n",
        "\n",
        "# Interpolate predictions from 1Q and 2Q models to match the target shape\n",
        "interpolated_predictions_1Q = interpolate_predictions(predictions_1Q, target_shape)\n",
        "target_shape = (2, 240, 240, 76)  # Adjust as necessary\n",
        "\n",
        "interpolated_predictions_2Q = interpolate_predictions(predictions_2Q, target_shape)\n",
        "\n",
        "# Combine predictions using averaging or another suitable method\n",
        "\n",
        "\n",
        "merged =np.concatenate((interpolated_predictions_1Q, interpolated_predictions_2Q), axis=3)\n",
        "\n",
        "print(f'merged: {merged.shape}')\n",
        "print(f'whole: {whole.shape}')\n",
        "\n",
        "ensemble_mean = (merged + whole) / 2  # Averaging for ensemble\n",
        "ensemble_maximum = np.maximum(merged,whole)\n",
        "\n",
        "# Now you have the ensemble predictions with the shape of the WHOLE model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9864209294319153\n",
            "0.9864209294319153\n"
          ]
        }
      ],
      "source": [
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "prediction = torch.tensor(merged)\n",
        "\n",
        "prediction_tensor = post_pred(prediction.to(device))\n",
        "label_tensor = post_label(label.to(device))\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(dice_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "merged = torch.tensor((np.maximum(merged,whole)))\n",
        "\n",
        "prediction_tensor = post_pred(merged.to(device))\n",
        "label_tensor = post_label(label.to(device))\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(dice_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_364452/43230831.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).unsqueeze(0).to(device)\n",
            "/tmp/ipykernel_364452/43230831.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Sequence must have length 4, got 3.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m ensemble_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 80\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43minfer_on_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     ensemble_predictions\u001b[38;5;241m.\u001b[39mappend(predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Assuming single prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Average the predictions (or apply other ensembling techniques)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[37], line 72\u001b[0m, in \u001b[0;36minfer_on_single_image\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     70\u001b[0m     roi_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m)\n\u001b[1;32m     71\u001b[0m     sw_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43msliding_window_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msw_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     post_processed_outputs \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mpost_pred(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m decollate_batch(outputs)]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m post_processed_outputs\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/inferers/utils.py:161\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, with_coord, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     temp_meta \u001b[38;5;241m=\u001b[39m MetaTensor([])\u001b[38;5;241m.\u001b[39mcopy_meta_from(inputs, copy_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    160\u001b[0m inputs \u001b[38;5;241m=\u001b[39m convert_data_type(inputs, torch\u001b[38;5;241m.\u001b[39mTensor, wrap_sequence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m roi_size \u001b[38;5;241m=\u001b[39m \u001b[43mfall_back_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# in case that image size is smaller than roi size\u001b[39;00m\n\u001b[1;32m    164\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmax\u001b[39m(image_size_[i], roi_size[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_spatial_dims))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/utils/misc.py:279\u001b[0m, in \u001b[0;36mfall_back_tuple\u001b[0;34m(user_provided, default, func)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mRefine `user_provided` according to the `default`, and returns as a validated tuple.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(default)\n\u001b[0;32m--> 279\u001b[0m user \u001b[38;5;241m=\u001b[39m \u001b[43mensure_tuple_rep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_provided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(  \u001b[38;5;66;03m# use the default values if user provided is not valid\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     user_c \u001b[38;5;28;01mif\u001b[39;00m func(user_c) \u001b[38;5;28;01melse\u001b[39;00m default_c \u001b[38;5;28;01mfor\u001b[39;00m default_c, user_c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(default, user)\n\u001b[1;32m    282\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/utils/misc.py:205\u001b[0m, in \u001b[0;36mensure_tuple_rep\u001b[0;34m(tup, dim)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tup) \u001b[38;5;241m==\u001b[39m dim:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tup)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence must have length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tup)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Sequence must have length 4, got 3."
          ]
        }
      ],
      "source": [
        "# Model inference and Dice score calculation\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, EnsureTyped, Compose, AsDiscrete\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "from monai.metrics import DiceMetric\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained models (add more as needed for ensembling)\n",
        "models = [\n",
        "    Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt'),\n",
        "    # Net.load_from_checkpoint('checkpoints/another_checkpoint.ckpt')\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    model.eval()\n",
        "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data and the label\n",
        "file_path = 'overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "label_path = 'overlapping_labels/0/quadrant_1_1_CT_HR_label_airways.nii.gz'  # Update with your actual label file path\n",
        "data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = common_transforms(data_dict)\n",
        "data = data_dict[\"image\"]\n",
        "label = data_dict[\"label\"]\n",
        "\n",
        "# Save the original shape for padding\n",
        "original_shape = data.shape[1:]  # exclude batch dimension\n",
        "\n",
        "# Convert data and label to tensors and add batch dimension\n",
        "data = torch.tensor(data).unsqueeze(0).unsqueeze(0).to(device)\n",
        "label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference on all models and aggregate the predictions\n",
        "ensemble_predictions = []\n",
        "\n",
        "for model in models:\n",
        "    predictions = infer_on_single_image(model, data)\n",
        "    ensemble_predictions.append(predictions[0].cpu().numpy())  # Assuming single prediction\n",
        "\n",
        "# Average the predictions (or apply other ensembling techniques)\n",
        "ensemble_predictions = np.mean(ensemble_predictions, axis=0)\n",
        "\n",
        "# Ensure the predictions are in the correct format\n",
        "ensemble_predictions = torch.tensor(ensemble_predictions).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "# Resize the ensemble predictions to the original shape if necessary\n",
        "if ensemble_predictions.shape[2:] != original_shape:\n",
        "    ensemble_predictions = torch.nn.functional.interpolate(\n",
        "        ensemble_predictions, size=original_shape, mode='trilinear'\n",
        "    ).squeeze(0).squeeze(0)\n",
        "\n",
        "# Convert back to numpy for saving\n",
        "ensemble_predictions = ensemble_predictions.cpu().numpy()\n",
        "\n",
        "# Save the ensemble prediction\n",
        "pred_img = nib.Nifti1Image(ensemble_predictions, nib.load(file_path).affine)\n",
        "nib.save(pred_img, \"ensemble_prediction.nii.gz\")\n",
        "\n",
        "# Convert predictions and labels to binary format if necessary\n",
        "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "prediction_tensor = post_pred(torch.tensor(ensemble_predictions).to(device))\n",
        "label_tensor = post_label(label.to(device))\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(f\"Dice Score: {dice_score:.4f}\")\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'AddChannelD' from 'monai.transforms' (/home/gasyna/.local/lib/python3.10/site-packages/monai/transforms/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[80], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, EnsureTyped, Compose, AsDiscrete, AddChannelD, RandSpatialCropD, SpatialPadD\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minferers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sliding_window_inference\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decollate_batch\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AddChannelD' from 'monai.transforms' (/home/gasyna/.local/lib/python3.10/site-packages/monai/transforms/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Model inference and Dice score calculation\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, EnsureTyped, Compose, AsDiscrete, AddChannelD, RandSpatialCropD, SpatialPadD\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "from monai.metrics import DiceMetric\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained models (add more as needed for ensembling)\n",
        "models = [\n",
        "    Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt'),\n",
        "    # Net.load_from_checkpoint('checkpoints/another_checkpoint.ckpt')\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    model.eval()\n",
        "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        AddChannelD(keys=[\"image\", \"label\"]),\n",
        "        RandSpatialCropD(keys=[\"image\", \"label\"], roi_size=(160, 160, 160), random_center=True, random_size=False),\n",
        "        SpatialPadD(keys=[\"image\", \"label\"], spatial_size=(160, 160, 160))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data and the label\n",
        "file_path = 'overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "label_path = 'overlapping_labels/0/quadrant_1_1_CT_HR_label_airways.nii.gz'  # Update with your actual label file path\n",
        "data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = common_transforms(data_dict)\n",
        "data = data_dict[\"image\"]\n",
        "label = data_dict[\"label\"]\n",
        "\n",
        "# Save the original shape for padding\n",
        "original_shape = data.shape[1:]  # exclude batch dimension\n",
        "\n",
        "# Convert data and label to tensors and add batch dimension\n",
        "data = torch.tensor(data).unsqueeze(0).to(device)\n",
        "label = torch.tensor(label).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)  # Use a 3D tuple for the spatial dimensions (depth, height, width)\n",
        "        sw_batch_size = 4\n",
        "        print(f\"Input data shape: {data.shape}\")\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        print(f\"Output data shape: {outputs.shape}\")\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference on all models and aggregate the predictions\n",
        "ensemble_predictions = []\n",
        "\n",
        "for model in models:\n",
        "    predictions = infer_on_single_image(model, data)\n",
        "    ensemble_predictions.append(predictions[0].cpu().numpy())  # Assuming single prediction\n",
        "\n",
        "# Average the predictions (or apply other ensembling techniques)\n",
        "ensemble_predictions = np.mean(ensemble_predictions, axis=0)\n",
        "\n",
        "# Ensure the predictions are in the correct format\n",
        "ensemble_predictions = torch.tensor(ensemble_predictions).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "# Resize the ensemble predictions to the original shape if necessary\n",
        "if ensemble_predictions.shape[2:] != original_shape:\n",
        "    ensemble_predictions = torch.nn.functional.interpolate(\n",
        "        ensemble_predictions, size=original_shape, mode='trilinear', align_corners=False\n",
        "    ).squeeze(0)\n",
        "\n",
        "# Convert back to numpy for saving\n",
        "ensemble_predictions = ensemble_predictions.cpu().numpy().squeeze(0)\n",
        "\n",
        "# Save the ensemble prediction\n",
        "pred_img = nib.Nifti1Image(ensemble_predictions, nib.load(file_path).affine)\n",
        "nib.save(pred_img, \"ensemble_prediction.nii.gz\")\n",
        "\n",
        "# Convert predictions and labels to binary format if necessary\n",
        "post_pred = Compose([EnsureTyped(keys=\"pred\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([EnsureTyped(keys=\"label\"), AsDiscrete(to_onehot=2)])\n",
        "\n",
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "prediction_tensor = post_pred({\"pred\": torch.tensor(ensemble_predictions).unsqueeze(0).unsqueeze(0).to(device)})[\"pred\"]\n",
        "label_tensor = post_label({\"label\": label.to(device)})[\"label\"]\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(f\"Dice Score: {dice_score:.4f}\")\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n",
            "\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor(data)  \u001b[38;5;66;03m# Convert data to type Double\u001b[39;00m\n",
            "\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "\u001b[0;32m---> 15\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
            "\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/nets/unet.py:300\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
            "\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n",
            "\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
            "\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/blocks/convolutions.py:316\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
            "\u001b[0;32m--> 316\u001b[0m     res: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# create the additive residual from x\u001b[39;00m\n",
            "\u001b[1;32m    317\u001b[0m     cx: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)  \u001b[38;5;66;03m# apply x to sequence of operations\u001b[39;00m\n",
            "\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cx \u001b[38;5;241m+\u001b[39m res\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
            "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
            "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
            "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
            "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
            "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
            "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n",
            "\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
            "\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n",
            "\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n",
            "\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n",
            "\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n",
            "\u001b[1;32m    607\u001b[0m     )\n",
            "\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n",
            "\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]"
          ]
        }
      ],
      "source": [
        "# Instantiate the Net class\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "file_path = '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the Net class\n",
        "model = Net()\n",
        "\n",
        "# Load the checkpoint for the single model\n",
        "checkpoint_path = 'checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint['state_dict'])  # Assuming 'state_dict' is the key for model state_dict in the checkpoint\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the paths to the NIfTI files\n",
        "file_paths = [\n",
        "    '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz',\n",
        "    # Add paths for other files if applicable\n",
        "]\n",
        "\n",
        "# Perform inference using the single model\n",
        "model_output = model.evaluate_single_model(model, file_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_nifti(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    # Apply any preprocessing steps here, such as normalization\n",
        "    # Remember to ensure the data shape matches the input shape expected by your models\n",
        "    return data\n",
        "\n",
        "\n",
        "model = Net().double()  # Ensure the model is set to accept Double type input\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Define the paths to the NIfTI files and load them\n",
        "input_data = [load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "# Convert input data to Double type\n",
        "input_data = [torch.DoubleTensor(data) for data in input_data]\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Ensure each data has a batch dimension\n",
        "    input_data = [data.unsqueeze(0) for data in input_data]\n",
        "    # Perform inference\n",
        "    model_output = [model(data) for data in input_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the paths to the checkpoints for each model\n",
        "checkpoint_paths = [\n",
        "    'checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt',\n",
        "    'checkpoints/best-checkpoint_new_method_2Q_spatial_size_48_0.7714 at epoch 584.ckpt'\n",
        "    # Add paths for other models if applicable\n",
        "]\n",
        "\n",
        "# Create instances of the Net class for each model\n",
        "models = []\n",
        "for checkpoint_path in checkpoint_paths:\n",
        "    # Initialize the model\n",
        "    model = Net()\n",
        "    \n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])  # Assuming 'state_dict' is the key for model state_dict in the checkpoint\n",
        "    \n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    model.prepare_data()\n",
        "    model.evaluate()\n",
        "    # Append the model to the list of models\n",
        "    models.append(model)\n",
        "\n",
        "    break\n",
        "# Now the models list contains instances of the Net class loaded with the trained weights from checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define a function to load and preprocess NIfTI files\n",
        "def load_and_preprocess_nifti(file_path):\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    # Apply any preprocessing steps here, such as normalization\n",
        "    # Remember to ensure the data shape matches the input shape expected by your models\n",
        "    return torch.tensor(data).unsqueeze(0).float()  # Assuming input shape is (batch_size, channels, height, width, depth)\n",
        "\n",
        "# Define a function to perform inference on NIfTI data using the ensemble of models\n",
        "def perform_inference(models, input_data):\n",
        "    model_outputs = []\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(input_data)\n",
        "            model_outputs.append(output)\n",
        "    # Aggregate the outputs from different models\n",
        "    aggregated_output = torch.mean(torch.stack(model_outputs), dim=0)\n",
        "    return aggregated_output\n",
        "\n",
        "# Define the paths to the NIfTI files and load them\n",
        "file_paths = [\"path_to_file1.nii.gz\", \"path_to_file2.nii.gz\", ...]\n",
        "input_data = [load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "# Perform inference using the ensemble of models\n",
        "ensemble_output = [perform_inference(models, data) for data in input_data]\n",
        "\n",
        "# Post-process the output predictions if necessary\n",
        "# For example, you can convert the logits to probabilities using softmax\n",
        "\n",
        "# Save or visualize the results\n",
        "# You can save the output as NIfTI files or visualize them using plotting libraries like matplotlib or mayavi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0TIqDI0vR"
      },
      "source": [
        "## View training in tensorboard\n",
        "\n",
        "Please uncomment the following cell to load tensorboard results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dteU4yMVI0vR",
        "outputId": "8aa4354d-4d1a-4e57-d50e-d9a6eb88db5f"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDZoq7ZI0vS"
      },
      "source": [
        "## Check best model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pp1HWrqI0vS",
        "jupyter": {
          "outputs_hidden": true
        },
        "lines_to_next_cell": 2,
        "outputId": "c7388a09-ec21-44bb-9747-1f1c8d1f81f9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net.to(device)\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(net.val_dataloader()):\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, net)\n",
        "        # plot the slice [:, :, 80]\n",
        "        plt.figure(\"check\", (18, 6))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(f\"image {i}\")\n",
        "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(f\"label {i}\")\n",
        "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(f\"output {i}\")\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzE-kjOI0vS"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhZOGgOI0vS"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
