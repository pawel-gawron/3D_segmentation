{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3HtVqI0vM"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaMSdU8I0vM",
        "outputId": "fbf973c9-5926-4875-cda6-903844ab9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: python: command not found\n",
            "/bin/bash: line 1: python: command not found\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install -q pytorch-lightning~=2.0\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7oM0xTwI0vN"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7HEKxFEKI0vN",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.4.dev2425\n",
            "Numpy version: 1.26.4\n",
            "Pytorch version: 2.3.1+cu121\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: b5d71491b839191430b236bd3b233c5f29daf229\n",
            "MONAI __file__: /home/<username>/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "scipy version: 1.14.0\n",
            "Pillow version: 10.3.0\n",
            "Tensorboard version: 2.17.0\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "tqdm version: 4.66.4\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 6.0.0\n",
            "pandas version: 2.2.2\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "AsDiscrete,\n",
        "EnsureChannelFirstd,\n",
        "Compose,\n",
        "CropForegroundd,\n",
        "LoadImaged,\n",
        "Orientationd,\n",
        "RandCropByPosNegLabeld,\n",
        "ScaleIntensityRanged,\n",
        "Spacingd,\n",
        "EnsureType,\n",
        "EnsureTyped,\n",
        "Resized,\n",
        "RandAdjustContrastd, \n",
        "RandFlipd, \n",
        "RandAffined, \n",
        "RandAdjustContrastd\n",
        "\n",
        ")\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzzE3GGNI0vO"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1f_XjbPrI0vO",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmp9o6xupp8\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(os.getcwd(), 'AeroPath')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTzgAPbI0vP"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/records/10069289/files/AeroPath.zip?download=1\"\n",
        "md5 = \"3fd5106c175c85d60eaece220f5dfd87\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"AeroPath.zip\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQVxGHhI0vQ"
      },
      "source": [
        "## Define the LightningModule\n",
        "\n",
        "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in `spleen_segmentation_3d.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y1-m7FfcI0vQ"
      },
      "outputs": [],
      "source": [
        "class UNetClass(pytorch_lightning.LightningModule):\n",
        "    def __init__(self, mode, roi_size, spatial_size):\n",
        "        super().__init__()\n",
        "        self._model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        )\n",
        "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "        self.mode = mode\n",
        "        self.roi_size = roi_size\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "        self.common_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "            Spacingd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                # pixdim=(1.5, 1.5, 2.0),\n",
        "                pixdim=(1.0, 1.0, 1.35),\n",
        "                mode=(\"bilinear\", \"nearest\"),\n",
        "            ),\n",
        "            ScaleIntensityRanged(\n",
        "                keys=[\"image\"],\n",
        "                a_min=-1024,\n",
        "                a_max=1024,\n",
        "                b_min=0.0,\n",
        "                b_max=1.0,\n",
        "                clip=True,\n",
        "            ),\n",
        "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            # Resized(keys=[\"image\", \"label\"], spatial_size=self.spatial_size),\n",
        "\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self, prepare_val_data=True, prepare_test_data=True):\n",
        "        # # set up the correct data path\n",
        "        if self.mode == 'whole':\n",
        "            pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'left_bottom':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'left_uppper':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*left_upper_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'right_bottom':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*right_bottom_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'right_upper':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*right_upper_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    # pixdim=(1.5, 1.5, 2.0),\n",
        "                    pixdim=(1.0, 1.0, 1.35),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-1024,\n",
        "                    a_max=1024,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "\n",
        "                # user can also add other random transforms\n",
        "                #                 RandAffined(\n",
        "                #                     keys=['image', 'label'],\n",
        "                #                     mode=('bilinear', 'nearest'),\n",
        "                #                     prob=1.0,\n",
        "                #                     spatial_size=(96, 96, 96),\n",
        "                #                     rotate_range=(0, 0, np.pi/15),\n",
        "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
        "                # Adding the data augmentation transforms with a probability of 50%\n",
        "                # RandFlipd(\n",
        "                #     keys=[\"image\", \"label\"],\n",
        "                #     spatial_axis=[0],  # Horizontal flip\n",
        "                #     prob=0.5\n",
        "                # ),\n",
        "                # RandFlipd(\n",
        "                #     keys=[\"image\", \"label\"],\n",
        "                #     spatial_axis=[1],  # Vertical flip\n",
        "                #     prob=0.5\n",
        "                # ),\n",
        "                RandAffined(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    prob=0.5,\n",
        "                    rotate_range=(np.deg2rad(20), np.deg2rad(20), np.deg2rad(20)),\n",
        "                    translate_range=(0.2, 0.2, 0.2),\n",
        "                    scale_range=(0.5, 1.5),\n",
        "                    mode=('bilinear', 'nearest')\n",
        "                ),\n",
        "                RandAdjustContrastd(\n",
        "                    keys=[\"image\"],\n",
        "                    gamma=(0.5, 2.0),\n",
        "                    prob=0.5\n",
        "                ),\n",
        "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    # pixdim=(1.5, 1.5, 2.0),\n",
        "                    pixdim=(1.0, 1.0, 1.35),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-1024,\n",
        "                    a_max=1024,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        if prepare_test_data:\n",
        "            self.train_ds = CacheDataset(\n",
        "                data=train_files,\n",
        "                transform=train_transforms,\n",
        "                cache_rate=1.0,\n",
        "                num_workers=4,\n",
        "            )\n",
        "        if prepare_val_data:\n",
        "            self.val_ds = CacheDataset(\n",
        "                data=val_files,\n",
        "                transform=val_transforms,\n",
        "                cache_rate=1.0,\n",
        "                num_workers=4,\n",
        "            )\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = self.roi_size\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "    \n",
        "\n",
        "    \n",
        "    def perform_inference(self, model, data):\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "            model_output = model(data.unsqueeze(0))\n",
        "        return model_output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True) # log\n",
        "\n",
        "        return {\"log\": tensorboard_logs}\n",
        "    \n",
        "    \n",
        "    def dice_score(self, prediction_tensor, label_tensor):\n",
        "        # Compute Dice score\n",
        "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "        dice_score = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "\n",
        "        print(dice_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Net_left_upper       = UNetClass(mode = 'left_upper',    roi_size=(160, 160, 160), spatial_size=(160, 160, 160))\n",
        "Net_left_bottom      = UNetClass(mode = 'left_bottom',    roi_size=(160, 160, 160), spatial_size=(160, 160, 160))\n",
        "Net_right_upper      = UNetClass(mode = 'right_upper',    roi_size=(160, 160, 160), spatial_size=(160, 160, 160))\n",
        "Net_right_bottom     = UNetClass(mode = 'right_bottom',    roi_size=(160, 160, 160), spatial_size=(160, 160, 160))\n",
        "NetWhole             = UNetClass(mode = 'whole', roi_size=(128, 128, 144),   spatial_size=(128, 128, 144))\n",
        "\n",
        "# NetWhole    = UNetClass(mode = 'whole', roi_size=(192, 192, 212),   spatial_size=(192, 192, 212))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVCubC2I0vR"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_dice',\n",
        "    dirpath=os.path.join(data_dir, 'checkpoints'),  # Directory to save checkpoints\n",
        "    filename='whole_lower_pixdim',  # Filename prefix for saving checkpoints\n",
        "    save_top_k=1,  # Save only the best checkpoint\n",
        "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
        "    verbose=True,  # Log a message when saving the best checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y8BpsB6sI0vR",
        "scrolled": false,
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "Loading dataset:  61%|██████    | 11/18 [00:06<00:04,  1.74it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7ea3a3034c40>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/io/dictionary.py:162\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_key_postfix):\n\u001b[0;32m--> 162\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loader\u001b[38;5;241m.\u001b[39mimage_only:\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/io/array.py:282\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[0;34m(self, filename, reader)\u001b[0m\n\u001b[1;32m    281\u001b[0m img_array: NdarrayOrTensor\n\u001b[0;32m--> 282\u001b[0m img_array, meta_data \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m img_array \u001b[38;5;241m=\u001b[39m convert_to_dst_type(img_array, dst\u001b[38;5;241m=\u001b[39mimg_array, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/data/image_reader.py:952\u001b[0m, in \u001b[0;36mNibabelReader.get_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    951\u001b[0m header[MetaKeys\u001b[38;5;241m.\u001b[39mSPACE] \u001b[38;5;241m=\u001b[39m SpaceKeys\u001b[38;5;241m.\u001b[39mRAS\n\u001b[0;32m--> 952\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_array_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze_non_spatial_dims:\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/data/image_reader.py:1026\u001b[0m, in \u001b[0;36mNibabelReader._get_array_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03mGet the raw array data of the image, converted to Numpy array.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/nibabel/arrayproxy.py:457\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03mIf ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/nibabel/arrayproxy.py:424\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_unscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m)\u001b[49m, scl_slope, scl_inter)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/nibabel/arrayproxy.py:394\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/nibabel/volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    464\u001b[0m data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n_bytes)\n\u001b[0;32m--> 465\u001b[0m n_read \u001b[38;5;241m=\u001b[39m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n",
            "File \u001b[0;32m/usr/lib/python3.10/gzip.py:488\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_read()\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos\n",
            "File \u001b[0;32m/usr/lib/python3.10/gzip.py:436\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\037\u001b[39;00m\u001b[38;5;130;01m\\213\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadGzipFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot a gzipped file (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m magic)\n\u001b[1;32m    438\u001b[0m (method, flag,\n\u001b[1;32m    439\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_mtime) \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BBIxx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_exact(\u001b[38;5;241m8\u001b[39m))\n",
            "\u001b[0;31mBadGzipFile\u001b[0m: Not a gzipped file (b'*\\xcb')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pytorch_lightning\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     12\u001b[0m     devices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     13\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:946\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__setup_profiler()\n\u001b[1;32m    945\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: preparing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_setup_hook(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:100\u001b[0m, in \u001b[0;36m_DataConnector.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InfiniteBarrier():\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (prepare_data_per_node \u001b[38;5;129;01mand\u001b[39;00m local_rank_zero) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m prepare_data_per_node \u001b[38;5;129;01mand\u001b[39;00m global_rank_zero):\n\u001b[0;32m--> 100\u001b[0m         \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprepare_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    162\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "Cell \u001b[0;32mIn[10], line 198\u001b[0m, in \u001b[0;36mUNetClass.prepare_data\u001b[0;34m(self, prepare_val_data, prepare_test_data)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# we use cached datasets - these are 10x faster than regular datasets\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prepare_test_data:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ds \u001b[38;5;241m=\u001b[39m \u001b[43mCacheDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prepare_val_data:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_ds \u001b[38;5;241m=\u001b[39m CacheDataset(\n\u001b[1;32m    206\u001b[0m         data\u001b[38;5;241m=\u001b[39mval_files,\n\u001b[1;32m    207\u001b[0m         transform\u001b[38;5;241m=\u001b[39mval_transforms,\n\u001b[1;32m    208\u001b[0m         cache_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    209\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/data/dataset.py:809\u001b[0m, in \u001b[0;36mCacheDataset.__init__\u001b[0;34m(self, data, transform, cache_num, cache_rate, num_workers, progress, copy_cache, as_contiguous, hash_as_key, hash_func, runtime_cache)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m ListProxy \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hash_keys: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 809\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/data/dataset.py:836\u001b[0m, in \u001b[0;36mCacheDataset.set_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    833\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_num))\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# prepare cache content immediately\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime_cache:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# this must be in the main process, not in dataloader's workers\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/data/dataset.py:865\u001b[0m, in \u001b[0;36mCacheDataset._fill_cache\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress \u001b[38;5;129;01mand\u001b[39;00m has_tqdm:\n\u001b[0;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cache_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(p\u001b[38;5;241m.\u001b[39mimap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cache_item, indices))\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
            "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/data/dataset.py:878\u001b[0m, in \u001b[0;36mCacheDataset._load_cache_item\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    873\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m    875\u001b[0m first_random \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mget_index_of_first(\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28misinstance\u001b[39m(t, RandomizableTrait) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, Transform)\n\u001b[1;32m    877\u001b[0m )\n\u001b[0;32m--> 878\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_contiguous:\n\u001b[1;32m    881\u001b[0m     item \u001b[38;5;241m=\u001b[39m convert_to_contiguous(item, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/Documents/RISA/3D_segmentation/.venv/lib/python3.10/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7ea3a3034c40>"
          ]
        }
      ],
      "source": [
        "# initialise the LightningModule\n",
        "# net = Net1Q\n",
        "# net = Net2Q\n",
        "net = Net_left_bottom\n",
        "# net = NetWhole\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2TucnezFI0vR",
        "outputId": "4e0a75d6-eb09-4989-bb2b-a2c6b1388edf",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train completed, best_metric: 0.7536 at epoch 540\n"
          ]
        }
      ],
      "source": [
        "print(f\"train completed, best_metric: {net.best_val_dice:.4f} \" f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "\n",
        "\n",
        "# Load the model weights from the checkpoint file\n",
        "checkpoint_path = 'best-checkpoint.ckpt'\n",
        "model = Net1Q.load_from_checkpoint('1Q_clipped_resized128_128_144_roibest_metric: 0.2797 at epoch 495.ckpt', mode = '1Q', roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0TIqDI0vR"
      },
      "source": [
        "## View training in tensorboard\n",
        "\n",
        "Please uncomment the following cell to load tensorboard results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dteU4yMVI0vR",
        "outputId": "8aa4354d-4d1a-4e57-d50e-d9a6eb88db5f"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDZoq7ZI0vS"
      },
      "source": [
        "## Load model and create prediction files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading dataset: 100%|██████████| 9/9 [01:05<00:00,  7.26s/it]\n"
          ]
        }
      ],
      "source": [
        "net = Net1Q.load_from_checkpoint('1Q_clipped_resized128_128_144_roibest_metric: 0.2797 at epoch 495.ckpt',                  mode = '1Q',    roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "net = Net2Q.load_from_checkpoint('2Q_clipped_resized320_160_160.ckpt',                                                      mode = '2Q',    roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "net = NetWhole.load_from_checkpoint('whole_clipped_resized160_best_metric: 0.8211 at epoch 562.ckpt',                       mode = 'whole', roi_size=(128, 128, 144),   spatial_size=(128, 128, 144))\n",
        "\n",
        "net.prepare_data(prepare_test_data=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test validation dataset labels\n",
        "for i, val_data in enumerate(net.val_dataloader()):\n",
        "    label = val_data['label'].cpu().numpy()[0, 0, :, :, :]\n",
        "    nib.save(nib.Nifti1Image(label.astype(float), nib.load('AeroPath/1/1_CT_HR_label_airways.nii.gz').affine), f'labels_spacingd_10less/{i}.nii.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8pp1HWrqI0vS",
        "jupyter": {
          "outputs_hidden": true
        },
        "lines_to_next_cell": 2,
        "outputId": "c7388a09-ec21-44bb-9747-1f1c8d1f81f9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# model = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "\n",
        "# model.eval()\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)\n",
        "\n",
        "# if net is None:\n",
        "#     net = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "#     net.prepare_data()\n",
        "\n",
        "net.eval()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net.to(device)\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(net.val_dataloader()):\n",
        "        # roi_size = (64, 64, 64)\n",
        "        roi_size = net.roi_size\n",
        "        # roi_size = (128, 128, 144)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, net)\n",
        "        # plot the slice [:, :, 80]\n",
        "        # plt.figure(\"check\", (18, 6))\n",
        "        # plt.subplot(1, 3, 1)\n",
        "        # plt.title(f\"image {i}\")\n",
        "        # plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        # plt.subplot(1, 3, 2)\n",
        "        # plt.title(f\"label {i}\")\n",
        "        # plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        # plt.subplot(1, 3, 3)\n",
        "        # plt.title(f\"output {i}\")\n",
        "        pred = torch.argmax(val_outputs, dim=1).detach().cpu()\n",
        "        # plt.imshow(pred[0, :, :, 80])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        pred_np = pred.cpu().numpy()[0, :, :, :]\n",
        "\n",
        "        # dir_name = 'whole_resized_roi160'\n",
        "        # dir_name = '1Q_resized_roi160'\n",
        "        # # dir_name = '1Q'\n",
        "        # # dir_name = '2Q'\n",
        "\n",
        "        dir_name = net.mode\n",
        "\n",
        "        nib.save(nib.Nifti1Image(pred_np.astype(float), nib.load('AeroPath/1/1_CT_HR_label_airways.nii.gz').affine), f'pred/{dir_name}/{i}.nii.gz')\n",
        "\n",
        "        # label = val_data['label'].cpu().numpy()[0, 0, :, :, :]\n",
        "        # nib.save(nib.Nifti1Image(label.astype(float), nib.load('AeroPath/1/1_CT_HR_label_airways.nii.gz').affine), f'labels_resized/{i}.nii.gz')\n",
        "\n",
        "        # print(f\"pred shape: {pred_np.shape}, label shape: {label.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolate, Concat and ensemble predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score for whole prediction: 0.8429\n",
            "Dice Score for ensembled prediction: 0.8514\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7267\n",
            "Dice Score for ensembled prediction: 0.7223\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7134\n",
            "Dice Score for ensembled prediction: 0.7225\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.8573\n",
            "Dice Score for ensembled prediction: 0.8615\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.8257\n",
            "Dice Score for ensembled prediction: 0.8436\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7070\n",
            "Dice Score for ensembled prediction: 0.7301\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7620\n",
            "Dice Score for ensembled prediction: 0.7973\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.6476\n",
            "Dice Score for ensembled prediction: 0.6588\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.8258\n",
            "Dice Score for ensembled prediction: 0.8360\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nibabel as nib\n",
        "from monai.transforms import Compose, AsDiscrete\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Ensure predictions tensor has 5 dimensions: (N, C, D, H, W)\n",
        "    predictions = torch.tensor(predictions).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "\n",
        "    # Interpolate the predictions using nearest neighbor interpolation\n",
        "    interpolated_predictions = F.interpolate(predictions, size=target_shape, mode='nearest')\n",
        "    \n",
        "    # Remove batch and channel dimensions after interpolation\n",
        "    interpolated_predictions = interpolated_predictions.squeeze(0).squeeze(0).numpy()\n",
        "    \n",
        "    return interpolated_predictions\n",
        "\n",
        "# Function to calculate Dice score\n",
        "def calc_dice_score(pred, label):\n",
        "    post_pred = Compose([AsDiscrete(argmax=False, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    # Ensure the inputs have the correct dimensions\n",
        "    pred = torch.tensor(pred).unsqueeze(0).unsqueeze(0)\n",
        "    label = torch.tensor(label).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    prediction_tensor = post_pred(pred)\n",
        "    label_tensor = post_label(label)\n",
        "\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    return dice_score\n",
        "\n",
        "\n",
        "labels = natsorted(glob.glob('labels/*', recursive=True))\n",
        "\n",
        "\n",
        "# Example usage with NIfTI files\n",
        "\n",
        "for idx, _ in enumerate(labels):\n",
        "    label = nib.load(f'labels_resized/{idx}.nii.gz').get_fdata()\n",
        "    # whole = nib.load(f'pred/whole_resized_roi160/{idx}.nii.gz').get_fdata()\n",
        "    whole = nib.load(f'pred/whole/{idx}.nii.gz').get_fdata()\n",
        "\n",
        "    pred_1Q = nib.load(f'pred/1Q_resized_roi160/{idx}.nii.gz').get_fdata()\n",
        "    pred_2Q = nib.load(f'pred/2Q_resized_roi160/{idx}.nii.gz').get_fdata()\n",
        "\n",
        "\n",
        "    # Interpolate predictions to allow for concatenation\n",
        "\n",
        "    inter_pred_1Q = interpolate_predictions(pred_1Q, (*whole.shape[:-1], pred_1Q.shape[-1]))\n",
        "    inter_pred_2Q = interpolate_predictions(pred_2Q, (*whole.shape[:-1], pred_2Q.shape[-1]))\n",
        "\n",
        "\n",
        "    # Merge interpolated predictions along the last axis (z-axis)\n",
        "    merged = np.concatenate((inter_pred_1Q, inter_pred_2Q), axis=2)\n",
        "\n",
        "    # Interpolate predictions to match whole.shape\n",
        "\n",
        "    merged = interpolate_predictions(merged, whole.shape)\n",
        "\n",
        "    # Save the merged predictions as a NIfTI file\n",
        "    merged_nifti = nib.Nifti1Image(merged, nib.load('AeroPath/1/1_CT_HR.nii.gz').affine)\n",
        "    nib.save(merged_nifti, 'pred_test_merged.nii.gz')\n",
        "    # print(f'Merged shape: {merged.shape}')\n",
        "\n",
        "    # Perform ensembling using the maximum values\n",
        "    ensembled = np.maximum(merged, whole)\n",
        "    \n",
        "    ensembled[:5, :, :] = 0\n",
        "    ensembled[-5:, :, :] = 0\n",
        "    ensembled[:, :5, :] = 0\n",
        "    ensembled[:, -5:, :] = 0\n",
        "    ensembled[:, :, :5] = 0\n",
        "    ensembled[:, :, -5:] = 0\n",
        "\n",
        "    # Save the ensembled predictions as a NIfTI file\n",
        "    ensembled_nifti = nib.Nifti1Image(ensembled, nib.load('AeroPath/1/1_CT_HR.nii.gz').affine)\n",
        "    nib.save(ensembled_nifti, f'pred/ensembled/{idx}.nii.gz')\n",
        "\n",
        "    # Compute Dice score\n",
        "    # print(f'whole: {whole.shape}, label: {label.shape}')\n",
        "    dice_whole = calc_dice_score(whole, label)\n",
        "    print(f'Dice Score for whole prediction: {dice_whole:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    dice_ensembled = calc_dice_score(ensembled, label)\n",
        "    print(f'Dice Score for ensembled prediction: {dice_ensembled:.4f}')\n",
        "\n",
        "    print('*'*50)\n",
        "\n",
        "# Dice Score for whole prediction: 0.8711\n",
        "# Dice Score for ensembled prediction: 0.8846\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7663\n",
        "# Dice Score for ensembled prediction: 0.7592\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7318\n",
        "# Dice Score for ensembled prediction: 0.7432\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.8730\n",
        "# Dice Score for ensembled prediction: 0.8764\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.8862\n",
        "# Dice Score for ensembled prediction: 0.8930\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7021\n",
        "# Dice Score for ensembled prediction: 0.7184\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7940\n",
        "# Dice Score for ensembled prediction: 0.8396\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.6629\n",
        "# Dice Score for ensembled prediction: 0.6776\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.8969\n",
        "# Dice Score for ensembled prediction: 0.9105\n",
        "# **************************************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score for whole prediction: 0.9051\n",
            "Dice Score for ensembled prediction: 0.8460\n"
          ]
        }
      ],
      "source": [
        "ensembled_nifti = nib.Nifti1Image(ensembled, nib.load('AeroPath/1/1_CT_HR.nii.gz').affine)\n",
        "nib.save(ensembled_nifti, f'pred/ensembled/{idx}.nii.gz')\n",
        "\n",
        "# Compute Dice score\n",
        "dice_whole = calc_dice_score(whole, label)\n",
        "print(f'Dice Score for whole prediction: {dice_whole:.4f}')\n",
        "\n",
        "dice_ensembled = calc_dice_score(ensembled, label)\n",
        "print(f'Dice Score for ensembled prediction: {dice_ensembled:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7544/2687408277.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred = torch.tensor(pred).unsqueeze(0).unsqueeze(0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score for whole prediction: 0.4997\n"
          ]
        }
      ],
      "source": [
        "def calc_dice_score(pred, label):\n",
        "    post_pred = Compose([AsDiscrete(argmax=False, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    # Ensure the inputs have the correct dimensions\n",
        "    pred = torch.tensor(pred).unsqueeze(0).unsqueeze(0)\n",
        "    label = torch.tensor(label).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    prediction_tensor = post_pred(pred)\n",
        "    label_tensor = post_label(label)\n",
        "\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    return dice_score\n",
        "\n",
        "label = val_data['label'].cpu().numpy()[0, 0, :, :, :]\n",
        "dice_whole = calc_dice_score(pred, label)\n",
        "print(f'Dice Score for whole prediction: {dice_whole:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzE-kjOI0vS"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhZOGgOI0vS"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
