{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3HtVqI0vM"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaMSdU8I0vM",
        "outputId": "fbf973c9-5926-4875-cda6-903844ab9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
            "2024-07-11 15:43:32.831357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "!python3 -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python3 -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install -q pytorch-lightning~=2.0\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7oM0xTwI0vN"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7HEKxFEKI0vN",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
            "2024-07-12 18:55:05.918851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.4.dev2427\n",
            "Numpy version: 1.26.4\n",
            "Pytorch version: 2.0.0+cu117\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: cbf90d0ddb27dc96a91385e4dd2f4eb239dea976\n",
            "MONAI __file__: /home/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: 5.4.0\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: 0.24.0\n",
            "scipy version: 1.14.0\n",
            "Pillow version: 10.4.0\n",
            "Tensorboard version: 2.12.2\n",
            "gdown version: 5.2.0\n",
            "TorchVision version: 0.15.1+cu117\n",
            "tqdm version: 4.66.4\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.8\n",
            "pandas version: 1.5.3\n",
            "einops version: 0.8.0\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: 1.0.0\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "AsDiscrete,\n",
        "EnsureChannelFirstd,\n",
        "Compose,\n",
        "CropForegroundd,\n",
        "LoadImaged,\n",
        "Orientationd,\n",
        "RandCropByPosNegLabeld,\n",
        "ScaleIntensityRanged,\n",
        "Spacingd,\n",
        "EnsureType,\n",
        "EnsureTyped,\n",
        "Resized,\n",
        "RandAdjustContrastd, \n",
        "RandFlipd, \n",
        "RandAffined, \n",
        "RandAdjustContrastd\n",
        "\n",
        ")\n",
        "from monai.networks.nets import UNet, UNETR\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "from sklearn.model_selection import KFold\n",
        "from pytorch_lightning.plugins import MixedPrecisionPlugin\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzzE3GGNI0vO"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1f_XjbPrI0vO",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmpm4b4_nf6\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(os.getcwd(), 'AeroPath')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTzgAPbI0vP"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/records/10069289/files/AeroPath.zip?download=1\"\n",
        "md5 = \"3fd5106c175c85d60eaece220f5dfd87\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"AeroPath.zip\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQVxGHhI0vQ"
      },
      "source": [
        "## Define the LightningModule\n",
        "\n",
        "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in `spleen_segmentation_3d.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y1-m7FfcI0vQ"
      },
      "outputs": [],
      "source": [
        "class UNetClass(pytorch_lightning.LightningModule):\n",
        "    def __init__(self, mode, roi_size, spatial_size):\n",
        "        super().__init__()\n",
        "        self._model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        )\n",
        "        # self._model = UNETR(\n",
        "        #     in_channels=1,\n",
        "        #     out_channels=2,\n",
        "        #     img_size=roi_size,\n",
        "        #     feature_size=16,\n",
        "        #     hidden_size=768,\n",
        "        #     mlp_dim=3072,\n",
        "        #     num_heads=12,\n",
        "        #     pos_embed=\"perceptron\",\n",
        "        #     norm_name=\"instance\",\n",
        "        #     res_block=True,\n",
        "        #     dropout_rate=0.0,\n",
        "        # )\n",
        "        \n",
        "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "        self.mode = mode\n",
        "        self.roi_size = roi_size\n",
        "        self.spatial_size = spatial_size\n",
        "\n",
        "        self.common_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "            Spacingd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                # pixdim=(1.5, 1.5, 2.0),\n",
        "                pixdim=(1.1, 1.1, 1.40),\n",
        "                mode=(\"bilinear\", \"nearest\"),\n",
        "            ),\n",
        "            ScaleIntensityRanged(\n",
        "                keys=[\"image\"],\n",
        "                a_min=-1024,\n",
        "                a_max=1024,\n",
        "                b_min=0.0,\n",
        "                b_max=1.0,\n",
        "                clip=True,\n",
        "            ),\n",
        "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            Resized(keys=[\"image\", \"label\"], spatial_size=self.spatial_size),\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self, prepare_val_data=True, prepare_test_data=True):\n",
        "        # # set up the correct data path\n",
        "        if self.mode == 'whole':\n",
        "            pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == '1Q':\n",
        "            pattern = os.path.join('nonoverlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('nonoverlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == '2Q':\n",
        "            pattern = os.path.join('nonoverlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('nonoverlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'left_bottom':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'left_uppper':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*left_upper_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'right_bottom':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*right_bottom_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'right_upper':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*right_upper_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    # pixdim=(1.5, 1.5, 2.0),\n",
        "                    pixdim=(1.1, 1.1, 1.40),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-1024,\n",
        "                    a_max=1024,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "\n",
        "                # user can also add other random transforms\n",
        "                #                 RandAffined(\n",
        "                #                     keys=['image', 'label'],\n",
        "                #                     mode=('bilinear', 'nearest'),\n",
        "                #                     prob=1.0,\n",
        "                #                     spatial_size=(96, 96, 96),\n",
        "                #                     rotate_range=(0, 0, np.pi/15),\n",
        "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
        "                # Adding the data augmentation transforms with a probability of 50%\n",
        "                # RandFlipd(\n",
        "                #     keys=[\"image\", \"label\"],\n",
        "                #     spatial_axis=[0],  # Horizontal flip\n",
        "                #     prob=0.5\n",
        "                # ),\n",
        "                # RandFlipd(\n",
        "                #     keys=[\"image\", \"label\"],\n",
        "                #     spatial_axis=[1],  # Vertical flip\n",
        "                #     prob=0.5\n",
        "                # ),\n",
        "                # RandAffined(\n",
        "                #     keys=[\"image\", \"label\"],\n",
        "                #     prob=0.5,\n",
        "                #     rotate_range=(np.deg2rad(20), np.deg2rad(20), np.deg2rad(20)),\n",
        "                #     translate_range=(0.2, 0.2, 0.2),\n",
        "                #     scale_range=(0.5, 1.5),\n",
        "                #     mode=('bilinear', 'nearest')\n",
        "                # ),\n",
        "                RandAdjustContrastd(\n",
        "                    keys=[\"image\"],\n",
        "                    gamma=(0.5, 2.0),\n",
        "                    prob=0.0\n",
        "                ),\n",
        "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    # pixdim=(1.5, 1.5, 2.0),\n",
        "                    pixdim=(1.1, 1.1, 1.40),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-1024,\n",
        "                    a_max=1024,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        if prepare_test_data:\n",
        "            self.train_ds = CacheDataset(\n",
        "                data=train_files,\n",
        "                transform=train_transforms,\n",
        "                cache_rate=0.4,\n",
        "                num_workers=4,\n",
        "            )\n",
        "        if prepare_val_data:\n",
        "            self.val_ds = CacheDataset(\n",
        "                data=val_files,\n",
        "                transform=val_transforms,\n",
        "                cache_rate=0.4,\n",
        "                num_workers=4,\n",
        "            )\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=1,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = self.roi_size\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "    \n",
        "\n",
        "    \n",
        "    def perform_inference(self, model, data):\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "            model_output = model(data.unsqueeze(0))\n",
        "        return model_output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True) # log\n",
        "\n",
        "        return {\"log\": tensorboard_logs}\n",
        "    \n",
        "    \n",
        "    def dice_score(self, prediction_tensor, label_tensor):\n",
        "        # Compute Dice score\n",
        "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "        dice_score = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "\n",
        "        print(dice_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from monai.losses import DiceCELoss\n",
        "\n",
        "class UnetrNet(pytorch_lightning.LightningModule):\n",
        "    def __init__(self, mode = 'whole'):\n",
        "        super().__init__()\n",
        "\n",
        "        self._model = UNETR(\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            img_size=(96, 96, 96),\n",
        "            feature_size=16,\n",
        "            hidden_size=768,\n",
        "            mlp_dim=3072,\n",
        "            num_heads=12,\n",
        "            proj_type=\"perceptron\",\n",
        "            norm_name=\"instance\",\n",
        "            res_block=True,\n",
        "            conv_block=True,\n",
        "            dropout_rate=0.0,\n",
        "        ).to(device)\n",
        "\n",
        "        self.loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = AsDiscrete(argmax=True, to_onehot=2)\n",
        "        self.post_label = AsDiscrete(to_onehot=2)\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.max_epochs = 600\n",
        "        self.check_val = 30\n",
        "        self.warmup_epochs = 20\n",
        "        self.metric_values = []\n",
        "        self.epoch_loss_values = []\n",
        "        self.validation_step_outputs = []\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self, prepare_val_data=True, prepare_test_data=True):\n",
        "        # # set up the correct data path\n",
        "        if self.mode == 'whole':\n",
        "            pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == '1Q':\n",
        "            pattern = os.path.join('nonoverlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('nonoverlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == '2Q':\n",
        "            pattern = os.path.join('nonoverlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('nonoverlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'left_bottom':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'left_uppper':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*left_upper_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'right_bottom':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*right_bottom_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        elif self.mode == 'right_upper':\n",
        "            pattern = os.path.join('dataset/airways_patched_4', '**/*right_upper_*.nii.gz')\n",
        "            train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "            pattern = os.path.join('dataset/scan_patched_4', '**/*left_bottom_*.nii.gz')\n",
        "            train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    # pixdim=(1.1, 1.1, 1.40),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-1024,\n",
        "                    a_max=1024,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "                RandAdjustContrastd(\n",
        "                    keys=[\"image\"],\n",
        "                    gamma=(0.5, 2.0),\n",
        "                    prob=0.0\n",
        "                ),\n",
        "                EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    # pixdim=(1.1, 1.1, 1.40),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-1024,\n",
        "                    a_max=1024,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        if prepare_test_data:\n",
        "            self.train_ds = CacheDataset(\n",
        "                data=train_files,\n",
        "                transform=train_transforms,\n",
        "                cache_rate=0.4,\n",
        "                num_workers=4,\n",
        "            )\n",
        "        if prepare_val_data:\n",
        "            self.val_ds = CacheDataset(\n",
        "                data=val_files,\n",
        "                transform=val_transforms,\n",
        "                cache_rate=0.4,\n",
        "                num_workers=4,\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=1,\n",
        "            shuffle=True,\n",
        "            num_workers=8,\n",
        "            pin_memory=True,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self._model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def on_train_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "        self.epoch_loss_values.append(avg_loss.detach().cpu().numpy())\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = (96, 96, 96)\n",
        "        sw_batch_size = 2\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self.forward)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.metric_values.append(mean_val_dice)\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        return {\"log\": tensorboard_logs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Net_left_upper       = UNetClass(mode = 'left_upper',       roi_size=(160, 160, 160),   spatial_size=(160, 160, 160))\n",
        "# Net_left_bottom      = UNetClass(mode = 'left_bottom',      roi_size=(160, 160, 160),   spatial_size=(160, 160, 160))\n",
        "# Net_right_upper      = UNetClass(mode = 'right_upper',      roi_size=(160, 160, 160),   spatial_size=(160, 160, 160))\n",
        "# Net_right_bottom     = UNetClass(mode = 'right_bottom',     roi_size=(160, 160, 160),   spatial_size=(160, 160, 160))\n",
        "# NetWhole             = UNetClass(mode = 'whole',            roi_size=(128, 128, 144),   spatial_size=(128, 128, 144))\n",
        "NetWhole             = UNetClass(mode = 'whole',            roi_size=(128, 128, 128),   spatial_size=(128, 128, 128))\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# NetWhole = UnetrNet(mode = 'whole')\n",
        "# Net1Q                = UNetClass(mode = '1Q',               roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "# Net2Q                = UNetClass(mode = '2Q',               roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "\n",
        "# NetWhole    = UNetClass(mode = 'whole', roi_size=(192, 192, 212),   spatial_size=(192, 192, 212))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVCubC2I0vR"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_dice',\n",
        "    dirpath=os.path.join(data_dir, 'checkpoints'),  # Directory to save checkpoints\n",
        "    filename='whole_bigger_pixdim_UNETR',  # Filename prefix for saving checkpoints\n",
        "    save_top_k=1,  # Save only the best checkpoint\n",
        "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
        "    verbose=True,  # Log a message when saving the best checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y8BpsB6sI0vR",
        "scrolled": false,
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Loading dataset:   0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# initialise the LightningModule\n",
        "# net = Net1Q\n",
        "# net = Net2Q\n",
        "# net = Net_left_bottom\n",
        "net = NetWhole\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        ")\n",
        "\n",
        "# Mixed precision trainer\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        "    precision=16,  # Mixed precision\n",
        "    # plugins=[MixedPrecisionPlugin()]\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TucnezFI0vR",
        "outputId": "4e0a75d6-eb09-4989-bb2b-a2c6b1388edf",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train completed, best_metric: 0.7536 at epoch 540\n"
          ]
        }
      ],
      "source": [
        "print(f\"train completed, best_metric: {net.best_val_dice:.4f} \" f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "\n",
        "\n",
        "# Load the model weights from the checkpoint file\n",
        "checkpoint_path = 'best-checkpoint.ckpt'\n",
        "model = Net1Q.load_from_checkpoint('1Q_clipped_resized128_128_144_roibest_metric: 0.2797 at epoch 495.ckpt', mode = '1Q', roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0TIqDI0vR"
      },
      "source": [
        "## View training in tensorboard\n",
        "\n",
        "Please uncomment the following cell to load tensorboard results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dteU4yMVI0vR",
        "outputId": "8aa4354d-4d1a-4e57-d50e-d9a6eb88db5f"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDZoq7ZI0vS"
      },
      "source": [
        "## Load model and create prediction files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading dataset: 100%|██████████| 9/9 [01:05<00:00,  7.26s/it]\n"
          ]
        }
      ],
      "source": [
        "net = Net1Q.load_from_checkpoint('1Q_clipped_resized128_128_144_roibest_metric: 0.2797 at epoch 495.ckpt',                  mode = '1Q',    roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "net = Net2Q.load_from_checkpoint('2Q_clipped_resized320_160_160.ckpt',                                                      mode = '2Q',    roi_size=(160*2, 160, 160), spatial_size=(160*2, 160, 160))\n",
        "net = NetWhole.load_from_checkpoint('whole_clipped_resized160_best_metric: 0.8211 at epoch 562.ckpt',                       mode = 'whole', roi_size=(128, 128, 144),   spatial_size=(128, 128, 144))\n",
        "\n",
        "net.prepare_data(prepare_test_data=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test validation dataset labels\n",
        "for i, val_data in enumerate(net.val_dataloader()):\n",
        "    label = val_data['label'].cpu().numpy()[0, 0, :, :, :]\n",
        "    nib.save(nib.Nifti1Image(label.astype(float), nib.load('AeroPath/1/1_CT_HR_label_airways.nii.gz').affine), f'labels_spacingd_10less/{i}.nii.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pp1HWrqI0vS",
        "jupyter": {
          "outputs_hidden": true
        },
        "lines_to_next_cell": 2,
        "outputId": "c7388a09-ec21-44bb-9747-1f1c8d1f81f9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# model = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "\n",
        "# model.eval()\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)\n",
        "\n",
        "# if net is None:\n",
        "#     net = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "#     net.prepare_data()\n",
        "\n",
        "net.eval()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net.to(device)\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(net.val_dataloader()):\n",
        "        # roi_size = (64, 64, 64)\n",
        "        roi_size = net.roi_size\n",
        "        # roi_size = (128, 128, 144)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, net)\n",
        "        # plot the slice [:, :, 80]\n",
        "        # plt.figure(\"check\", (18, 6))\n",
        "        # plt.subplot(1, 3, 1)\n",
        "        # plt.title(f\"image {i}\")\n",
        "        # plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        # plt.subplot(1, 3, 2)\n",
        "        # plt.title(f\"label {i}\")\n",
        "        # plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        # plt.subplot(1, 3, 3)\n",
        "        # plt.title(f\"output {i}\")\n",
        "        pred = torch.argmax(val_outputs, dim=1).detach().cpu()\n",
        "        # plt.imshow(pred[0, :, :, 80])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        pred_np = pred.cpu().numpy()[0, :, :, :]\n",
        "\n",
        "        # dir_name = 'whole_resized_roi160'\n",
        "        # dir_name = '1Q_resized_roi160'\n",
        "        # # dir_name = '1Q'\n",
        "        # # dir_name = '2Q'\n",
        "\n",
        "        dir_name = net.mode\n",
        "\n",
        "        nib.save(nib.Nifti1Image(pred_np.astype(float), nib.load('AeroPath/1/1_CT_HR_label_airways.nii.gz').affine), f'pred/{dir_name}/{i}.nii.gz')\n",
        "\n",
        "        # label = val_data['label'].cpu().numpy()[0, 0, :, :, :]\n",
        "        # nib.save(nib.Nifti1Image(label.astype(float), nib.load('AeroPath/1/1_CT_HR_label_airways.nii.gz').affine), f'labels_resized/{i}.nii.gz')\n",
        "\n",
        "        # print(f\"pred shape: {pred_np.shape}, label shape: {label.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolate, Concat and ensemble predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score for whole prediction: 0.8429\n",
            "Dice Score for ensembled prediction: 0.8514\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7267\n",
            "Dice Score for ensembled prediction: 0.7223\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7134\n",
            "Dice Score for ensembled prediction: 0.7225\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.8573\n",
            "Dice Score for ensembled prediction: 0.8615\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.8257\n",
            "Dice Score for ensembled prediction: 0.8436\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7070\n",
            "Dice Score for ensembled prediction: 0.7301\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.7620\n",
            "Dice Score for ensembled prediction: 0.7973\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.6476\n",
            "Dice Score for ensembled prediction: 0.6588\n",
            "**************************************************\n",
            "Dice Score for whole prediction: 0.8258\n",
            "Dice Score for ensembled prediction: 0.8360\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nibabel as nib\n",
        "from monai.transforms import Compose, AsDiscrete\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Ensure predictions tensor has 5 dimensions: (N, C, D, H, W)\n",
        "    predictions = torch.tensor(predictions).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "\n",
        "    # Interpolate the predictions using nearest neighbor interpolation\n",
        "    interpolated_predictions = F.interpolate(predictions, size=target_shape, mode='nearest')\n",
        "    \n",
        "    # Remove batch and channel dimensions after interpolation\n",
        "    interpolated_predictions = interpolated_predictions.squeeze(0).squeeze(0).numpy()\n",
        "    \n",
        "    return interpolated_predictions\n",
        "\n",
        "# Function to calculate Dice score\n",
        "def calc_dice_score(pred, label):\n",
        "    post_pred = Compose([AsDiscrete(argmax=False, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    # Ensure the inputs have the correct dimensions\n",
        "    pred = torch.tensor(pred).unsqueeze(0).unsqueeze(0)\n",
        "    label = torch.tensor(label).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    prediction_tensor = post_pred(pred)\n",
        "    label_tensor = post_label(label)\n",
        "\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    return dice_score\n",
        "\n",
        "\n",
        "labels = natsorted(glob.glob('labels/*', recursive=True))\n",
        "\n",
        "\n",
        "# Example usage with NIfTI files\n",
        "\n",
        "for idx, _ in enumerate(labels):\n",
        "    label = nib.load(f'labels_resized/{idx}.nii.gz').get_fdata()\n",
        "    # whole = nib.load(f'pred/whole_resized_roi160/{idx}.nii.gz').get_fdata()\n",
        "    whole = nib.load(f'pred/whole/{idx}.nii.gz').get_fdata()\n",
        "\n",
        "    pred_1Q = nib.load(f'pred/1Q_resized_roi160/{idx}.nii.gz').get_fdata()\n",
        "    pred_2Q = nib.load(f'pred/2Q_resized_roi160/{idx}.nii.gz').get_fdata()\n",
        "\n",
        "\n",
        "    # Interpolate predictions to allow for concatenation\n",
        "\n",
        "    inter_pred_1Q = interpolate_predictions(pred_1Q, (*whole.shape[:-1], pred_1Q.shape[-1]))\n",
        "    inter_pred_2Q = interpolate_predictions(pred_2Q, (*whole.shape[:-1], pred_2Q.shape[-1]))\n",
        "\n",
        "\n",
        "    # Merge interpolated predictions along the last axis (z-axis)\n",
        "    merged = np.concatenate((inter_pred_1Q, inter_pred_2Q), axis=2)\n",
        "\n",
        "    # Interpolate predictions to match whole.shape\n",
        "\n",
        "    merged = interpolate_predictions(merged, whole.shape)\n",
        "\n",
        "    # Save the merged predictions as a NIfTI file\n",
        "    merged_nifti = nib.Nifti1Image(merged, nib.load('AeroPath/1/1_CT_HR.nii.gz').affine)\n",
        "    nib.save(merged_nifti, 'pred_test_merged.nii.gz')\n",
        "    # print(f'Merged shape: {merged.shape}')\n",
        "\n",
        "    # Perform ensembling using the maximum values\n",
        "    ensembled = np.maximum(merged, whole)\n",
        "    \n",
        "    ensembled[:5, :, :] = 0\n",
        "    ensembled[-5:, :, :] = 0\n",
        "    ensembled[:, :5, :] = 0\n",
        "    ensembled[:, -5:, :] = 0\n",
        "    ensembled[:, :, :5] = 0\n",
        "    ensembled[:, :, -5:] = 0\n",
        "\n",
        "    # Save the ensembled predictions as a NIfTI file\n",
        "    ensembled_nifti = nib.Nifti1Image(ensembled, nib.load('AeroPath/1/1_CT_HR.nii.gz').affine)\n",
        "    nib.save(ensembled_nifti, f'pred/ensembled/{idx}.nii.gz')\n",
        "\n",
        "    # Compute Dice score\n",
        "    # print(f'whole: {whole.shape}, label: {label.shape}')\n",
        "    dice_whole = calc_dice_score(whole, label)\n",
        "    print(f'Dice Score for whole prediction: {dice_whole:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "    dice_ensembled = calc_dice_score(ensembled, label)\n",
        "    print(f'Dice Score for ensembled prediction: {dice_ensembled:.4f}')\n",
        "\n",
        "    print('*'*50)\n",
        "\n",
        "# Dice Score for whole prediction: 0.8711\n",
        "# Dice Score for ensembled prediction: 0.8846\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7663\n",
        "# Dice Score for ensembled prediction: 0.7592\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7318\n",
        "# Dice Score for ensembled prediction: 0.7432\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.8730\n",
        "# Dice Score for ensembled prediction: 0.8764\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.8862\n",
        "# Dice Score for ensembled prediction: 0.8930\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7021\n",
        "# Dice Score for ensembled prediction: 0.7184\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.7940\n",
        "# Dice Score for ensembled prediction: 0.8396\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.6629\n",
        "# Dice Score for ensembled prediction: 0.6776\n",
        "# **************************************************\n",
        "# Dice Score for whole prediction: 0.8969\n",
        "# Dice Score for ensembled prediction: 0.9105\n",
        "# **************************************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score for whole prediction: 0.9051\n",
            "Dice Score for ensembled prediction: 0.8460\n"
          ]
        }
      ],
      "source": [
        "ensembled_nifti = nib.Nifti1Image(ensembled, nib.load('AeroPath/1/1_CT_HR.nii.gz').affine)\n",
        "nib.save(ensembled_nifti, f'pred/ensembled/{idx}.nii.gz')\n",
        "\n",
        "# Compute Dice score\n",
        "dice_whole = calc_dice_score(whole, label)\n",
        "print(f'Dice Score for whole prediction: {dice_whole:.4f}')\n",
        "\n",
        "dice_ensembled = calc_dice_score(ensembled, label)\n",
        "print(f'Dice Score for ensembled prediction: {dice_ensembled:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7544/2687408277.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred = torch.tensor(pred).unsqueeze(0).unsqueeze(0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score for whole prediction: 0.4997\n"
          ]
        }
      ],
      "source": [
        "def calc_dice_score(pred, label):\n",
        "    post_pred = Compose([AsDiscrete(argmax=False, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    # Ensure the inputs have the correct dimensions\n",
        "    pred = torch.tensor(pred).unsqueeze(0).unsqueeze(0)\n",
        "    label = torch.tensor(label).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    prediction_tensor = post_pred(pred)\n",
        "    label_tensor = post_label(label)\n",
        "\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    return dice_score\n",
        "\n",
        "label = val_data['label'].cpu().numpy()[0, 0, :, :, :]\n",
        "dice_whole = calc_dice_score(pred, label)\n",
        "print(f'Dice Score for whole prediction: {dice_whole:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzE-kjOI0vS"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhZOGgOI0vS"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
