{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3HtVqI0vM"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaMSdU8I0vM",
        "outputId": "fbf973c9-5926-4875-cda6-903844ab9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: python: command not found\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install -q pytorch-lightning~=2.0\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7oM0xTwI0vN"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7HEKxFEKI0vN",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
            "2024-06-20 19:53:07.073935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.3.0\n",
            "Numpy version: 1.23.5\n",
            "Pytorch version: 2.0.0+cu117\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
            "MONAI __file__: /home/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "scipy version: 1.8.0\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.12.2\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: 0.15.1+cu117\n",
            "tqdm version: 4.65.0\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.7\n",
            "pandas version: 1.5.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        "    EnsureTyped,\n",
        ")\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import nibabel as nib\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzzE3GGNI0vO"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1f_XjbPrI0vO",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmpuznlvoil\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTzgAPbI0vP"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset.\n",
        "The dataset comes from http://medicaldecathlon.com/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/records/10069289/files/AeroPath.zip?download=1\"\n",
        "md5 = \"3fd5106c175c85d60eaece220f5dfd87\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"AeroPath.zip\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQVxGHhI0vQ"
      },
      "source": [
        "## Define the LightningModule\n",
        "\n",
        "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in `spleen_segmentation_3d.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_dice',\n",
        "    dirpath=os.path.join(root_dir, 'checkpoints'),  # Directory to save checkpoints\n",
        "    filename='best-checkpoint_whole_64',  # Filename prefix for saving checkpoints\n",
        "    save_top_k=1,  # Save only the best checkpoint\n",
        "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
        "    verbose=True,  # Log a message when saving the best checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y1-m7FfcI0vQ"
      },
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        )\n",
        "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "\n",
        "        self.common_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "            Spacingd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                pixdim=(1.5, 1.5, 2.0),\n",
        "                mode=(\"bilinear\", \"nearest\"),\n",
        "            ),\n",
        "            ScaleIntensityRanged(\n",
        "                keys=[\"image\"],\n",
        "                a_min=-57,\n",
        "                a_max=164,\n",
        "                b_min=0.0,\n",
        "                b_max=1.0,\n",
        "                clip=True,\n",
        "            ),\n",
        "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # # set up the correct data path\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "        # train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "        # train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "        train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "        train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "\n",
        "                # user can also add other random transforms\n",
        "                #                 RandAffined(\n",
        "                #                     keys=['image', 'label'],\n",
        "                #                     mode=('bilinear', 'nearest'),\n",
        "                #                     prob=1.0,\n",
        "                #                     spatial_size=(96, 96, 96),\n",
        "                #                     rotate_range=(0, 0, np.pi/15),\n",
        "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        self.train_ds = CacheDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "        self.val_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    def perform_inference(self, model, data):\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "            model_output = model(data.unsqueeze(0))\n",
        "        return model_output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True) # log\n",
        "\n",
        "        return {\"log\": tensorboard_logs}\n",
        "    \n",
        "    def infer_on_single_image(self, data):\n",
        "        self._model.eval()\n",
        "        with torch.no_grad():\n",
        "            roi_size = (64, 64, 64)\n",
        "            sw_batch_size = 4\n",
        "            outputs = sliding_window_inference(data, roi_size, sw_batch_size, self._model)\n",
        "            post_processed_outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        return post_processed_outputs\n",
        "    \n",
        "\n",
        "    def infer(self, file_path, label_path = None):\n",
        "        self._model.eval()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self._model.to(device)\n",
        "\n",
        "        data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "        # Apply transformations\n",
        "        data_dict = self.common_transforms(data_dict)\n",
        "        data = data_dict[\"image\"]\n",
        "        label = data_dict[\"label\"]\n",
        "\n",
        "        # Convert data and label to tensors and add batch dimension\n",
        "\n",
        "        data = torch.tensor(data).unsqueeze(0).to(device)\n",
        "        label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # Run inference\n",
        "        predictions = self.infer_on_single_image(data)\n",
        "\n",
        "        # Process and save the predictions as needed\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            # Save or process each prediction here\n",
        "\n",
        "            prediction_np = prediction.cpu().numpy()\n",
        "            # Save as NIfTI file, for testing only\n",
        "            pred_img = nib.Nifti1Image(prediction_np, nib.load(file_path).affine)\n",
        "\n",
        "        # Convert predictions and labels to binary format if necessary\n",
        "        post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "        # Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "        prediction_tensor = post_pred(prediction.to(device))\n",
        "        label_tensor = post_label(label.to(device))\n",
        "\n",
        "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "        dice_score = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "\n",
        "        print(f\"Dice Score: {dice_score:.4f}\")\n",
        "        print(\"Inference complete.\")\n",
        "\n",
        "        return prediction_tensor.cpu(), label_tensor.cpu()\n",
        "    \n",
        "    def dice_score(self, prediction_tensor, label_tensor):\n",
        "        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        # post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "        # prediction_tensor = post_pred(prediction.to(device))\n",
        "        # label_tensor = post_label(label.to(device))\n",
        "\n",
        "        # Compute Dice score\n",
        "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "        dice_score = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "\n",
        "        print(dice_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing Dice score on sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_69282/2421075485.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(device)\n",
            "/tmp/ipykernel_69282/2421075485.py:255: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score: 0.5216\n",
            "Inference complete.\n",
            "Dice Score: 0.6703\n",
            "Inference complete.\n",
            "Dice Score: 0.9771\n",
            "Inference complete.\n",
            "0.5117281079292297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_69282/4272306739.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  model.dice_score(torch.tensor(ensemble_maximum).unsqueeze(0).to(device), torch.tensor(label_whole).unsqueeze(0).to(device))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "file_path = 'nonoverlapping_quadrants/7/quadrant_1_7_CT_HR.nii.gz'\n",
        "label_path = 'nonoverlapping_labels/7/quadrant_1_7_CT_HR_label_airways.nii.gz'\n",
        "\n",
        "pred_1Q, label_1Q = model.infer(file_path, label_path)\n",
        "\n",
        "del model\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_2Q_spatial_size_48_0.7714 at epoch 584.ckpt')\n",
        "file_path = 'nonoverlapping_quadrants/7/quadrant_2_7_CT_HR.nii.gz'\n",
        "label_path = 'nonoverlapping_labels/7/quadrant_2_7_CT_HR_label_airways.nii.gz'\n",
        "\n",
        "pred_2Q, label_2Q = model.infer(file_path, label_path)\n",
        "\n",
        "del model\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_48_0.7861 at epoch 411.ckpt')\n",
        "file_path = 'AeroPath/7/7_CT_HR_label_lungs.nii.gz'\n",
        "label_path = 'AeroPath/7/7_CT_HR_label_lungs.nii.gz'\n",
        "\n",
        "pred_whole, label_whole = model.infer(file_path, label_path)\n",
        "\n",
        "\n",
        "\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Extract the original shape of the predictions\n",
        "    original_shape = predictions.shape\n",
        "    # Compute the scaling factors for interpolation along each axis\n",
        "    scale_factors = [t / o for t, o in zip(target_shape, original_shape)]\n",
        "    # Interpolate the predictions using zoom\n",
        "    interpolated_predictions = zoom(predictions, zoom=scale_factors, mode='nearest')\n",
        "    return interpolated_predictions\n",
        "\n",
        "pred_1Q_copy = pred_1Q\n",
        "pred_2Q_copy = pred_2Q\n",
        "\n",
        "\n",
        "# interpolate the predictions to the same shape as the whole image\n",
        "\n",
        "pred_1Q = interpolate_predictions(pred_1Q, (*list(pred_whole.shape[:3]), pred_1Q.shape[3]))\n",
        "pred_2Q = interpolate_predictions(pred_2Q, (*list(pred_whole.shape[:3]), pred_2Q.shape[3]))\n",
        "\n",
        "\n",
        "# concatenate the predictions\n",
        "\n",
        "pred_Q1_Q2 = np.concatenate((pred_1Q, pred_2Q), axis=3)\n",
        "\n",
        "\n",
        "# use the maximum value to ensemble the predictions\n",
        "\n",
        "ensemble_maximum = np.maximum(pred_Q1_Q2, pred_whole)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.dice_score(torch.tensor(ensemble_maximum).unsqueeze(0).to(device), torch.tensor(label_whole).unsqueeze(0).to(device))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_1Q = interpolate_predictions(pred_1Q, (*list(pred_whole.shape[:3]), pred_1Q.shape[3]))\n",
        "pred_2Q = interpolate_predictions(pred_2Q, (*list(pred_whole.shape[:3]), pred_2Q.shape[3]))\n",
        "\n",
        "pred_Q1_Q2 = np.concatenate((pred_1Q, pred_2Q), axis=3)\n",
        "\n",
        "ensemble_maximum = np.maximum(pred_Q1_Q2, pred_whole)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create overlapping quadrants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape:  (512, 512, 767)\n",
            "Saved: nonoverlapping_quadrants/1/quadrant_1_1_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 383)\n",
            "Saved: nonoverlapping_quadrants/1/quadrant_2_1_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 384)\n",
            "input shape:  (512, 512, 767)\n",
            "Saved: nonoverlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 383)\n",
            "Saved: nonoverlapping_labels/1/quadrant_2_1_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 384)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 829)\n",
            "Saved: nonoverlapping_quadrants/2/quadrant_1_2_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 414)\n",
            "Saved: nonoverlapping_quadrants/2/quadrant_2_2_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 415)\n",
            "input shape:  (512, 512, 829)\n",
            "Saved: nonoverlapping_labels/2/quadrant_1_2_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 414)\n",
            "Saved: nonoverlapping_labels/2/quadrant_2_2_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 415)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 714)\n",
            "Saved: nonoverlapping_quadrants/3/quadrant_1_3_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 357)\n",
            "Saved: nonoverlapping_quadrants/3/quadrant_2_3_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 357)\n",
            "input shape:  (512, 512, 714)\n",
            "Saved: nonoverlapping_labels/3/quadrant_1_3_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 357)\n",
            "Saved: nonoverlapping_labels/3/quadrant_2_3_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 357)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 487, 598)\n",
            "Saved: nonoverlapping_quadrants/4/quadrant_1_4_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 487, 299)\n",
            "Saved: nonoverlapping_quadrants/4/quadrant_2_4_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 487, 299)\n",
            "input shape:  (487, 487, 598)\n",
            "Saved: nonoverlapping_labels/4/quadrant_1_4_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 487, 299)\n",
            "Saved: nonoverlapping_labels/4/quadrant_2_4_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 487, 299)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 619)\n",
            "Saved: nonoverlapping_quadrants/5/quadrant_1_5_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 309)\n",
            "Saved: nonoverlapping_quadrants/5/quadrant_2_5_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "input shape:  (512, 512, 619)\n",
            "Saved: nonoverlapping_labels/5/quadrant_1_5_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 309)\n",
            "Saved: nonoverlapping_labels/5/quadrant_2_5_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 441, 575)\n",
            "Saved: nonoverlapping_quadrants/6/quadrant_1_6_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 441, 287)\n",
            "Saved: nonoverlapping_quadrants/6/quadrant_2_6_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 441, 288)\n",
            "input shape:  (487, 441, 575)\n",
            "Saved: nonoverlapping_labels/6/quadrant_1_6_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 441, 287)\n",
            "Saved: nonoverlapping_labels/6/quadrant_2_6_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 441, 288)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_quadrants/7/quadrant_1_7_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_quadrants/7/quadrant_2_7_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_labels/7/quadrant_1_7_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_labels/7/quadrant_2_7_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 533)\n",
            "Saved: nonoverlapping_quadrants/8/quadrant_1_8_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 266)\n",
            "Saved: nonoverlapping_quadrants/8/quadrant_2_8_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 267)\n",
            "input shape:  (512, 512, 533)\n",
            "Saved: nonoverlapping_labels/8/quadrant_1_8_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 266)\n",
            "Saved: nonoverlapping_labels/8/quadrant_2_8_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 267)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 727)\n",
            "Saved: nonoverlapping_quadrants/9/quadrant_1_9_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 363)\n",
            "Saved: nonoverlapping_quadrants/9/quadrant_2_9_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 364)\n",
            "input shape:  (512, 512, 727)\n",
            "Saved: nonoverlapping_labels/9/quadrant_1_9_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 363)\n",
            "Saved: nonoverlapping_labels/9/quadrant_2_9_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 364)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 241)\n",
            "Saved: nonoverlapping_quadrants/10/quadrant_1_10_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 120)\n",
            "Saved: nonoverlapping_quadrants/10/quadrant_2_10_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 121)\n",
            "input shape:  (512, 512, 241)\n",
            "Saved: nonoverlapping_labels/10/quadrant_1_10_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 120)\n",
            "Saved: nonoverlapping_labels/10/quadrant_2_10_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 121)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 553)\n",
            "Saved: nonoverlapping_quadrants/11/quadrant_1_11_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 276)\n",
            "Saved: nonoverlapping_quadrants/11/quadrant_2_11_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 277)\n",
            "input shape:  (512, 512, 553)\n",
            "Saved: nonoverlapping_labels/11/quadrant_1_11_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 276)\n",
            "Saved: nonoverlapping_labels/11/quadrant_2_11_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 277)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 602)\n",
            "Saved: nonoverlapping_quadrants/12/quadrant_1_12_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 301)\n",
            "Saved: nonoverlapping_quadrants/12/quadrant_2_12_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 301)\n",
            "input shape:  (512, 512, 602)\n",
            "Saved: nonoverlapping_labels/12/quadrant_1_12_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 301)\n",
            "Saved: nonoverlapping_labels/12/quadrant_2_12_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 301)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 620)\n",
            "Saved: nonoverlapping_quadrants/13/quadrant_1_13_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 310)\n",
            "Saved: nonoverlapping_quadrants/13/quadrant_2_13_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "input shape:  (512, 512, 620)\n",
            "Saved: nonoverlapping_labels/13/quadrant_1_13_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 310)\n",
            "Saved: nonoverlapping_labels/13/quadrant_2_13_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 584)\n",
            "Saved: nonoverlapping_quadrants/14/quadrant_1_14_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 292)\n",
            "Saved: nonoverlapping_quadrants/14/quadrant_2_14_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 292)\n",
            "input shape:  (512, 512, 584)\n",
            "Saved: nonoverlapping_labels/14/quadrant_1_14_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 292)\n",
            "Saved: nonoverlapping_labels/14/quadrant_2_14_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 292)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 556)\n",
            "Saved: nonoverlapping_quadrants/15/quadrant_1_15_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 278)\n",
            "Saved: nonoverlapping_quadrants/15/quadrant_2_15_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 278)\n",
            "input shape:  (512, 512, 556)\n",
            "Saved: nonoverlapping_labels/15/quadrant_1_15_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 278)\n",
            "Saved: nonoverlapping_labels/15/quadrant_2_15_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 278)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 843)\n",
            "Saved: nonoverlapping_quadrants/16/quadrant_1_16_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 421)\n",
            "Saved: nonoverlapping_quadrants/16/quadrant_2_16_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 422)\n",
            "input shape:  (512, 512, 843)\n",
            "Saved: nonoverlapping_labels/16/quadrant_1_16_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 421)\n",
            "Saved: nonoverlapping_labels/16/quadrant_2_16_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 422)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 407, 602)\n",
            "Saved: nonoverlapping_quadrants/17/quadrant_1_17_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 407, 301)\n",
            "Saved: nonoverlapping_quadrants/17/quadrant_2_17_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 407, 301)\n",
            "input shape:  (487, 407, 602)\n",
            "Saved: nonoverlapping_labels/17/quadrant_1_17_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 407, 301)\n",
            "Saved: nonoverlapping_labels/17/quadrant_2_17_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 407, 301)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 706)\n",
            "Saved: nonoverlapping_quadrants/18/quadrant_1_18_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 353)\n",
            "Saved: nonoverlapping_quadrants/18/quadrant_2_18_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 353)\n",
            "input shape:  (512, 512, 706)\n",
            "Saved: nonoverlapping_labels/18/quadrant_1_18_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 353)\n",
            "Saved: nonoverlapping_labels/18/quadrant_2_18_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 353)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 666)\n",
            "Saved: nonoverlapping_quadrants/19/quadrant_1_19_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 333)\n",
            "Saved: nonoverlapping_quadrants/19/quadrant_2_19_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 333)\n",
            "input shape:  (512, 512, 666)\n",
            "Saved: nonoverlapping_labels/19/quadrant_1_19_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 333)\n",
            "Saved: nonoverlapping_labels/19/quadrant_2_19_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 333)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 286)\n",
            "Saved: nonoverlapping_quadrants/20/quadrant_1_20_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 143)\n",
            "Saved: nonoverlapping_quadrants/20/quadrant_2_20_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 143)\n",
            "input shape:  (512, 512, 286)\n",
            "Saved: nonoverlapping_labels/20/quadrant_1_20_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 143)\n",
            "Saved: nonoverlapping_labels/20/quadrant_2_20_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 143)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 717)\n",
            "Saved: nonoverlapping_quadrants/21/quadrant_1_21_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 358)\n",
            "Saved: nonoverlapping_quadrants/21/quadrant_2_21_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 359)\n",
            "input shape:  (512, 512, 717)\n",
            "Saved: nonoverlapping_labels/21/quadrant_1_21_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 358)\n",
            "Saved: nonoverlapping_labels/21/quadrant_2_21_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 359)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 329)\n",
            "Saved: nonoverlapping_quadrants/22/quadrant_1_22_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_quadrants/22/quadrant_2_22_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 165)\n",
            "input shape:  (512, 512, 329)\n",
            "Saved: nonoverlapping_labels/22/quadrant_1_22_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_labels/22/quadrant_2_22_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 165)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 694)\n",
            "Saved: nonoverlapping_quadrants/23/quadrant_1_23_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 347)\n",
            "Saved: nonoverlapping_quadrants/23/quadrant_2_23_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 347)\n",
            "input shape:  (512, 512, 694)\n",
            "Saved: nonoverlapping_labels/23/quadrant_1_23_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 347)\n",
            "Saved: nonoverlapping_labels/23/quadrant_2_23_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 347)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 607)\n",
            "Saved: nonoverlapping_quadrants/24/quadrant_1_24_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 303)\n",
            "Saved: nonoverlapping_quadrants/24/quadrant_2_24_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 304)\n",
            "input shape:  (512, 512, 607)\n",
            "Saved: nonoverlapping_labels/24/quadrant_1_24_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 303)\n",
            "Saved: nonoverlapping_labels/24/quadrant_2_24_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 304)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 328)\n",
            "Saved: nonoverlapping_quadrants/25/quadrant_1_25_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_quadrants/25/quadrant_2_25_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 164)\n",
            "input shape:  (512, 512, 328)\n",
            "Saved: nonoverlapping_labels/25/quadrant_1_25_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_labels/25/quadrant_2_25_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 164)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 265)\n",
            "Saved: nonoverlapping_quadrants/26/quadrant_1_26_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 132)\n",
            "Saved: nonoverlapping_quadrants/26/quadrant_2_26_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 133)\n",
            "input shape:  (512, 512, 265)\n",
            "Saved: nonoverlapping_labels/26/quadrant_1_26_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 132)\n",
            "Saved: nonoverlapping_labels/26/quadrant_2_26_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 133)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_quadrants/27/quadrant_1_27_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_quadrants/27/quadrant_2_27_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_labels/27/quadrant_1_27_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_labels/27/quadrant_2_27_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "\n",
            "###########\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "def create_overlapping_quadrants(image_shape, overlap_percentage=0.0):\n",
        "    x, y, z = image_shape\n",
        "    overlap_z = int(z * overlap_percentage)\n",
        "    if z < 3 * overlap_z:\n",
        "        return [(0, z)]\n",
        "    mid_z = z // 2\n",
        "    return [\n",
        "        (0, mid_z + overlap_z),\n",
        "        (mid_z - overlap_z, z)\n",
        "    ]\n",
        "\n",
        "def load_nifti_file(file_path):\n",
        "    return nib.load(file_path)\n",
        "\n",
        "def save_nifti_file(data, affine, file_path):\n",
        "    img = nib.Nifti1Image(data, affine)\n",
        "    nib.save(img, file_path)\n",
        "\n",
        "def process_file(file_path, output_dir):\n",
        "    img = load_nifti_file(file_path)\n",
        "    data = img.get_fdata()\n",
        "    print(\"input shape: \", data.shape)\n",
        "    affine = img.affine\n",
        "    quadrants = create_overlapping_quadrants(data.shape)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for i, (start, end) in enumerate(quadrants):\n",
        "        quadrant_data = data[:, :, start:end]\n",
        "        quadrant_file_path = os.path.join(output_dir, f\"quadrant_{i+1}_{os.path.basename(file_path)}\")\n",
        "        save_nifti_file(quadrant_data, affine, quadrant_file_path)\n",
        "        print(f\"Saved: {quadrant_file_path}\")\n",
        "        print(f'Quadrant {i} shape: {quadrant_data.shape}')\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "train_labels = natsorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "train_images = natsorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "\n",
        "\n",
        "for idx, (image_name, label_name) in enumerate(zip(train_images, train_labels)):\n",
        "    output_dir = f'nonoverlapping_quadrants/{idx + 1}'\n",
        "    process_file(image_name, output_dir)\n",
        "    output_dir = f'nonoverlapping_labels/{idx + 1}'\n",
        "    process_file(label_name, output_dir)\n",
        "    \n",
        "    print('')\n",
        "    print('###########')\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/1/1_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/10/10_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/11/11_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/12/12_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/13/13_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/14/14_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/15/15_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/16/16_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/17/17_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/18/18_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/19/19_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/2/2_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/20/20_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/21/21_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/22/22_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/23/23_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/24/24_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/25/25_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/26/26_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/27/27_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/3/3_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/4/4_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/5/5_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/6/6_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/7/7_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/8/8_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/9/9_CT_HR.nii.gz']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "train_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img shape: (512, 512, 362), label shape: (512, 512, 362)\n"
          ]
        }
      ],
      "source": [
        "# Check dimensions of random sample\n",
        "\n",
        "img = 'nonoverlapping_quadrants/24/quadrant_2_7_CT_HR.nii.gz'\n",
        "label = 'nonoverlapping_labels/24/quadrant_2_7_CT_HR_label_airways.nii.gz'\n",
        "img = nib.load(img).get_fdata()\n",
        "label = nib.load(label).get_fdata()\n",
        "\n",
        "print(f'img shape: {img.shape}, label shape: {label.shape}')\n",
        "\n",
        "\n",
        "# whole\n",
        "# img shape: (512, 512, 723), label shape: (512, 512, 723)\n",
        "\n",
        "# 1q\n",
        "# img shape: (512, 512, 361), label shape: (512, 512, 361)\n",
        "\n",
        "# 2q\n",
        "# img shape: (512, 512, 362), label shape: (512, 512, 362)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVCubC2I0vR"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8BpsB6sI0vR",
        "scrolled": false,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# initialise the LightningModule\n",
        "net = Net()\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TucnezFI0vR",
        "outputId": "4e0a75d6-eb09-4989-bb2b-a2c6b1388edf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f\"train completed, best_metric: {net.best_val_dice:.4f} \" f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "\n",
        "\n",
        "# Load the model weights from the checkpoint file\n",
        "checkpoint_path = 'best-checkpoint.ckpt'\n",
        "model = Net.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dice score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 259, 259, 182])\n",
            "(2, 259, 259, 364)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from monai.data.meta_tensor import MetaTensor\n",
        "\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def infer(img): #TODO metoda Infer powinna być dodana do klasy Net()\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    \n",
        "def merge_predictions(pred1: MetaTensor, pred2: MetaTensor) -> MetaTensor:\n",
        "    pred_merged = np.concatenate((pred1, pred2), axis=3)\n",
        "    return pred_merged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "\n",
        "file_paths_1Q = natsorted(glob.glob('nonoverlapping_labels/*/quadrant_1_*_CT_HR_label_airways.nii.gz', recursive=True))\n",
        "label_paths_1Q = natsorted(glob.glob('nonoverlapping_quadrants/*/quadrant_1_*_CT_HR.nii.gz', recursive=True))\n",
        "\n",
        "\n",
        "file_paths_2Q = natsorted(glob.glob('nonoverlapping_labels/*/quadrant_2_*_CT_HR_label_airways.nii.gz', recursive=True))\n",
        "label_paths_2Q = natsorted(glob.glob('nonoverlapping_quadrants/*/quadrant_2_*_CT_HR.nii.gz', recursive=True))\n",
        "\n",
        "file_paths_whole = natsorted(glob.glob('AeroPath/*/*_CT_HR.nii.gz', recursive=True))\n",
        "label_paths_whole = natsorted(glob.glob('AeroPath/*/*_CT_HR_label_airways.nii.gz', recursive=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/tmp/ipykernel_47506/3465595338.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(device)\n",
            "/tmp/ipykernel_47506/3465595338.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input nii gz shape:  (512, 512, 767)\n",
            "input shape:  torch.Size([1, 1, 234, 234, 192])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 829)\n",
            "input shape:  torch.Size([1, 1, 254, 213, 208])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 714)\n",
            "input shape:  torch.Size([1, 1, 235, 214, 179])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (487, 487, 598)\n",
            "input shape:  torch.Size([1, 1, 220, 209, 150])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 619)\n",
            "input shape:  torch.Size([1, 1, 222, 187, 156])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (487, 441, 575)\n",
            "input shape:  torch.Size([1, 1, 216, 168, 144])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 723)\n",
            "input shape:  torch.Size([1, 1, 259, 194, 182])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 533)\n",
            "input shape:  torch.Size([1, 1, 251, 221, 134])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 727)\n",
            "input shape:  torch.Size([1, 1, 257, 230, 182])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 241)\n",
            "input shape:  torch.Size([1, 1, 240, 214, 151])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 553)\n",
            "input shape:  torch.Size([1, 1, 233, 199, 139])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 602)\n",
            "input shape:  torch.Size([1, 1, 231, 214, 151])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 620)\n",
            "input shape:  torch.Size([1, 1, 231, 225, 156])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 584)\n",
            "input shape:  torch.Size([1, 1, 260, 216, 147])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 556)\n",
            "input shape:  torch.Size([1, 1, 282, 238, 140])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 843)\n",
            "input shape:  torch.Size([1, 1, 234, 201, 212])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (487, 407, 602)\n",
            "input shape:  torch.Size([1, 1, 245, 198, 151])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 706)\n",
            "input shape:  torch.Size([1, 1, 257, 240, 177])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 666)\n",
            "input shape:  torch.Size([1, 1, 253, 214, 167])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 286)\n",
            "input shape:  torch.Size([1, 1, 217, 185, 143])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 717)\n",
            "input shape:  torch.Size([1, 1, 261, 230, 180])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 329)\n",
            "input shape:  torch.Size([1, 1, 254, 233, 165])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 694)\n",
            "input shape:  torch.Size([1, 1, 193, 162, 174])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 607)\n",
            "input shape:  torch.Size([1, 1, 237, 226, 152])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 328)\n",
            "input shape:  torch.Size([1, 1, 224, 183, 164])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 265)\n",
            "input shape:  torch.Size([1, 1, 267, 182, 165])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 723)\n",
            "input shape:  torch.Size([1, 1, 278, 223, 181])\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# Model inference and Dice score calculation\n",
        "# Load the trained model\n",
        "# del model\n",
        "# model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "# model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_2Q_spatial_size_48_0.7714 at epoch 584.ckpt')\n",
        "moel = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_48_0.7861 at epoch 411.ckpt')\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert data and label to tensors and add batch dimension\n",
        "for idx, (file_path, label_path) in enumerate(zip(file_paths_whole, label_paths_whole)):\n",
        "    prediction_filename = f'predictions_whole/whole_{idx+1}_prediction.nii.gz'\n",
        "\n",
        "    data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "\n",
        "    # Apply transformations\n",
        "    data_dict = common_transforms(data_dict)\n",
        "    data = data_dict[\"image\"]\n",
        "    label = data_dict[\"label\"]\n",
        "\n",
        "    data = torch.tensor(data).unsqueeze(0).to(device)\n",
        "    label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "    data_nii = nib.load(file_path).get_fdata()\n",
        "    label_nii = nib.load(label_path).get_fdata()\n",
        "\n",
        "    print(\"input nii gz shape: \", data_nii.shape)\n",
        "    print(\"input shape: \", data.shape)\n",
        "\n",
        "    # Function to run inference\n",
        "    def infer_on_single_image(model, data):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            roi_size = (48, 48, 48)\n",
        "            sw_batch_size = 4\n",
        "            outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "            post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        return post_processed_outputs\n",
        "\n",
        "    # Run inference\n",
        "    predictions = infer_on_single_image(model, data)\n",
        "\n",
        "    # Process and save the predictions as needed\n",
        "    for i, prediction in enumerate(predictions):\n",
        "        # Save or process each prediction here\n",
        "        # For example, save as NIfTI file\n",
        "        prediction_np = prediction.cpu().numpy()\n",
        "        print(type(prediction_np))\n",
        "        pred_img = nib.Nifti1Image(prediction_np, nib.load(file_path).affine)\n",
        "        nib.save(pred_img, prediction_filename)\n",
        "\n",
        "        # label_np = label.cpu().numpy()\n",
        "        # label_img = nib.Nifti1Image(label_np, nib.load(label_path).affine)\n",
        "        # nib.save(label_img, f'labels_whole/whole_{idx+1}_label.nii.gz')\n",
        "\n",
        "\n",
        "while False:\n",
        "    # Convert predictions and labels to binary format if necessary\n",
        "    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    # Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "    prediction_tensor = post_pred(prediction.to(device))\n",
        "    label_tensor = post_label(label.to(device))\n",
        "\n",
        "    print('pred type:', type(prediction_tensor))\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    print(\"input shape: \", data.shape)\n",
        "    print(\"output shape: \", pred_img.shape)\n",
        "    print(f\"Dice Score: {dice_score:.4f}\")\n",
        "    print(\"Inference complete.\")\n",
        "\n",
        "    pred_nii = nib.load(prediction_filename).get_fdata()\n",
        "\n",
        "    print(\"output nii gz shape: \", pred_nii.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_1Q_paths = natsorted(glob.glob('predictions1Q/*', recursive=True))\n",
        "pred_2Q_paths = natsorted(glob.glob('predictions2Q/*', recursive=True))\n",
        "pred_whole_paths = natsorted(glob.glob('predictions_whole/*', recursive=True))\n",
        "label_paths = natsorted(glob.glob('nonoverlapping_labels/*/*_CT_HR_label_airways.nii.gz', recursive=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pred_1Q shape: (2, 234, 234, 96), pred_2Q shape: (2, 234, 234, 97), pred_whole shape: (2, 234, 234, 192), label shape: (512, 512, 383)\n",
            "difference merged =  0.001886399055202474 %\n",
            "difference ensembled =  0.000943199527601237 %\n",
            "pred_1Q shape: (2, 254, 254, 104), pred_2Q shape: (2, 254, 254, 104), pred_whole shape: (2, 254, 213, 208), label shape: (512, 512, 384)\n",
            "difference merged =  0.0014681136485783263 %\n",
            "difference ensembled =  0.0007340568242891632 %\n",
            "pred_1Q shape: (2, 235, 235, 90), pred_2Q shape: (2, 235, 235, 90), pred_whole shape: (2, 235, 214, 179), label shape: (512, 512, 414)\n",
            "difference merged =  0.0009895677695066936 %\n",
            "difference ensembled =  0.0004947838847533468 %\n",
            "pred_1Q shape: (2, 220, 220, 76), pred_2Q shape: (2, 220, 220, 76), pred_whole shape: (2, 220, 209, 150), label shape: (512, 512, 415)\n",
            "difference merged =  0.001497317674351167 %\n",
            "difference ensembled =  0.0007486588371755836 %\n",
            "pred_1Q shape: (2, 222, 222, 78), pred_2Q shape: (2, 222, 222, 78), pred_whole shape: (2, 222, 187, 156), label shape: (512, 512, 357)\n",
            "difference merged =  0.001098177568765804 %\n",
            "difference ensembled =  0.000549088784382902 %\n",
            "pred_1Q shape: (2, 216, 196, 72), pred_2Q shape: (2, 216, 196, 73), pred_whole shape: (2, 216, 168, 144), label shape: (512, 512, 357)\n",
            "difference merged =  0.0010561725333137371 %\n",
            "difference ensembled =  0.0005280862666568686 %\n",
            "pred_1Q shape: (2, 259, 259, 91), pred_2Q shape: (2, 259, 259, 91), pred_whole shape: (2, 259, 194, 182), label shape: (487, 487, 299)\n",
            "difference merged =  0.000901061283977337 %\n",
            "difference ensembled =  0.0004505306419886685 %\n",
            "pred_1Q shape: (2, 251, 251, 67), pred_2Q shape: (2, 251, 251, 68), pred_whole shape: (2, 251, 221, 134), label shape: (487, 487, 299)\n",
            "difference merged =  0.0008698911384918891 %\n",
            "difference ensembled =  0.00043494556924594455 %\n",
            "pred_1Q shape: (2, 257, 257, 92), pred_2Q shape: (2, 257, 257, 92), pred_whole shape: (2, 257, 230, 182), label shape: (512, 512, 309)\n",
            "difference merged =  0.001424518638188068 %\n",
            "difference ensembled =  0.000712259319094034 %\n",
            "pred_1Q shape: (2, 240, 240, 75), pred_2Q shape: (2, 240, 240, 76), pred_whole shape: (2, 240, 214, 151), label shape: (512, 512, 310)\n",
            "difference merged =  0.002087975284190547 %\n",
            "difference ensembled =  0.0010439876420952736 %\n",
            "pred_1Q shape: (2, 233, 233, 70), pred_2Q shape: (2, 233, 233, 70), pred_whole shape: (2, 233, 199, 139), label shape: (487, 441, 287)\n",
            "difference merged =  0.0013833951925310314 %\n",
            "difference ensembled =  0.0006916975962655157 %\n",
            "pred_1Q shape: (2, 231, 231, 76), pred_2Q shape: (2, 231, 231, 76), pred_whole shape: (2, 231, 214, 151), label shape: (487, 441, 288)\n",
            "difference merged =  0.0015680818119389636 %\n",
            "difference ensembled =  0.0007840409059694818 %\n",
            "pred_1Q shape: (2, 231, 231, 78), pred_2Q shape: (2, 231, 231, 78), pred_whole shape: (2, 231, 225, 156), label shape: (512, 512, 361)\n",
            "difference merged =  0.0012304978971645639 %\n",
            "difference ensembled =  0.0006152489485822819 %\n",
            "pred_1Q shape: (2, 260, 260, 74), pred_2Q shape: (2, 260, 260, 74), pred_whole shape: (2, 260, 216, 147), label shape: (512, 512, 362)\n",
            "difference merged =  0.001249224761129523 %\n",
            "difference ensembled =  0.0006246123805647615 %\n",
            "pred_1Q shape: (2, 282, 282, 70), pred_2Q shape: (2, 282, 282, 70), pred_whole shape: (2, 282, 238, 140), label shape: (512, 512, 266)\n",
            "difference merged =  0.0008438481775688999 %\n",
            "difference ensembled =  0.00042192408878444996 %\n",
            "pred_1Q shape: (2, 234, 234, 106), pred_2Q shape: (2, 234, 234, 106), pred_whole shape: (2, 234, 201, 212), label shape: (512, 512, 267)\n",
            "difference merged =  0.001779724181864424 %\n",
            "difference ensembled =  0.000889862090932212 %\n",
            "pred_1Q shape: (2, 245, 205, 76), pred_2Q shape: (2, 245, 205, 76), pred_whole shape: (2, 245, 198, 151), label shape: (512, 512, 363)\n",
            "difference merged =  0.0012156979990471002 %\n",
            "difference ensembled =  0.0006078489995235501 %\n",
            "pred_1Q shape: (2, 257, 257, 89), pred_2Q shape: (2, 257, 257, 89), pred_whole shape: (2, 257, 240, 177), label shape: (512, 512, 364)\n",
            "difference merged =  0.0012394937970351806 %\n",
            "difference ensembled =  0.0006197468985175903 %\n",
            "pred_1Q shape: (2, 253, 253, 84), pred_2Q shape: (2, 253, 253, 84), pred_whole shape: (2, 253, 214, 167), label shape: (512, 512, 120)\n",
            "difference merged =  0.0018074006764646614 %\n",
            "difference ensembled =  0.0009037003382323307 %\n",
            "pred_1Q shape: (2, 217, 217, 72), pred_2Q shape: (2, 217, 217, 72), pred_whole shape: (2, 217, 185, 143), label shape: (512, 512, 121)\n",
            "difference merged =  0.00179645986097599 %\n",
            "difference ensembled =  0.000898229930487995 %\n",
            "pred_1Q shape: (2, 261, 261, 90), pred_2Q shape: (2, 261, 261, 90), pred_whole shape: (2, 261, 230, 180), label shape: (512, 512, 276)\n",
            "difference merged =  0.0013896755326040684 %\n",
            "difference ensembled =  0.0006948377663020342 %\n",
            "pred_1Q shape: (2, 254, 254, 82), pred_2Q shape: (2, 254, 254, 83), pred_whole shape: (2, 254, 233, 165), label shape: (512, 512, 277)\n",
            "difference merged =  0.001985656982108606 %\n",
            "difference ensembled =  0.000992828491054303 %\n",
            "pred_1Q shape: (2, 193, 193, 87), pred_2Q shape: (2, 193, 193, 87), pred_whole shape: (2, 193, 162, 174), label shape: (512, 512, 301)\n",
            "difference merged =  0.0012254874929323544 %\n",
            "difference ensembled =  0.0006127437464661772 %\n",
            "pred_1Q shape: (2, 237, 237, 76), pred_2Q shape: (2, 237, 237, 77), pred_whole shape: (2, 237, 226, 152), label shape: (512, 512, 301)\n",
            "difference merged =  0.0014054052460601488 %\n",
            "difference ensembled =  0.0007027026230300744 %\n",
            "pred_1Q shape: (2, 224, 224, 82), pred_2Q shape: (2, 224, 224, 82), pred_whole shape: (2, 224, 183, 164), label shape: (512, 512, 310)\n",
            "difference merged =  0.0014717327354772376 %\n",
            "difference ensembled =  0.0007358663677386188 %\n",
            "pred_1Q shape: (2, 267, 267, 83), pred_2Q shape: (2, 267, 267, 83), pred_whole shape: (2, 267, 182, 165), label shape: (512, 512, 310)\n",
            "difference merged =  0.0016704893109387492 %\n",
            "difference ensembled =  0.0008352446554693746 %\n",
            "pred_1Q shape: (2, 278, 278, 91), pred_2Q shape: (2, 278, 278, 91), pred_whole shape: (2, 278, 223, 181), label shape: (512, 512, 292)\n",
            "difference merged =  0.0014063025525371641 %\n",
            "difference ensembled =  0.0007031512762685821 %\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Define a function to interpolate predictions to match the dimensions of the WHOLE model\n",
        "# def interpolate_predictions(predictions, target_shape):\n",
        "#     # Extract the original shape of the predictions\n",
        "#     original_shape = predictions.shape\n",
        "#     # Compute the scaling factors for interpolation along each axis\n",
        "#     scale_factors = [t / o for t, o in zip(target_shape, original_shape)]\n",
        "#     # Interpolate the predictions using zoom\n",
        "#     interpolated_predictions = zoom(predictions, zoom=scale_factors, mode='nearest')\n",
        "#     return interpolated_predictions\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Ensure predictions tensor has 5 dimensions\n",
        "    predictions = torch.tensor(predictions)\n",
        "    if predictions.dim() == 6:\n",
        "        predictions = predictions.squeeze(0)  # Remove batch dimension if present\n",
        "    \n",
        "    # Interpolate the predictions using bilinear interpolation\n",
        "    interpolated_predictions = F.interpolate(predictions, size=target_shape[2:], mode='nearest')\n",
        "    \n",
        "    return interpolated_predictions\n",
        "\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "for idx, (pred_1Q_path, pred_2Q_path, pred_whole_path, label_path) in enumerate(zip(pred_1Q_paths, pred_2Q_paths, pred_whole_paths, label_paths)):\n",
        "    pred_1Q = nib.load(pred_1Q_path).get_fdata()\n",
        "    pred_2Q = nib.load(pred_2Q_path).get_fdata()\n",
        "    pred_whole = nib.load(pred_whole_path).get_fdata()\n",
        "    label = nib.load(label_path).get_fdata()\n",
        "\n",
        "    print(f'pred_1Q shape: {pred_1Q.shape}, pred_2Q shape: {pred_2Q.shape}, pred_whole shape: {pred_whole.shape}, label shape: {label.shape}')\n",
        "\n",
        "    merged =np.concatenate((pred_1Q, pred_2Q), axis=3)\n",
        "    # print(f'merged shape: {merged.shape}')\n",
        "\n",
        "    interpolated_merged = interpolate_predictions(merged, pred_whole.shape)\n",
        "\n",
        "    # print(f'interpolated_merged shape: {interpolated_merged.shape}')\n",
        "\n",
        "    # Save the interpolated predictions\n",
        "    interpolated_merged = interpolated_merged.cpu().numpy()\n",
        "    merged = nib.Nifti1Image(interpolated_merged, nib.load(pred_whole_path).affine)\n",
        "    nib.save(merged, f'predictions_merged/merged_{idx+1}_prediction.nii.gz')\n",
        "\n",
        "    ensembled = np.maximum(interpolated_merged, pred_whole)\n",
        "    ensembled_nifti = nib.Nifti1Image(ensembled, nib.load(pred_whole_path).affine)\n",
        "\n",
        "    print('difference merged = ', (interpolated_merged != pred_whole).sum()/pred_whole.size, '%')\n",
        "    print('difference ensembled = ', 100 * (ensembled != pred_whole).sum()/pred_whole.size, '%')\n",
        "\n",
        "    nib.save(ensembled_nifti, f'predictions_ensembled/ensembled_{idx+1}_prediction.nii.gz')\n",
        "\n",
        "    continue \n",
        "\n",
        "    #TODO FIX THE DICE SCORE CALCULATION (COMMON TRANSISTIONS)\n",
        "    prediction = torch.tensor(ensembled).unsqueeze(0).to(device)\n",
        "\n",
        "    data_dict = {\"image\": pred_whole_path, \"label\": label_path}\n",
        "    data_dict = common_transforms(data_dict)\n",
        "    data = data_dict[\"image\"]\n",
        "    label = data_dict[\"label\"]\n",
        "\n",
        "    label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    prediction_tensor = post_pred(prediction.to(device))\n",
        "    label_tensor = post_label(label.to(device))\n",
        "\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    print('DICE: ', dice_score)\n",
        "    print('idx: ', idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1, 257, 230, 182)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_ensemble = nib.load('predictions_whole/whole_9_prediction.nii.gz').get_fdata()\n",
        "\n",
        "label = nib.load('labels_whole/whole_9_label.nii.gz').get_fdata()\n",
        "\n",
        "pred_ensemble.shape\n",
        "label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ensembled: 0.668695867061615, whole: 0.669117271900177, difference: 0.0943199527601237%\n",
            "ensembled: 0.6395779848098755, whole: 0.6395779848098755, difference: 0.07340568242891632%\n",
            "ensembled: 0.5913287997245789, whole: 0.5912396311759949, difference: 0.04947838847533468%\n",
            "ensembled: 0.6233521103858948, whole: 0.6249682307243347, difference: 0.07486588371755835%\n",
            "ensembled: 0.6002840995788574, whole: 0.5980216264724731, difference: 0.054908878438290204%\n",
            "ensembled: 0.6032496690750122, whole: 0.6032496690750122, difference: 0.05280862666568685%\n",
            "ensembled: 0.6036602854728699, whole: 0.6038090586662292, difference: 0.04505306419886685%\n",
            "ensembled: 0.5743035674095154, whole: 0.5768281817436218, difference: 0.04349455692459445%\n",
            "ensembled: 0.6304212808609009, whole: 0.6304212808609009, difference: 0.0712259319094034%\n",
            "ensembled: 0.68426513671875, whole: 0.6845105290412903, difference: 0.10439876420952734%\n",
            "ensembled: 0.6173056364059448, whole: 0.6173912286758423, difference: 0.06916975962655157%\n",
            "ensembled: 0.6491787433624268, whole: 0.6491787433624268, difference: 0.07840409059694818%\n",
            "ensembled: 0.5919966697692871, whole: 0.5921018123626709, difference: 0.061524894858228193%\n",
            "ensembled: 0.6380487680435181, whole: 0.6381813883781433, difference: 0.06246123805647615%\n",
            "ensembled: 0.6067380905151367, whole: 0.6082671880722046, difference: 0.042192408878445%\n",
            "ensembled: 0.6403195261955261, whole: 0.6402636170387268, difference: 0.0889862090932212%\n",
            "ensembled: 0.6571524143218994, whole: 0.6571524143218994, difference: 0.06078489995235501%\n",
            "ensembled: 0.5924535393714905, whole: 0.5926820039749146, difference: 0.06197468985175903%\n",
            "ensembled: 0.6561290621757507, whole: 0.6561290621757507, difference: 0.09037003382323307%\n",
            "ensembled: 0.6357117295265198, whole: 0.6358718276023865, difference: 0.0898229930487995%\n",
            "ensembled: 0.6300366520881653, whole: 0.6300126314163208, difference: 0.06948377663020341%\n",
            "ensembled: 0.6372061967849731, whole: 0.6372374296188354, difference: 0.0992828491054303%\n",
            "ensembled: 0.5918018221855164, whole: 0.5921297073364258, difference: 0.06127437464661771%\n",
            "ensembled: 0.6370023488998413, whole: 0.6370594501495361, difference: 0.07027026230300744%\n",
            "ensembled: 0.6287682056427002, whole: 0.6287682056427002, difference: 0.07358663677386189%\n",
            "ensembled: 0.6769058704376221, whole: 0.6769058704376221, difference: 0.08352446554693746%\n",
            "ensembled: 0.6386497020721436, whole: 0.6386497020721436, difference: 0.07031512762685821%\n"
          ]
        }
      ],
      "source": [
        "ensemble_paths = natsorted(glob.glob('predictions_ensembled/*', recursive=True))\n",
        "whole_paths = natsorted(glob.glob('predictions_whole/*', recursive=True))\n",
        "labels_paths = natsorted(glob.glob('labels_whole/*', recursive=True))\n",
        "\n",
        "\n",
        "def calc_dice_score(outputs, labels, device=\"cuda\"):\n",
        "    # Convert outputs and labels to tensors and apply post-processing transformations\n",
        "    outputs = [EnsureType(\"tensor\", device=device)(output) for output in outputs]\n",
        "    labels = [EnsureType(\"tensor\", device=device)(label) for label in labels]\n",
        "    outputs = [AsDiscrete(argmax=True, to_onehot=2)(output) for output in outputs]\n",
        "    labels = [AsDiscrete(to_onehot=2)(label) for label in labels]\n",
        "\n",
        "    # Initialize DiceMetric with appropriate settings\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "\n",
        "    # Compute Dice score\n",
        "    for output, label in zip(outputs, labels):\n",
        "        dice_metric(y_pred=output, y=label)\n",
        "\n",
        "    # Aggregate Dice scores\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "\n",
        "    return dice_score\n",
        "\n",
        "for idx, (ensemble, whole, label) in enumerate(zip(ensemble_paths, whole_paths, labels_paths)):\n",
        "\n",
        "    pred_ensemble = nib.load(ensemble).get_fdata()\n",
        "    pred_whole = nib.load(whole).get_fdata()\n",
        "\n",
        "    label = nib.load(label).get_fdata()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    dice_ensembled = calc_dice_score([pred_ensemble], [label])\n",
        "    dice_whole = calc_dice_score([pred_whole], [label])\n",
        "\n",
        "    print(f'ensembled: {dice_ensembled}, whole: {dice_whole}, difference: {100 * (pred_ensemble != pred_whole).sum() / pred_whole.size}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolate ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_1Q_paths = natsorted(glob.glob('predictions1Q/*', recursive=True))\n",
        "pred_2Q_paths = natsorted(glob.glob('predictions2Q/*', recursive=True))\n",
        "pred_whole_paths = natsorted(glob.glob('predictions_whole/*', recursive=True))\n",
        "label_paths = natsorted(glob.glob('AeroPath/*/*_CT_HR_label_airways.nii.gz', recursive=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_whole = nib.load(pred_whole_path).get_fdata()\n",
        "label = nib.load(label_path).get_fdata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0TIqDI0vR"
      },
      "source": [
        "## View training in tensorboard\n",
        "\n",
        "Please uncomment the following cell to load tensorboard results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dteU4yMVI0vR",
        "outputId": "8aa4354d-4d1a-4e57-d50e-d9a6eb88db5f"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDZoq7ZI0vS"
      },
      "source": [
        "## Check best model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pp1HWrqI0vS",
        "jupyter": {
          "outputs_hidden": true
        },
        "lines_to_next_cell": 2,
        "outputId": "c7388a09-ec21-44bb-9747-1f1c8d1f81f9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net.to(device)\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(net.val_dataloader()):\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, net)\n",
        "        # plot the slice [:, :, 80]\n",
        "        plt.figure(\"check\", (18, 6))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(f\"image {i}\")\n",
        "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(f\"label {i}\")\n",
        "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(f\"output {i}\")\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzE-kjOI0vS"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhZOGgOI0vS"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
