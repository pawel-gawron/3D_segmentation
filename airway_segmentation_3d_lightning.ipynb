{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3obUW0I7I0vL"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.\n",
        "\n",
        "# Spleen 3D segmentation with MONAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-YQ1gBbI0vL"
      },
      "source": [
        "This tutorial demonstrates how MONAI can be used in conjunction with the [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) framework.\n",
        "\n",
        "We demonstrate use of the following MONAI features:\n",
        "1. Transforms for dictionary format data.\n",
        "1. Loading Nifti images with metadata.\n",
        "1. Add channel dim to the data if no channel dimension.\n",
        "1. Scaling medical image intensity with expected range.\n",
        "1. Croping out a batch of balanced images based on  the positive / negative label ratio.\n",
        "1. Cache IO and transforms to accelerate training and validation.\n",
        "1. Use of a a 3D UNet model, Dice loss function, and mean Dice metric for a 3D segmentation task.\n",
        "1. The sliding window inference method.\n",
        "1. Deterministic training for reproducibility.\n",
        "\n",
        "The Spleen dataset can be downloaded from http://medicaldecathlon.com/.\n",
        "\n",
        "![spleen](http://medicaldecathlon.com/img/spleen0.png)\n",
        "\n",
        "Target: Spleen  \n",
        "Modality: CT  \n",
        "Size: 61 3D volumes (41 Training + 20 Testing)  \n",
        "Source: Memorial Sloan Kettering Cancer Center  \n",
        "Challenge: Large ranging foreground size\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d_lightning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTe3HtVqI0vM"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igaMSdU8I0vM",
        "outputId": "fbf973c9-5926-4875-cda6-903844ab9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: python: command not found\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: line 1: python: command not found\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!pip install -q pytorch-lightning~=2.0\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7oM0xTwI0vN"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7HEKxFEKI0vN",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
            "2024-06-10 16:06:01.913735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONAI version: 1.3.0\n",
            "Numpy version: 1.23.5\n",
            "Pytorch version: 2.0.0+cu117\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
            "MONAI __file__: /home/<username>/.local/lib/python3.10/site-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 5.2.1\n",
            "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "scipy version: 1.8.0\n",
            "Pillow version: 9.0.1\n",
            "Tensorboard version: 2.12.2\n",
            "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "TorchVision version: 0.15.1+cu117\n",
            "tqdm version: 4.65.0\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.7\n",
            "pandas version: 1.5.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    EnsureType,\n",
        "    EnsureTyped,\n",
        ")\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import nibabel as nib\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzzE3GGNI0vO"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1f_XjbPrI0vO",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/tmp8moyekwr\n"
          ]
        }
      ],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YTzgAPbI0vP"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "Downloads and extracts the dataset.\n",
        "The dataset comes from http://medicaldecathlon.com/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/records/10069289/files/AeroPath.zip?download=1\"\n",
        "md5 = \"3fd5106c175c85d60eaece220f5dfd87\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"AeroPath.zip\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQVxGHhI0vQ"
      },
      "source": [
        "## Define the LightningModule\n",
        "\n",
        "The LightningModule contains a refactoring of your training code. The following module is a refactoring of the code in `spleen_segmentation_3d.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y1-m7FfcI0vQ"
      },
      "outputs": [],
      "source": [
        "class Net(pytorch_lightning.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = UNet(\n",
        "            spatial_dims=3,\n",
        "            in_channels=1,\n",
        "            out_channels=2,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2),\n",
        "            num_res_units=2,\n",
        "            norm=Norm.BATCH,\n",
        "        )\n",
        "        self.loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
        "        self.post_pred = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        self.post_label = Compose([EnsureType(\"tensor\", device=\"cpu\"), AsDiscrete(to_onehot=2)])\n",
        "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "        self.best_val_dice = 0\n",
        "        self.best_val_epoch = 0\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "\n",
        "        self.common_transforms = Compose(\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "            Spacingd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                pixdim=(1.5, 1.5, 2.0),\n",
        "                mode=(\"bilinear\", \"nearest\"),\n",
        "            ),\n",
        "            ScaleIntensityRanged(\n",
        "                keys=[\"image\"],\n",
        "                a_min=-57,\n",
        "                a_max=164,\n",
        "                b_min=0.0,\n",
        "                b_max=1.0,\n",
        "                clip=True,\n",
        "            ),\n",
        "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._model(x)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # # set up the correct data path\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "        # train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        # pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "        # train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "        train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "        train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "        data_dicts = [\n",
        "            {\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)\n",
        "        ]\n",
        "        train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
        "\n",
        "        # set deterministic training for reproducibility\n",
        "        set_determinism(seed=0)\n",
        "\n",
        "        # define the data transforms\n",
        "        train_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "                # randomly crop out patch samples from\n",
        "                # big image based on pos / neg ratio\n",
        "                # the image centers of negative samples\n",
        "                # must be in valid image area\n",
        "\n",
        "                RandCropByPosNegLabeld(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    label_key=\"label\",\n",
        "                    spatial_size=(64, 64, 64),\n",
        "                    pos=1,\n",
        "                    neg=1,\n",
        "                    num_samples=4,\n",
        "                    image_key=\"image\",\n",
        "                    image_threshold=0,\n",
        "                ),\n",
        "\n",
        "                # user can also add other random transforms\n",
        "                #                 RandAffined(\n",
        "                #                     keys=['image', 'label'],\n",
        "                #                     mode=('bilinear', 'nearest'),\n",
        "                #                     prob=1.0,\n",
        "                #                     spatial_size=(96, 96, 96),\n",
        "                #                     rotate_range=(0, 0, np.pi/15),\n",
        "                #                     scale_range=(0.1, 0.1, 0.1)),\n",
        "            ]\n",
        "        )\n",
        "        val_transforms = Compose(\n",
        "            [\n",
        "                LoadImaged(keys=[\"image\", \"label\"]),\n",
        "                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "                Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "                Spacingd(\n",
        "                    keys=[\"image\", \"label\"],\n",
        "                    pixdim=(1.5, 1.5, 2.0),\n",
        "                    mode=(\"bilinear\", \"nearest\"),\n",
        "                ),\n",
        "                ScaleIntensityRanged(\n",
        "                    keys=[\"image\"],\n",
        "                    a_min=-57,\n",
        "                    a_max=164,\n",
        "                    b_min=0.0,\n",
        "                    b_max=1.0,\n",
        "                    clip=True,\n",
        "                ),\n",
        "                CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "            ]\n",
        "        )\n",
        "                    \n",
        "\n",
        "        # we use cached datasets - these are 10x faster than regular datasets\n",
        "        self.train_ds = CacheDataset(\n",
        "            data=train_files,\n",
        "            transform=train_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "        self.val_ds = CacheDataset(\n",
        "            data=val_files,\n",
        "            transform=val_transforms,\n",
        "            cache_rate=1.0,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "\n",
        "    #         self.train_ds = monai.data.Dataset(\n",
        "    #             data=train_files, transform=train_transforms)\n",
        "    #         self.val_ds = monai.data.Dataset(\n",
        "    #             data=val_files, transform=val_transforms)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            self.train_ds,\n",
        "            batch_size=2,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            collate_fn=list_data_collate,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
        "        return val_loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        output = self.forward(images)\n",
        "        loss = self.loss_function(output, labels)\n",
        "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch[\"image\"], batch[\"label\"]\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(images, roi_size, sw_batch_size, self)\n",
        "        loss = self.loss_function(outputs, labels)\n",
        "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
        "        self.dice_metric(y_pred=outputs, y=labels)\n",
        "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
        "        self.validation_step_outputs.append(d)\n",
        "        return d\n",
        "    \n",
        "    # def evaluate_single_model(self, model, file_paths):\n",
        "    #     # Load and preprocess NIfTI files\n",
        "    #     input_data = [self.load_and_preprocess_nifti(file_path) for file_path in file_paths]\n",
        "\n",
        "    #     # Perform inference using the model\n",
        "    #     model_output = [self.perform_inference(model, data) for data in input_data]\n",
        "\n",
        "    #     # return many preprocessing steps here, such as normalization\n",
        "    #     # Remember to ensure the data shape matches the input shape expected by your models\n",
        "    #     return data\n",
        "\n",
        "\n",
        "    \n",
        "    def perform_inference(self, model, data):\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "            model_output = model(data.unsqueeze(0))\n",
        "        return model_output\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss, num_items = 0, 0\n",
        "        for output in self.validation_step_outputs:\n",
        "            val_loss += output[\"val_loss\"].sum().item()\n",
        "            num_items += output[\"val_number\"]\n",
        "        mean_val_dice = self.dice_metric.aggregate().item()\n",
        "        self.dice_metric.reset()\n",
        "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
        "        tensorboard_logs = {\n",
        "            \"val_dice\": mean_val_dice,\n",
        "            \"val_loss\": mean_val_loss,\n",
        "        }\n",
        "        if mean_val_dice > self.best_val_dice:\n",
        "            self.best_val_dice = mean_val_dice\n",
        "            self.best_val_epoch = self.current_epoch\n",
        "        print(\n",
        "            f\"current epoch: {self.current_epoch} \"\n",
        "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
        "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
        "            f\"at epoch: {self.best_val_epoch}\"\n",
        "        )\n",
        "        self.validation_step_outputs.clear()  # free memory\n",
        "        self.log('val_dice', mean_val_dice, on_step=False, on_epoch=True, prog_bar=True, logger=True) # log\n",
        "\n",
        "        return {\"log\": tensorboard_logs}\n",
        "    \n",
        "    def infer_on_single_image(self, data):\n",
        "        self._model.eval()\n",
        "        with torch.no_grad():\n",
        "            roi_size = (64, 64, 64)\n",
        "            sw_batch_size = 4\n",
        "            outputs = sliding_window_inference(data, roi_size, sw_batch_size, self._model)\n",
        "            post_processed_outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        return post_processed_outputs\n",
        "    \n",
        "\n",
        "    def infer(self, file_path, label_path = None):\n",
        "        self._model.eval()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self._model.to(device)\n",
        "\n",
        "        data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "        # Apply transformations\n",
        "        data_dict = self.common_transforms(data_dict)\n",
        "        data = data_dict[\"image\"]\n",
        "        label = data_dict[\"label\"]\n",
        "\n",
        "        # Convert data and label to tensors and add batch dimension\n",
        "\n",
        "        data = torch.tensor(data).unsqueeze(0).to(device)\n",
        "        label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # Run inference\n",
        "        predictions = self.infer_on_single_image(data)\n",
        "\n",
        "        # Process and save the predictions as needed\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            # Save or process each prediction here\n",
        "\n",
        "            prediction_np = prediction.cpu().numpy()\n",
        "            # For example, save as NIfTI file\n",
        "            # pred_img = nib.Nifti1Image(prediction_np, nib.load(file_path).affine)\n",
        "\n",
        "        # Convert predictions and labels to binary format if necessary\n",
        "        post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "        # Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "        prediction_tensor = post_pred(prediction.to(device))\n",
        "        label_tensor = post_label(label.to(device))\n",
        "\n",
        "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "        dice_score = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "\n",
        "        print(f\"Dice Score: {dice_score:.4f}\")\n",
        "        print(\"Inference complete.\")\n",
        "\n",
        "        return prediction_tensor.cpu(), label_tensor.cpu()\n",
        "    \n",
        "    def dice_score(self, prediction_tensor, label_tensor):\n",
        "        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "        # post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "        # prediction_tensor = post_pred(prediction.to(device))\n",
        "        # label_tensor = post_label(label.to(device))\n",
        "\n",
        "        # Compute Dice score\n",
        "        dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "        dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "        dice_score = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "\n",
        "        print(dice_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_11608/1320700615.py:275: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(device)\n",
            "/tmp/ipykernel_11608/1320700615.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Score: 0.5216\n",
            "Inference complete.\n",
            "Dice Score: 0.6679\n",
            "Inference complete.\n",
            "Dice Score: 0.9771\n",
            "Inference complete.\n",
            "0.5122274160385132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_11608/3453226296.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  model.dice_score(torch.tensor(ensemble_maximum).unsqueeze(0).to(device), torch.tensor(label_whole).unsqueeze(0).to(device))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_64_0.6679 at epoch: 498.ckpt')\n",
        "file_path = 'nonoverlapping_quadrants/7/quadrant_1_7_CT_HR.nii.gz'\n",
        "label_path = 'nonoverlapping_labels/7/quadrant_1_7_CT_HR_label_airways.nii.gz'\n",
        "\n",
        "pred_1Q, label_1Q = model.infer(file_path, label_path)\n",
        "\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_2Q_spatial_size_64_0.8143 at epoch 499.ckpt')\n",
        "file_path = 'nonoverlapping_quadrants/7/quadrant_2_7_CT_HR.nii.gz'\n",
        "label_path = 'nonoverlapping_labels/7/quadrant_2_7_CT_HR_label_airways.nii.gz'\n",
        "\n",
        "pred_2Q, label_2Q = model.infer(file_path, label_path)\n",
        "\n",
        "\n",
        "moel = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "file_path = 'AeroPath/7/7_CT_HR_label_lungs.nii.gz'\n",
        "label_path = 'AeroPath/7/7_CT_HR_label_lungs.nii.gz'\n",
        "\n",
        "pred_whole, label_whole = model.infer(file_path, label_path)\n",
        "\n",
        "\n",
        "\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Extract the original shape of the predictions\n",
        "    original_shape = predictions.shape\n",
        "    # Compute the scaling factors for interpolation along each axis\n",
        "    scale_factors = [t / o for t, o in zip(target_shape, original_shape)]\n",
        "    # Interpolate the predictions using zoom\n",
        "    interpolated_predictions = zoom(predictions, zoom=scale_factors, mode='nearest')\n",
        "    return interpolated_predictions\n",
        "\n",
        "pred_1Q_copy = pred_1Q\n",
        "pred_2Q_copy = pred_2Q\n",
        "\n",
        "pred_1Q = interpolate_predictions(pred_1Q, (*list(pred_whole.shape[:3]), pred_1Q.shape[3]))\n",
        "pred_2Q = interpolate_predictions(pred_2Q, (*list(pred_whole.shape[:3]), pred_2Q.shape[3]))\n",
        "\n",
        "\n",
        "\n",
        "pred_Q1_Q2 = np.concatenate((pred_1Q, pred_2Q), axis=3)\n",
        "\n",
        "ensemble_maximum = np.maximum(pred_Q1_Q2, pred_whole)\n",
        "\n",
        "\n",
        "\n",
        "# Now you have the ensemble predictions with the shape of the WHOLE model\n",
        "\n",
        "# model.dice_score(torch(ensemble_maximum), torch(label_whole))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model.dice_score(torch.tensor(ensemble_maximum).unsqueeze(0).to(device), torch.tensor(label_whole).unsqueeze(0).to(device))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 257, 230, 92])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_1Q_copy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 257, 257, 92)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_1Q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 257, 257, 182])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_whole.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 257, 257, 182])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_whole.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_1Q = interpolate_predictions(pred_1Q, (*list(pred_whole.shape[:3]), pred_1Q.shape[3]))\n",
        "pred_2Q = interpolate_predictions(pred_2Q, (*list(pred_whole.shape[:3]), pred_2Q.shape[3]))\n",
        "\n",
        "pred_Q1_Q2 = np.concatenate((pred_1Q, pred_2Q), axis=3)\n",
        "\n",
        "ensemble_maximum = np.maximum(pred_Q1_Q2, pred_whole)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "metatensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "          ...,\n",
              "\n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "          ...,\n",
              "\n",
              "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           ...,\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "           [0., 0., 0.,  ..., 0., 0., 0.]]]]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_whole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         ...,\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
              "\n",
              "        [[ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         ...,\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
              "\n",
              "        [[ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         ...,\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         ...,\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
              "\n",
              "        [[ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         ...,\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00]],\n",
              "\n",
              "        [[ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         ...,\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00],\n",
              "         [ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00, ...,\n",
              "           1.0000000e+00,  1.0000000e+00,  1.0000000e+00]]],\n",
              "\n",
              "\n",
              "       [[[-2.4491452e-16, -2.9348678e-16, -2.4491452e-16, ...,\n",
              "          -2.4144507e-16, -2.9348678e-16, -2.4491452e-16],\n",
              "         [-1.5491757e-16, -2.2777595e-16, -1.5491757e-16, ...,\n",
              "          -3.9904989e-17, -1.5618862e-17, -3.2966095e-17],\n",
              "         [-1.8593689e-16, -1.7205910e-16, -1.8593689e-16, ...,\n",
              "          -1.4126090e-16, -1.4473035e-16, -1.4126090e-16],\n",
              "         ...,\n",
              "         [-1.3666388e-16, -1.5054167e-16, -1.3666388e-16, ...,\n",
              "          -1.4291059e-16, -1.6025783e-16, -1.4291059e-16],\n",
              "         [-9.9800565e-17, -1.1367836e-16, -9.9800565e-17, ...,\n",
              "          -9.9884931e-17, -1.1723217e-16, -9.9884931e-17],\n",
              "         [-2.6573120e-16, -2.5185341e-16, -2.6573120e-16, ...,\n",
              "          -2.6573120e-16, -2.5185341e-16, -2.6573120e-16]],\n",
              "\n",
              "        [[-2.4491452e-16, -2.9348678e-16, -2.4491452e-16, ...,\n",
              "          -2.5532286e-16, -2.6920065e-16, -2.4491452e-16],\n",
              "         [-1.5491757e-16, -2.2777595e-16, -1.5491757e-16, ...,\n",
              "          -6.4191121e-17, -1.7410742e-18, -3.2966095e-17],\n",
              "         [-1.8593689e-16, -1.7205910e-16, -1.8593689e-16, ...,\n",
              "          -1.4819980e-16, -1.3432201e-16, -1.4126090e-16],\n",
              "         ...,\n",
              "         [-1.3666388e-16, -1.5054167e-16, -1.3666388e-16, ...,\n",
              "          -1.4291059e-16, -1.6025783e-16, -1.4291059e-16],\n",
              "         [-9.9800565e-17, -1.1367836e-16, -9.9800565e-17, ...,\n",
              "          -9.9884931e-17, -1.1723217e-16, -9.9884931e-17],\n",
              "         [-2.6573120e-16, -2.5185341e-16, -2.6573120e-16, ...,\n",
              "          -2.6573120e-16, -2.5185341e-16, -2.6573120e-16]],\n",
              "\n",
              "        [[-2.4491452e-16, -2.9348678e-16, -2.4491452e-16, ...,\n",
              "          -2.4144507e-16, -2.9348678e-16, -2.4491452e-16],\n",
              "         [-1.5491757e-16, -2.2777595e-16, -1.5491757e-16, ...,\n",
              "          -3.9904989e-17, -7.1130015e-17, -3.2966095e-17],\n",
              "         [-1.8593689e-16, -1.7205910e-16, -1.8593689e-16, ...,\n",
              "          -1.4126090e-16, -1.7248592e-16, -1.4126090e-16],\n",
              "         ...,\n",
              "         [-1.3666388e-16, -1.5054167e-16, -1.3666388e-16, ...,\n",
              "          -1.4291059e-16, -1.6025783e-16, -1.4291059e-16],\n",
              "         [-9.9800565e-17, -1.1367836e-16, -9.9800565e-17, ...,\n",
              "          -9.9884931e-17, -1.1723217e-16, -9.9884931e-17],\n",
              "         [-2.6573120e-16, -2.5185341e-16, -2.6573120e-16, ...,\n",
              "          -2.6573120e-16, -2.5185341e-16, -2.6573120e-16]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-2.4491452e-16, -2.9348678e-16, -2.4491452e-16, ...,\n",
              "          -2.4491452e-16, -2.9348678e-16, -2.4491452e-16],\n",
              "         [-1.5491757e-16, -2.2777595e-16, -1.5491757e-16, ...,\n",
              "          -3.2966095e-17, -1.5618862e-17, -3.2966095e-17],\n",
              "         [-1.8593689e-16, -1.7205910e-16, -1.8593689e-16, ...,\n",
              "          -1.4126090e-16, -1.4819980e-16, -1.4126090e-16],\n",
              "         ...,\n",
              "         [-1.3666388e-16, -1.5054167e-16, -1.3666388e-16, ...,\n",
              "          -1.4291059e-16, -1.6025783e-16, -1.4291059e-16],\n",
              "         [-9.9800565e-17, -1.1367836e-16, -9.9800565e-17, ...,\n",
              "          -9.9884931e-17, -1.1723217e-16, -9.9884931e-17],\n",
              "         [-2.6573120e-16, -2.5185341e-16, -2.6573120e-16, ...,\n",
              "          -2.6573120e-16, -2.5185341e-16, -2.6573120e-16]],\n",
              "\n",
              "        [[-2.4491452e-16, -2.9348678e-16, -2.4491452e-16, ...,\n",
              "          -2.4491452e-16, -2.9348678e-16, -2.4491452e-16],\n",
              "         [-1.5491757e-16, -2.2777595e-16, -1.5491757e-16, ...,\n",
              "          -3.2966095e-17, -1.5618862e-17, -3.2966095e-17],\n",
              "         [-1.8593689e-16, -1.7205910e-16, -1.8593689e-16, ...,\n",
              "          -1.4126090e-16, -1.4819980e-16, -1.4126090e-16],\n",
              "         ...,\n",
              "         [-1.3666388e-16, -1.5054167e-16, -1.3666388e-16, ...,\n",
              "          -1.4291059e-16, -1.6025783e-16, -1.4291059e-16],\n",
              "         [-9.9800565e-17, -1.1367836e-16, -9.9800565e-17, ...,\n",
              "          -9.9884931e-17, -1.1723217e-16, -9.9884931e-17],\n",
              "         [-2.6573120e-16, -2.5185341e-16, -2.6573120e-16, ...,\n",
              "          -2.6573120e-16, -2.5185341e-16, -2.6573120e-16]],\n",
              "\n",
              "        [[-2.4491452e-16, -2.9348678e-16, -2.4491452e-16, ...,\n",
              "          -2.4491452e-16, -2.9348678e-16, -2.4491452e-16],\n",
              "         [-1.5491757e-16, -2.2777595e-16, -1.5491757e-16, ...,\n",
              "          -3.2966095e-17, -1.5618862e-17, -3.2966095e-17],\n",
              "         [-1.8593689e-16, -1.7205910e-16, -1.8593689e-16, ...,\n",
              "          -1.4126090e-16, -1.4819980e-16, -1.4126090e-16],\n",
              "         ...,\n",
              "         [-1.3666388e-16, -1.5054167e-16, -1.3666388e-16, ...,\n",
              "          -1.4291059e-16, -1.6025783e-16, -1.4291059e-16],\n",
              "         [-9.9800565e-17, -1.1367836e-16, -9.9800565e-17, ...,\n",
              "          -9.9884931e-17, -1.1723217e-16, -9.9884931e-17],\n",
              "         [-2.6573120e-16, -2.5185341e-16, -2.6573120e-16, ...,\n",
              "          -2.6573120e-16, -2.5185341e-16, -2.6573120e-16]]]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_dice',\n",
        "    dirpath=os.path.join(root_dir, 'checkpoints'),  # Directory to save checkpoints\n",
        "    filename='best-checkpoint_whole_64',  # Filename prefix for saving checkpoints\n",
        "    save_top_k=1,  # Save only the best checkpoint\n",
        "    mode='max',  # `min` for minimizing the metric, `max` for maximizing\n",
        "    verbose=True,  # Log a message when saving the best checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create overlapping quadrants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape:  (512, 512, 767)\n",
            "Saved: nonoverlapping_quadrants/1/quadrant_1_1_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 383)\n",
            "Saved: nonoverlapping_quadrants/1/quadrant_2_1_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 384)\n",
            "input shape:  (512, 512, 767)\n",
            "Saved: nonoverlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 383)\n",
            "Saved: nonoverlapping_labels/1/quadrant_2_1_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 384)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 829)\n",
            "Saved: nonoverlapping_quadrants/2/quadrant_1_2_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 414)\n",
            "Saved: nonoverlapping_quadrants/2/quadrant_2_2_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 415)\n",
            "input shape:  (512, 512, 829)\n",
            "Saved: nonoverlapping_labels/2/quadrant_1_2_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 414)\n",
            "Saved: nonoverlapping_labels/2/quadrant_2_2_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 415)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 714)\n",
            "Saved: nonoverlapping_quadrants/3/quadrant_1_3_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 357)\n",
            "Saved: nonoverlapping_quadrants/3/quadrant_2_3_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 357)\n",
            "input shape:  (512, 512, 714)\n",
            "Saved: nonoverlapping_labels/3/quadrant_1_3_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 357)\n",
            "Saved: nonoverlapping_labels/3/quadrant_2_3_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 357)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 487, 598)\n",
            "Saved: nonoverlapping_quadrants/4/quadrant_1_4_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 487, 299)\n",
            "Saved: nonoverlapping_quadrants/4/quadrant_2_4_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 487, 299)\n",
            "input shape:  (487, 487, 598)\n",
            "Saved: nonoverlapping_labels/4/quadrant_1_4_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 487, 299)\n",
            "Saved: nonoverlapping_labels/4/quadrant_2_4_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 487, 299)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 619)\n",
            "Saved: nonoverlapping_quadrants/5/quadrant_1_5_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 309)\n",
            "Saved: nonoverlapping_quadrants/5/quadrant_2_5_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "input shape:  (512, 512, 619)\n",
            "Saved: nonoverlapping_labels/5/quadrant_1_5_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 309)\n",
            "Saved: nonoverlapping_labels/5/quadrant_2_5_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 441, 575)\n",
            "Saved: nonoverlapping_quadrants/6/quadrant_1_6_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 441, 287)\n",
            "Saved: nonoverlapping_quadrants/6/quadrant_2_6_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 441, 288)\n",
            "input shape:  (487, 441, 575)\n",
            "Saved: nonoverlapping_labels/6/quadrant_1_6_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 441, 287)\n",
            "Saved: nonoverlapping_labels/6/quadrant_2_6_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 441, 288)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_quadrants/7/quadrant_1_7_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_quadrants/7/quadrant_2_7_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_labels/7/quadrant_1_7_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_labels/7/quadrant_2_7_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 533)\n",
            "Saved: nonoverlapping_quadrants/8/quadrant_1_8_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 266)\n",
            "Saved: nonoverlapping_quadrants/8/quadrant_2_8_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 267)\n",
            "input shape:  (512, 512, 533)\n",
            "Saved: nonoverlapping_labels/8/quadrant_1_8_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 266)\n",
            "Saved: nonoverlapping_labels/8/quadrant_2_8_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 267)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 727)\n",
            "Saved: nonoverlapping_quadrants/9/quadrant_1_9_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 363)\n",
            "Saved: nonoverlapping_quadrants/9/quadrant_2_9_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 364)\n",
            "input shape:  (512, 512, 727)\n",
            "Saved: nonoverlapping_labels/9/quadrant_1_9_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 363)\n",
            "Saved: nonoverlapping_labels/9/quadrant_2_9_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 364)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 241)\n",
            "Saved: nonoverlapping_quadrants/10/quadrant_1_10_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 120)\n",
            "Saved: nonoverlapping_quadrants/10/quadrant_2_10_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 121)\n",
            "input shape:  (512, 512, 241)\n",
            "Saved: nonoverlapping_labels/10/quadrant_1_10_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 120)\n",
            "Saved: nonoverlapping_labels/10/quadrant_2_10_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 121)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 553)\n",
            "Saved: nonoverlapping_quadrants/11/quadrant_1_11_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 276)\n",
            "Saved: nonoverlapping_quadrants/11/quadrant_2_11_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 277)\n",
            "input shape:  (512, 512, 553)\n",
            "Saved: nonoverlapping_labels/11/quadrant_1_11_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 276)\n",
            "Saved: nonoverlapping_labels/11/quadrant_2_11_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 277)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 602)\n",
            "Saved: nonoverlapping_quadrants/12/quadrant_1_12_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 301)\n",
            "Saved: nonoverlapping_quadrants/12/quadrant_2_12_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 301)\n",
            "input shape:  (512, 512, 602)\n",
            "Saved: nonoverlapping_labels/12/quadrant_1_12_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 301)\n",
            "Saved: nonoverlapping_labels/12/quadrant_2_12_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 301)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 620)\n",
            "Saved: nonoverlapping_quadrants/13/quadrant_1_13_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 310)\n",
            "Saved: nonoverlapping_quadrants/13/quadrant_2_13_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "input shape:  (512, 512, 620)\n",
            "Saved: nonoverlapping_labels/13/quadrant_1_13_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 310)\n",
            "Saved: nonoverlapping_labels/13/quadrant_2_13_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 310)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 584)\n",
            "Saved: nonoverlapping_quadrants/14/quadrant_1_14_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 292)\n",
            "Saved: nonoverlapping_quadrants/14/quadrant_2_14_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 292)\n",
            "input shape:  (512, 512, 584)\n",
            "Saved: nonoverlapping_labels/14/quadrant_1_14_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 292)\n",
            "Saved: nonoverlapping_labels/14/quadrant_2_14_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 292)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 556)\n",
            "Saved: nonoverlapping_quadrants/15/quadrant_1_15_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 278)\n",
            "Saved: nonoverlapping_quadrants/15/quadrant_2_15_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 278)\n",
            "input shape:  (512, 512, 556)\n",
            "Saved: nonoverlapping_labels/15/quadrant_1_15_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 278)\n",
            "Saved: nonoverlapping_labels/15/quadrant_2_15_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 278)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 843)\n",
            "Saved: nonoverlapping_quadrants/16/quadrant_1_16_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 421)\n",
            "Saved: nonoverlapping_quadrants/16/quadrant_2_16_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 422)\n",
            "input shape:  (512, 512, 843)\n",
            "Saved: nonoverlapping_labels/16/quadrant_1_16_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 421)\n",
            "Saved: nonoverlapping_labels/16/quadrant_2_16_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 422)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (487, 407, 602)\n",
            "Saved: nonoverlapping_quadrants/17/quadrant_1_17_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (487, 407, 301)\n",
            "Saved: nonoverlapping_quadrants/17/quadrant_2_17_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (487, 407, 301)\n",
            "input shape:  (487, 407, 602)\n",
            "Saved: nonoverlapping_labels/17/quadrant_1_17_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (487, 407, 301)\n",
            "Saved: nonoverlapping_labels/17/quadrant_2_17_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (487, 407, 301)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 706)\n",
            "Saved: nonoverlapping_quadrants/18/quadrant_1_18_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 353)\n",
            "Saved: nonoverlapping_quadrants/18/quadrant_2_18_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 353)\n",
            "input shape:  (512, 512, 706)\n",
            "Saved: nonoverlapping_labels/18/quadrant_1_18_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 353)\n",
            "Saved: nonoverlapping_labels/18/quadrant_2_18_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 353)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 666)\n",
            "Saved: nonoverlapping_quadrants/19/quadrant_1_19_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 333)\n",
            "Saved: nonoverlapping_quadrants/19/quadrant_2_19_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 333)\n",
            "input shape:  (512, 512, 666)\n",
            "Saved: nonoverlapping_labels/19/quadrant_1_19_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 333)\n",
            "Saved: nonoverlapping_labels/19/quadrant_2_19_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 333)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 286)\n",
            "Saved: nonoverlapping_quadrants/20/quadrant_1_20_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 143)\n",
            "Saved: nonoverlapping_quadrants/20/quadrant_2_20_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 143)\n",
            "input shape:  (512, 512, 286)\n",
            "Saved: nonoverlapping_labels/20/quadrant_1_20_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 143)\n",
            "Saved: nonoverlapping_labels/20/quadrant_2_20_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 143)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 717)\n",
            "Saved: nonoverlapping_quadrants/21/quadrant_1_21_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 358)\n",
            "Saved: nonoverlapping_quadrants/21/quadrant_2_21_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 359)\n",
            "input shape:  (512, 512, 717)\n",
            "Saved: nonoverlapping_labels/21/quadrant_1_21_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 358)\n",
            "Saved: nonoverlapping_labels/21/quadrant_2_21_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 359)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 329)\n",
            "Saved: nonoverlapping_quadrants/22/quadrant_1_22_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_quadrants/22/quadrant_2_22_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 165)\n",
            "input shape:  (512, 512, 329)\n",
            "Saved: nonoverlapping_labels/22/quadrant_1_22_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_labels/22/quadrant_2_22_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 165)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 694)\n",
            "Saved: nonoverlapping_quadrants/23/quadrant_1_23_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 347)\n",
            "Saved: nonoverlapping_quadrants/23/quadrant_2_23_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 347)\n",
            "input shape:  (512, 512, 694)\n",
            "Saved: nonoverlapping_labels/23/quadrant_1_23_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 347)\n",
            "Saved: nonoverlapping_labels/23/quadrant_2_23_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 347)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 607)\n",
            "Saved: nonoverlapping_quadrants/24/quadrant_1_24_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 303)\n",
            "Saved: nonoverlapping_quadrants/24/quadrant_2_24_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 304)\n",
            "input shape:  (512, 512, 607)\n",
            "Saved: nonoverlapping_labels/24/quadrant_1_24_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 303)\n",
            "Saved: nonoverlapping_labels/24/quadrant_2_24_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 304)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 328)\n",
            "Saved: nonoverlapping_quadrants/25/quadrant_1_25_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_quadrants/25/quadrant_2_25_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 164)\n",
            "input shape:  (512, 512, 328)\n",
            "Saved: nonoverlapping_labels/25/quadrant_1_25_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 164)\n",
            "Saved: nonoverlapping_labels/25/quadrant_2_25_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 164)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 265)\n",
            "Saved: nonoverlapping_quadrants/26/quadrant_1_26_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 132)\n",
            "Saved: nonoverlapping_quadrants/26/quadrant_2_26_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 133)\n",
            "input shape:  (512, 512, 265)\n",
            "Saved: nonoverlapping_labels/26/quadrant_1_26_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 132)\n",
            "Saved: nonoverlapping_labels/26/quadrant_2_26_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 133)\n",
            "\n",
            "###########\n",
            "\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_quadrants/27/quadrant_1_27_CT_HR.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_quadrants/27/quadrant_2_27_CT_HR.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "input shape:  (512, 512, 723)\n",
            "Saved: nonoverlapping_labels/27/quadrant_1_27_CT_HR_label_airways.nii.gz\n",
            "Quadrant 0 shape: (512, 512, 361)\n",
            "Saved: nonoverlapping_labels/27/quadrant_2_27_CT_HR_label_airways.nii.gz\n",
            "Quadrant 1 shape: (512, 512, 362)\n",
            "\n",
            "###########\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "def create_overlapping_quadrants(image_shape, overlap_percentage=0.0):\n",
        "    x, y, z = image_shape\n",
        "    overlap_z = int(z * overlap_percentage)\n",
        "    if z < 3 * overlap_z:\n",
        "        return [(0, z)]\n",
        "    mid_z = z // 2\n",
        "    return [\n",
        "        (0, mid_z + overlap_z),\n",
        "        (mid_z - overlap_z, z)\n",
        "    ]\n",
        "\n",
        "def load_nifti_file(file_path):\n",
        "    return nib.load(file_path)\n",
        "\n",
        "def save_nifti_file(data, affine, file_path):\n",
        "    img = nib.Nifti1Image(data, affine)\n",
        "    nib.save(img, file_path)\n",
        "\n",
        "def process_file(file_path, output_dir):\n",
        "    img = load_nifti_file(file_path)\n",
        "    data = img.get_fdata()\n",
        "    print(\"input shape: \", data.shape)\n",
        "    affine = img.affine\n",
        "    quadrants = create_overlapping_quadrants(data.shape)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for i, (start, end) in enumerate(quadrants):\n",
        "        quadrant_data = data[:, :, start:end]\n",
        "        quadrant_file_path = os.path.join(output_dir, f\"quadrant_{i+1}_{os.path.basename(file_path)}\")\n",
        "        save_nifti_file(quadrant_data, affine, quadrant_file_path)\n",
        "        print(f\"Saved: {quadrant_file_path}\")\n",
        "        print(f'Quadrant {i} shape: {quadrant_data.shape}')\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "pattern = os.path.join(data_dir, '**/*_CT_HR_label_airways.nii.gz')\n",
        "train_labels = natsorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "train_images = natsorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "# for idx, (image_name, label_name) in enumerate(zip(train_images, train_labels)):\n",
        "#     output_dir = f'nonoverlapping_quadrants/{idx}'\n",
        "#     process_file(image_name, output_dir)\n",
        "#     output_dir = f'nonoverlapping_labels/{idx}'\n",
        "#     process_file(label_name, output_dir)\n",
        "\n",
        "\n",
        "\n",
        "for idx, (image_name, label_name) in enumerate(zip(train_images, train_labels)):\n",
        "    output_dir = f'nonoverlapping_quadrants/{idx + 1}'\n",
        "    process_file(image_name, output_dir)\n",
        "    output_dir = f'nonoverlapping_labels/{idx + 1}'\n",
        "    process_file(label_name, output_dir)\n",
        "    \n",
        "    print('')\n",
        "    print('###########')\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/1/1_CT_HR.nii.gz\n",
            "1\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/2/2_CT_HR.nii.gz\n",
            "2\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/3/3_CT_HR.nii.gz\n",
            "3\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/4/4_CT_HR.nii.gz\n",
            "4\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/5/5_CT_HR.nii.gz\n",
            "5\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/6/6_CT_HR.nii.gz\n",
            "6\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/7/7_CT_HR.nii.gz\n",
            "7\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/8/8_CT_HR.nii.gz\n",
            "8\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/9/9_CT_HR.nii.gz\n",
            "9\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/10/10_CT_HR.nii.gz\n",
            "10\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/11/11_CT_HR.nii.gz\n",
            "11\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/12/12_CT_HR.nii.gz\n",
            "12\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/13/13_CT_HR.nii.gz\n",
            "13\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/14/14_CT_HR.nii.gz\n",
            "14\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/15/15_CT_HR.nii.gz\n",
            "15\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/16/16_CT_HR.nii.gz\n",
            "16\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/17/17_CT_HR.nii.gz\n",
            "17\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/18/18_CT_HR.nii.gz\n",
            "18\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/19/19_CT_HR.nii.gz\n",
            "19\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/20/20_CT_HR.nii.gz\n",
            "20\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/21/21_CT_HR.nii.gz\n",
            "21\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/22/22_CT_HR.nii.gz\n",
            "22\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/23/23_CT_HR.nii.gz\n",
            "23\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/24/24_CT_HR.nii.gz\n",
            "24\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/25/25_CT_HR.nii.gz\n",
            "25\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/26/26_CT_HR.nii.gz\n",
            "26\n",
            "/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/27/27_CT_HR.nii.gz\n"
          ]
        }
      ],
      "source": [
        "from natsort import natsorted\n",
        "import glob\n",
        "\n",
        "# Assuming 'pattern' is defined\n",
        "pattern = \"/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/*/*_CT_HR.nii.gz\"\n",
        "\n",
        "# Get the list of files\n",
        "train_images = glob.glob(pattern, recursive=True)\n",
        "\n",
        "# Sort the files naturally\n",
        "sorted_train_images = natsorted(train_images)\n",
        "\n",
        "# Print the sorted list\n",
        "for idx, image in enumerate(sorted_train_images):\n",
        "    print(idx)\n",
        "    print(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/1/1_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/10/10_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/11/11_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/12/12_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/13/13_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/14/14_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/15/15_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/16/16_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/17/17_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/18/18_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/19/19_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/2/2_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/20/20_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/21/21_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/22/22_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/23/23_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/24/24_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/25/25_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/26/26_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/27/27_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/3/3_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/4/4_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/5/5_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/6/6_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/7/7_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/8/8_CT_HR.nii.gz',\n",
              " '/home/gasyna/RiSA_S3/3D_segmentation/AeroPath/9/9_CT_HR.nii.gz']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = os.path.join(data_dir, '**/*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))\n",
        "train_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pattern = os.path.join('overlapping_labels', '**/quadrant_1_*.nii.gz')\n",
        "train_labels = sorted(glob.glob(pattern, recursive=True))\n",
        "\n",
        "pattern = os.path.join('overlapping_quadrants', '**/quadrant_1_*_CT_HR.nii.gz')\n",
        "train_images = sorted(glob.glob(pattern, recursive=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img shape: (512, 512, 362), label shape: (512, 512, 362)\n"
          ]
        }
      ],
      "source": [
        "img = 'nonoverlapping_quadrants/24/quadrant_2_7_CT_HR.nii.gz'\n",
        "label = 'nonoverlapping_labels/24/quadrant_2_7_CT_HR_label_airways.nii.gz'\n",
        "img = nib.load(img).get_fdata()\n",
        "label = nib.load(label).get_fdata()\n",
        "\n",
        "print(f'img shape: {img.shape}, label shape: {label.shape}')\n",
        "\n",
        "\n",
        "# whole\n",
        "# img shape: (512, 512, 723), label shape: (512, 512, 723)\n",
        "\n",
        "# 1q\n",
        "# img shape: (512, 512, 361), label shape: (512, 512, 361)\n",
        "\n",
        "# 2q\n",
        "# img shape: (512, 512, 362), label shape: (512, 512, 362)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for img, label in zip (train_images, train_labels):\n",
        "    img = nib.load(img).get_fdata()\n",
        "    label = nib.load(label).get_fdata()\n",
        "\n",
        "    print(f'img shape: {img.shape}, label shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# quadrant1 = nib.load('quadrant1.nii.gz').get_fdata()\n",
        "# quadrant2 = nib.load('quadrant2.nii.gz').get_fdata()\n",
        "# quadrant3 = nib.load('quadrant3.nii.gz').get_fdata()\n",
        "# quadrant4 = nib.load('quadrant4.nii.gz').get_fdata()\n",
        "\n",
        "\n",
        "def plot_slice(data, title):\n",
        "    plt.figure()\n",
        "    plt.imshow(data[data.shape[0] // 2, :, :], cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# plot_slice(quadrant1, \"Quadrant 1\")\n",
        "# plot_slice(quadrant2, \"Quadrant 2\")\n",
        "# plot_slice(quadrant3, \"Quadrant 3\")\n",
        "# plot_slice(quadrant4, \"Quadrant 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSVCubC2I0vR"
      },
      "source": [
        "## Run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8BpsB6sI0vR",
        "scrolled": false,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# initialise the LightningModule\n",
        "net = Net()\n",
        "\n",
        "# set up loggers and checkpoints\n",
        "log_dir = os.path.join(root_dir, \"logs\")\n",
        "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
        "\n",
        "# initialise Lightning's trainer.\n",
        "trainer = pytorch_lightning.Trainer(\n",
        "    devices=[0],\n",
        "    max_epochs=600,\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    num_sanity_val_steps=1,\n",
        "    log_every_n_steps=16,\n",
        ")\n",
        "\n",
        "# train\n",
        "trainer.fit(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TucnezFI0vR",
        "outputId": "4e0a75d6-eb09-4989-bb2b-a2c6b1388edf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f\"train completed, best_metric: {net.best_val_dice:.4f} \" f\"at epoch {net.best_val_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "\n",
        "\n",
        "# Load the model weights from the checkpoint file\n",
        "checkpoint_path = 'best-checkpoint.ckpt'\n",
        "model = Net.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor(data)  \u001b[38;5;66;03m# Convert data to type Double\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/nets/unet.py:300\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/networks/blocks/convolutions.py:316\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 316\u001b[0m     res: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# create the additive residual from x\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     cx: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)  \u001b[38;5;66;03m# apply x to sequence of operations\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cx \u001b[38;5;241m+\u001b[39m res\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [512, 512, 536]"
          ]
        }
      ],
      "source": [
        "# Instantiate the Net class\n",
        "\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_48_0.6382 at epoch 338.ckpt')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "file_path = '/home/gasyna/RiSA_S3/3D_segmentation/overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "data = torch.DoubleTensor(data)  # Convert data to type Double\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model inference (works)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7c4230bad900>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/io/dictionary.py:162\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_key_postfix):\n\u001b[0;32m--> 162\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loader\u001b[38;5;241m.\u001b[39mimage_only:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/io/array.py:245\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[0;34m(self, filename, reader)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mLoad image file and metadata from the given filename(s).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mIf `reader` is not specified, this class automatically chooses readers based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mensure_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# allow Path objects\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m img, err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, []\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/io/array.py:246\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mLoad image file and metadata from the given filename(s).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mIf `reader` is not specified, this class automatically chooses readers based on the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpanduser \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m ensure_tuple(filename)  \u001b[38;5;66;03m# allow Path objects\u001b[39;00m\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    248\u001b[0m img, err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, []\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:960\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:594\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n",
            "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:578\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not ndarray",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: data}\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43minfer_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Convert data to tensor and add batch dimension\u001b[39;00m\n\u001b[1;32m     51\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7c4230bad900>"
          ]
        }
      ],
      "source": [
        "# Model inference\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    Spacingd, ScaleIntensityRanged, CropForegroundd, Orientationd, EnsureTyped, Compose\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "\n",
        "# Load the trained model\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt')\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for new data\n",
        "infer_transforms = Compose(\n",
        "    [\n",
        "        Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
        "        # Spacingd(keys=\"image\", pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\")),\n",
        "        ScaleIntensityRanged(keys=\"image\", a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
        "        # CropForegroundd(keys=\"image\", source_key=\"image\"),\n",
        "        EnsureTyped(keys=\"image\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data\n",
        "file_path = 'overlapping_quadrants/18/quadrant_1_26_CT_HR.nii.gz'\n",
        "img = nib.load(file_path)\n",
        "data = img.get_fdata()\n",
        "\n",
        "# Manually add the channel dimension\n",
        "data = data[None, ...]\n",
        "data_raw = data\n",
        "# Create a dictionary with the image\n",
        "data_dict = {\"image\": data}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = infer_transforms(data_dict)\n",
        "\n",
        "# Convert data to tensor and add batch dimension\n",
        "data = torch.tensor(data_dict[\"image\"]).unsqueeze(0).to(model.device)\n",
        "\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference\n",
        "predictions = infer_on_single_image(model, data)\n",
        "\n",
        "# Process and save the predictions as needed\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # Save or process each prediction here\n",
        "    # For example, save as NIfTI file\n",
        "    prediction_np = prediction.cpu().numpy()\n",
        "    pred_img = nib.Nifti1Image(prediction_np, img.affine)\n",
        "    # nib.save(pred_img, f\"prediction_{i}.nii.gz\")\n",
        "    nib.save(pred_img, \"prediction.nii.gz\")\n",
        "\n",
        "print(\"input raw shape: \", data_raw.shape)\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/tmp/ipykernel_81960/684908623.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(model.device)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'img' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Save or process each prediction here\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# For example, save as NIfTI file\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     prediction_np \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 59\u001b[0m     pred_img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mNifti1Image(prediction_np, \u001b[43mimg\u001b[49m\u001b[38;5;241m.\u001b[39maffine)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# nib.save(pred_img, f\"prediction_{i}.nii.gz\")\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     nib\u001b[38;5;241m.\u001b[39msave(pred_img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
          ]
        }
      ],
      "source": [
        "# Model inference\n",
        "\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd, ScaleIntensityRanged, CropForegroundd, EnsureTyped, Compose\n",
        ")\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import decollate_batch\n",
        "\n",
        "# Load the trained model\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_96_0.6293 at epoch 340.ckpt')\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "infer_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=\"image\"),\n",
        "        EnsureChannelFirstd(keys=\"image\"),\n",
        "        Orientationd(keys=\"image\", axcodes=\"RAS\"),\n",
        "        Spacingd(keys=\"image\", pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\")),\n",
        "        ScaleIntensityRanged(keys=\"image\", a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=\"image\", source_key=\"image\"),\n",
        "        EnsureTyped(keys=\"image\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data\n",
        "file_path = 'overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz'\n",
        "data_dict = {\"image\": file_path}\n",
        "\n",
        "# Apply transformations\n",
        "data_dict = infer_transforms(data_dict)\n",
        "data = data_dict[\"image\"]\n",
        "\n",
        "data_raw = data\n",
        "# Convert data to tensor and add batch dimension\n",
        "data = torch.tensor(data).unsqueeze(0).to(model.device)\n",
        "\n",
        "# Function to run inference\n",
        "def infer_on_single_image(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "        post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "    return post_processed_outputs\n",
        "\n",
        "# Run inference\n",
        "predictions = infer_on_single_image(model, data)\n",
        "\n",
        "# Process and save the predictions as needed\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # Save or process each prediction here\n",
        "    # For example, save as NIfTI file\n",
        "    prediction_np = prediction.cpu().numpy()\n",
        "    pred_img = nib.Nifti1Image(prediction_np, img.affine)\n",
        "    # nib.save(pred_img, f\"prediction_{i}.nii.gz\")\n",
        "    nib.save(pred_img, \"prediction.nii.gz\")\n",
        "\n",
        "print(\"input raw shape: \", data_raw.shape)\n",
        "print(\"input shape: \", data.shape)\n",
        "print(\"output shape: \", pred_img.shape)\n",
        "print(\"Inference complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dice score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "No such file or no access: 'overlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/loadsave.py:100\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     stat_result \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'overlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# image = nib.load('overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction.nii.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nibabel/loadsave.py:102\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     stat_result \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(filename)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or no access: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stat_result\u001b[38;5;241m.\u001b[39mst_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImageFileError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: 'overlapping_labels/1/quadrant_1_1_CT_HR_label_airways.nii.gz'"
          ]
        }
      ],
      "source": [
        "label = nib.load('overlapping_labels/0/quadrant_1_1_CT_HR_label_airways.nii.gz')\n",
        "# image = nib.load('overlapping_quadrants/0/quadrant_1_1_CT_HR.nii.gz')\n",
        "image = nib.load('prediction.nii.gz')\n",
        "\n",
        "label = label.get_fdata()\n",
        "image = image.get_fdata()\n",
        "\n",
        "image = image[0, :, :]\n",
        "\n",
        "print(f'img shape: {image.shape}, label shape: {label.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 259, 259, 182])\n",
            "(2, 259, 259, 364)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from monai.data.meta_tensor import MetaTensor\n",
        "\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def infer(img): #TODO metoda Infer powinna być dodana do klasy Net()\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    \n",
        "def merge_predictions(pred1: MetaTensor, pred2: MetaTensor) -> MetaTensor:\n",
        "    pred_merged = np.concatenate((pred1, pred2), axis=3)\n",
        "    return pred_merged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "\n",
        "file_paths_1Q = natsorted(glob.glob('nonoverlapping_labels/*/quadrant_1_*_CT_HR_label_airways.nii.gz', recursive=True))\n",
        "label_paths_1Q = natsorted(glob.glob('nonoverlapping_quadrants/*/quadrant_1_*_CT_HR.nii.gz', recursive=True))\n",
        "\n",
        "\n",
        "file_paths_2Q = natsorted(glob.glob('nonoverlapping_labels/*/quadrant_2_*_CT_HR_label_airways.nii.gz', recursive=True))\n",
        "label_paths_2Q = natsorted(glob.glob('nonoverlapping_quadrants/*/quadrant_2_*_CT_HR.nii.gz', recursive=True))\n",
        "\n",
        "file_paths_whole = natsorted(glob.glob('AeroPath/*/*_CT_HR.nii.gz', recursive=True))\n",
        "label_paths_whole = natsorted(glob.glob('AeroPath/*/*_CT_HR_label_airways.nii.gz', recursive=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gasyna/.local/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/tmp/ipykernel_99147/1917503314.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  data = torch.tensor(data).unsqueeze(0).to(device)\n",
            "/tmp/ipykernel_99147/1917503314.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).unsqueeze(0).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input nii gz shape:  (512, 512, 767)\n",
            "input shape:  torch.Size([1, 1, 234, 234, 192])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 829)\n",
            "input shape:  torch.Size([1, 1, 254, 213, 208])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 714)\n",
            "input shape:  torch.Size([1, 1, 235, 214, 179])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (487, 487, 598)\n",
            "input shape:  torch.Size([1, 1, 220, 209, 150])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 619)\n",
            "input shape:  torch.Size([1, 1, 222, 187, 156])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (487, 441, 575)\n",
            "input shape:  torch.Size([1, 1, 216, 168, 144])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 723)\n",
            "input shape:  torch.Size([1, 1, 259, 194, 182])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 533)\n",
            "input shape:  torch.Size([1, 1, 251, 221, 134])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 727)\n",
            "input shape:  torch.Size([1, 1, 257, 230, 182])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 241)\n",
            "input shape:  torch.Size([1, 1, 240, 214, 151])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 553)\n",
            "input shape:  torch.Size([1, 1, 233, 199, 139])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 602)\n",
            "input shape:  torch.Size([1, 1, 231, 214, 151])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 620)\n",
            "input shape:  torch.Size([1, 1, 231, 225, 156])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 584)\n",
            "input shape:  torch.Size([1, 1, 260, 216, 147])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 556)\n",
            "input shape:  torch.Size([1, 1, 282, 238, 140])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 843)\n",
            "input shape:  torch.Size([1, 1, 234, 201, 212])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (487, 407, 602)\n",
            "input shape:  torch.Size([1, 1, 245, 198, 151])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 706)\n",
            "input shape:  torch.Size([1, 1, 257, 240, 177])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 666)\n",
            "input shape:  torch.Size([1, 1, 253, 214, 167])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 286)\n",
            "input shape:  torch.Size([1, 1, 217, 185, 143])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 717)\n",
            "input shape:  torch.Size([1, 1, 261, 230, 180])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 329)\n",
            "input shape:  torch.Size([1, 1, 254, 233, 165])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 694)\n",
            "input shape:  torch.Size([1, 1, 193, 162, 174])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 607)\n",
            "input shape:  torch.Size([1, 1, 237, 226, 152])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 328)\n",
            "input shape:  torch.Size([1, 1, 224, 183, 164])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 265)\n",
            "input shape:  torch.Size([1, 1, 267, 182, 165])\n",
            "<class 'numpy.ndarray'>\n",
            "input nii gz shape:  (512, 512, 723)\n",
            "input shape:  torch.Size([1, 1, 278, 223, 181])\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# Model inference and Dice score calculation\n",
        "# Load the trained model\n",
        "model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_1Q_spatial_size_64_0.6679 at epoch: 498.ckpt')\n",
        "# model = Net.load_from_checkpoint('checkpoints/best-checkpoint_new_method_2Q_spatial_size_64_0.8143 at epoch 499.ckpt')\n",
        "# moel = Net.load_from_checkpoint('checkpoints/best-checkpoint_whole_64_0.8023 at epoch: 427.ckpt')\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the transformations for validation and inference\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load and preprocess the new data and the label\n",
        "\n",
        "# # 1Q\n",
        "# file_path = 'nonoverlapping_quadrants/5/quadrant_1_5_CT_HR.nii.gz'\n",
        "# label_path = 'nonoverlapping_labels/5/quadrant_1_5_CT_HR_label_airways.nii.gz'  # Update with your actual label file path\n",
        "\n",
        "# 2Q\n",
        "# file_path = 'nonoverlapping_quadrants/5/quadrant_2_5_CT_HR.nii.gz'\n",
        "# label_path = 'nonoverlapping_labels/5/quadrant_2_5_CT_HR_label_airways.nii.gz'  # Update with your actual label file path\n",
        "\n",
        "# # WHOLE\n",
        "# file_path = 'AeroPath/5/5_CT_HR_label_lungs.nii.gz'\n",
        "# label_path = 'AeroPath/5/5_CT_HR_label_lungs.nii.gz'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert data and label to tensors and add batch dimension\n",
        "for idx, (file_path, label_path) in enumerate(zip(file_paths_whole, label_paths_whole)):\n",
        "    prediction_filename = f'predictions_whole/whole_{idx+1}_prediction.nii.gz'\n",
        "\n",
        "    data_dict = {\"image\": file_path, \"label\": label_path}\n",
        "\n",
        "\n",
        "    # Apply transformations\n",
        "    data_dict = common_transforms(data_dict)\n",
        "    data = data_dict[\"image\"]\n",
        "    label = data_dict[\"label\"]\n",
        "\n",
        "    data = torch.tensor(data).unsqueeze(0).to(device)\n",
        "    label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "    data_nii = nib.load(file_path).get_fdata()\n",
        "    label_nii = nib.load(label_path).get_fdata()\n",
        "\n",
        "    print(\"input nii gz shape: \", data_nii.shape)\n",
        "    print(\"input shape: \", data.shape)\n",
        "\n",
        "    # Function to run inference\n",
        "    def infer_on_single_image(model, data):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            roi_size = (64, 64, 64)\n",
        "            sw_batch_size = 4\n",
        "            outputs = sliding_window_inference(data, roi_size, sw_batch_size, model)\n",
        "            post_processed_outputs = [model.post_pred(i) for i in decollate_batch(outputs)]\n",
        "        return post_processed_outputs\n",
        "\n",
        "    # Run inference\n",
        "    predictions = infer_on_single_image(model, data)\n",
        "\n",
        "    # Process and save the predictions as needed\n",
        "    for i, prediction in enumerate(predictions):\n",
        "        # Save or process each prediction here\n",
        "        # For example, save as NIfTI file\n",
        "        prediction_np = prediction.cpu().numpy()\n",
        "        print(type(prediction_np))\n",
        "        pred_img = nib.Nifti1Image(prediction_np, nib.load(file_path).affine)\n",
        "        nib.save(pred_img, prediction_filename)\n",
        "\n",
        "\n",
        "while False:\n",
        "    # Convert predictions and labels to binary format if necessary\n",
        "    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    # Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "    prediction_tensor = post_pred(prediction.to(device))\n",
        "    label_tensor = post_label(label.to(device))\n",
        "\n",
        "    print('pred type:', type(prediction_tensor))\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    print(\"input shape: \", data.shape)\n",
        "    print(\"output shape: \", pred_img.shape)\n",
        "    print(f\"Dice Score: {dice_score:.4f}\")\n",
        "    print(\"Inference complete.\")\n",
        "\n",
        "    pred_nii = nib.load(prediction_filename).get_fdata()\n",
        "\n",
        "    print(\"output nii gz shape: \", pred_nii.shape)\n",
        "\n",
        "#NONOVERLAPPING\n",
        "# 1Q\n",
        "# input nii gz shape:  (512, 512, 361)\n",
        "# input shape:  torch.Size([1, 1, 259, 192, 91])\n",
        "# input shape:  torch.Size([1, 1, 259, 192, 91])\n",
        "# output shape:  (2, 259, 192, 91)\n",
        "# Dice Score: 0.5217\n",
        "\n",
        "# 2Q\n",
        "# input nii gz shape:  (512, 512, 362)\n",
        "# input shape:  torch.Size([1, 1, 259, 188, 91])\n",
        "# input shape:  torch.Size([1, 1, 259, 188, 91])\n",
        "# output shape:  (2, 259, 188, 91)\n",
        "# Dice Score: 0.6679\n",
        "\n",
        "# WHOLE\n",
        "# input nii gz shape:  (512, 512, 723)\n",
        "# input shape:  torch.Size([1, 1, 259, 259, 182])\n",
        "# input shape:  torch.Size([1, 1, 259, 259, 182])\n",
        "# output shape:  (2, 259, 259, 182)\n",
        "# Dice Score: 0.9771\n",
        "\n",
        "# OVERLAPPING\n",
        "# 1Q\n",
        "# input shape:  torch.Size([1, 1, 259, 194, 127])\n",
        "# output shape:  (2, 259, 194, 127)\n",
        "# Dice Score: 0.5829\n",
        "\n",
        "# 2Q\n",
        "# input shape:  torch.Size([1, 1, 259, 190, 127])\n",
        "# output shape:  (2, 259, 190, 127)\n",
        "# Dice Score: 0.6390\n",
        "\n",
        "#NONOVERLAPPING WITHOUT SPACINGD\n",
        "# 1Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Define a function to interpolate predictions to match the dimensions of the WHOLE model\n",
        "# def interpolate_predictions(predictions, target_shape):\n",
        "#     # Extract the original shape of the predictions\n",
        "#     original_shape = predictions.shape\n",
        "#     # Compute the scaling factors for interpolation along each axis\n",
        "#     scale_factors = [t / o for t, o in zip(target_shape, original_shape)]\n",
        "#     # Interpolate the predictions using zoom\n",
        "#     interpolated_predictions = zoom(predictions, zoom=scale_factors, mode='nearest')\n",
        "#     return interpolated_predictions\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Ensure predictions tensor has 5 dimensions\n",
        "    predictions = torch.tensor(predictions)\n",
        "    if predictions.dim() == 6:\n",
        "        predictions = predictions.squeeze(0)  # Remove batch dimension if present\n",
        "    \n",
        "    # Interpolate the predictions using bilinear interpolation\n",
        "    interpolated_predictions = F.interpolate(predictions, size=target_shape[2:], mode='nearest')\n",
        "    \n",
        "    return interpolated_predictions\n",
        "\n",
        "common_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-57,\n",
        "            a_max=164,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "for idx, (pred_1Q_path, pred_2Q_path, pred_whole_path, label_path) in enumerate(zip(pred_1Q_paths, pred_2Q_paths, pred_whole_paths, label_paths)):\n",
        "    pred_1Q = nib.load(pred_1Q_path).get_fdata()\n",
        "    pred_2Q = nib.load(pred_2Q_path).get_fdata()\n",
        "    pred_whole = nib.load(pred_whole_path).get_fdata()\n",
        "    label = nib.load(label_path).get_fdata()\n",
        "\n",
        "    print(f'pred_1Q shape: {pred_1Q.shape}, pred_2Q shape: {pred_2Q.shape}, pred_whole shape: {pred_whole.shape}, label shape: {label.shape}')\n",
        "\n",
        "    merged =np.concatenate((pred_1Q, pred_2Q), axis=3)\n",
        "    # print(f'merged shape: {merged.shape}')\n",
        "\n",
        "    interpolated_merged = interpolate_predictions(merged, pred_whole.shape)\n",
        "\n",
        "    # print(f'interpolated_merged shape: {interpolated_merged.shape}')\n",
        "\n",
        "    # Save the interpolated predictions\n",
        "    interpolated_merged = interpolated_merged.cpu().numpy()\n",
        "    merged = nib.Nifti1Image(interpolated_merged, nib.load(pred_whole_path).affine)\n",
        "    nib.save(merged, f'predictions_merged/merged_{idx+1}_prediction.nii.gz')\n",
        "\n",
        "    ensembled = np.maximum(interpolated_merged, pred_whole)\n",
        "    ensembled_nifti = nib.Nifti1Image(ensembled, nib.load(pred_whole_path).affine)\n",
        "\n",
        "    print('difference merged = ', (interpolated_merged != pred_whole).sum()/pred_whole.size, '%')\n",
        "    print('difference ensembled = ', (ensembled != pred_whole).sum()/pred_whole.size, '%')\n",
        "\n",
        "    nib.save(ensembled_nifti, f'predictions_ensembled/ensembled_{idx+1}_prediction.nii.gz')\n",
        "\n",
        "    prediction = torch.tensor(ensembled).unsqueeze(0).to(device)\n",
        "\n",
        "    data_dict = {\"image\": pred_whole_path, \"label\": label_path}\n",
        "    data_dict = common_transforms(data_dict)\n",
        "    data = data_dict[\"image\"]\n",
        "    label = data_dict[\"label\"]\n",
        "\n",
        "    label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "    post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "    prediction_tensor = post_pred(prediction.to(device))\n",
        "    label_tensor = post_label(label.to(device))\n",
        "\n",
        "    # Compute Dice score\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "    dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    dice_metric.reset()\n",
        "\n",
        "    print('DICE: ', dice_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpolate ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_1Q_paths = natsorted(glob.glob('predictions1Q/*', recursive=True))\n",
        "pred_2Q_paths = natsorted(glob.glob('predictions2Q/*', recursive=True))\n",
        "pred_whole_paths = natsorted(glob.glob('predictions_whole/*', recursive=True))\n",
        "label_paths = natsorted(glob.glob('AeroPath/*/*_CT_HR_label_airways.nii.gz', recursive=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['AeroPath/1/1_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/2/2_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/3/3_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/4/4_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/5/5_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/6/6_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/7/7_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/8/8_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/9/9_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/10/10_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/11/11_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/12/12_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/13/13_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/14/14_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/15/15_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/16/16_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/17/17_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/18/18_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/19/19_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/20/20_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/21/21_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/22/22_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/23/23_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/24/24_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/25/25_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/26/26_CT_HR_label_airways.nii.gz',\n",
              " 'AeroPath/27/27_CT_HR_label_airways.nii.gz']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_whole = nib.load(pred_whole_path).get_fdata()\n",
        "label = nib.load(label_path).get_fdata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 234, 234, 192)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_whole.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1.], dtype=torch.float64)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 5.80 GiB total capacity; 3.50 GiB already allocated; 762.06 MiB free; 4.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label)\n\u001b[0;32m----> 2\u001b[0m label_new \u001b[38;5;241m=\u001b[39m post_label(\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 5.80 GiB total capacity; 3.50 GiB already allocated; 762.06 MiB free; 4.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "label = torch.tensor(label)\n",
        "label_new = post_label(label.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 234, 234, 192])\n",
            "torch.Size([2, 512, 512, 767])\n"
          ]
        }
      ],
      "source": [
        "print(prediction_tensor.shape)\n",
        "print(label_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2581"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Define a function to interpolate predictions to match the dimensions of the WHOLE model\n",
        "def interpolate_predictions(predictions, target_shape):\n",
        "    # Extract the original shape of the predictions\n",
        "    original_shape = predictions.shape\n",
        "    # Compute the scaling factors for interpolation along each axis\n",
        "    scale_factors = [t / o for t, o in zip(target_shape, original_shape)]\n",
        "    # Interpolate the predictions using zoom\n",
        "    interpolated_predictions = zoom(predictions, zoom=scale_factors, mode='nearest')\n",
        "    return interpolated_predictions\n",
        "\n",
        "# Assuming you have predictions from both 1Q and 2Q models\n",
        "predictions_1Q = nib.load('1Q_5_prediction.nii.gz').get_fdata()\n",
        "predictions_2Q = nib.load('2Q_5_prediction.nii.gz').get_fdata()\n",
        "whole = nib.load('whole_5_prediction.nii.gz').get_fdata()\n",
        "\n",
        "# Assuming target shape is the shape of the WHOLE model\n",
        "# target_shape = (2, 259, 259, 91)  # Adjust as necessary\n",
        "# target_shape = (2, 240, 240, 75)  # Adjust as necessary\n",
        "target_shape = (2, 222, 222, int(156/2))\n",
        "                \n",
        "# Interpolate predictions from 1Q and 2Q models to match the target shape\n",
        "interpolated_predictions_1Q = interpolate_predictions(predictions_1Q, target_shape)\n",
        "# target_shape = (2, 240, 240, 76)  # Adjust as necessary\n",
        "\n",
        "interpolated_predictions_2Q = interpolate_predictions(predictions_2Q, target_shape)\n",
        "\n",
        "# Combine predictions using averaging or another suitable method\n",
        "\n",
        "\n",
        "merged =np.concatenate((interpolated_predictions_1Q, interpolated_predictions_2Q), axis=3)\n",
        "\n",
        "print(f'merged: {merged.shape}')\n",
        "print(f'whole: {whole.shape}')\n",
        "\n",
        "ensemble_mean = (merged + whole) / 2  # Averaging for ensemble\n",
        "ensemble_maximum = np.maximum(merged,whole)\n",
        "\n",
        "# Now you have the ensemble predictions with the shape of the WHOLE model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'whole' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the transformations directly to the tensors and ensure they are on the same device\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mwhole\u001b[49m)\n\u001b[1;32m      4\u001b[0m prediction_tensor \u001b[38;5;241m=\u001b[39m post_pred(prediction\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      5\u001b[0m label_tensor \u001b[38;5;241m=\u001b[39m post_label(label\u001b[38;5;241m.\u001b[39mto(device))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'whole' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "prediction = whole\n",
        "# Convert predictions and labels to binary format if necessary\n",
        "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
        "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
        "\n",
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "prediction_tensor = post_pred(prediction.to(device))\n",
        "label_tensor = post_label(label.to(device))\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(f\"Dice Score: {dice_score:.4f}\")\n",
        "print(\"Inference complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.],\n",
              "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "whole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24417484"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "whole.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[[1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1]],\n",
              "\n",
              "        [[1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1]],\n",
              "\n",
              "        [[1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1]],\n",
              "\n",
              "        [[1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1]],\n",
              "\n",
              "        [[1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1],\n",
              "         [1, 1, 1, ..., 1, 1, 1]]],\n",
              "\n",
              "\n",
              "       [[[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "        [[0, 0, 0, ..., 0, 0, 0],\n",
              "         [1, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 1, 0, ..., 0, 0, 0],\n",
              "         [1, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "        [[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 1, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 0, 1, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 1, 0, 0],\n",
              "         [0, 0, 1, ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 1, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "        [[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 1, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 1, ..., 0, 0, 0]],\n",
              "\n",
              "        [[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]]]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_copy = (merged > 0).astype(int)\n",
        "merged_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.947058755111706"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(merged_copy == whole) / whole.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the transformations directly to the tensors and ensure they are on the same device\n",
        "merged = torch.tensor((np.maximum(merged,whole)))\n",
        "\n",
        "prediction_tensor = post_pred(merged.to(device))\n",
        "label_tensor = post_label(label.to(device))\n",
        "\n",
        "# Compute Dice score\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "dice_metric(y_pred=prediction_tensor, y=label_tensor)\n",
        "dice_score = dice_metric.aggregate().item()\n",
        "dice_metric.reset()\n",
        "\n",
        "print(dice_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0TIqDI0vR"
      },
      "source": [
        "## View training in tensorboard\n",
        "\n",
        "Please uncomment the following cell to load tensorboard results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dteU4yMVI0vR",
        "outputId": "8aa4354d-4d1a-4e57-d50e-d9a6eb88db5f"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDZoq7ZI0vS"
      },
      "source": [
        "## Check best model output with the input image and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pp1HWrqI0vS",
        "jupyter": {
          "outputs_hidden": true
        },
        "lines_to_next_cell": 2,
        "outputId": "c7388a09-ec21-44bb-9747-1f1c8d1f81f9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "device = torch.device(\"cuda:0\")\n",
        "net.to(device)\n",
        "with torch.no_grad():\n",
        "    for i, val_data in enumerate(net.val_dataloader()):\n",
        "        roi_size = (160, 160, 160)\n",
        "        sw_batch_size = 4\n",
        "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, net)\n",
        "        # plot the slice [:, :, 80]\n",
        "        plt.figure(\"check\", (18, 6))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(f\"image {i}\")\n",
        "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(f\"label {i}\")\n",
        "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(f\"output {i}\")\n",
        "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grzE-kjOI0vS"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfhZOGgOI0vS"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
